{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "file_path = \"../../Combined_Data/combined_spar_data_full_parameters_split.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define features and labels\n",
    "# Features - based on your provided images (excluding ib and ic which are now labels)\n",
    "feature_columns = [\n",
    "    \"freq\",  # Frequency\n",
    "    \"vb\",\n",
    "    \"vc\",  # Voltage parameters\n",
    "    \"DEV_GEOM_L\",\n",
    "    \"NUM_OF_TRANS_RF\",  # Device geometry\n",
    "]\n",
    "\n",
    "# Labels - de-embedded S-parameters\n",
    "\n",
    "s_parameter_labels = [\n",
    "    \"S_deemb(1,1)_real\",\n",
    "    \"S_deemb(1,1)_imag\",\n",
    "    \"S_deemb(1,2)_real\",\n",
    "    \"S_deemb(1,2)_imag\",\n",
    "    \"S_deemb(2,1)_real\",\n",
    "    \"S_deemb(2,1)_imag\",\n",
    "    \"S_deemb(2,2)_real\",\n",
    "    \"S_deemb(2,2)_imag\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check for null values in both features and labels\n",
    "print(\"Checking for null values in features:\")\n",
    "feature_nulls = df[feature_columns].isnull().sum()\n",
    "print(feature_nulls[feature_nulls > 0])  # Only show features with nulls\n",
    "\n",
    "print(\"\\nChecking for null values in labels:\")\n",
    "label_nulls = df[s_parameter_labels].isnull().sum()\n",
    "print(label_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter rows with any null values in features or labels\n",
    "df_clean = df.dropna(subset=feature_columns + s_parameter_labels)\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows with null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create separate dataframes for features and labels\n",
    "X = df_clean[feature_columns].copy()\n",
    "Y = df_clean[s_parameter_labels].copy()\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"\\nFeature dataset shape: {X.shape}\")\n",
    "print(f\"S-parameter labels shape: {Y.shape}\")\n",
    "\n",
    "# Step 6: Basic statistics for all datasets\n",
    "print(\"\\nFeature statistics (first 5 columns):\")\n",
    "print(X.iloc[:, :5].describe())\n",
    "\n",
    "\n",
    "print(\"\\nS-parameter statistics (first 4 columns):\")\n",
    "print(Y.iloc[:, :4].describe())\n",
    "\n",
    "# Optional: Save cleaned datasets to files\n",
    "# X.to_csv(\"hbt_features.csv\", index=False)\n",
    "# Y.to_csv(\"hbt_sparam_labels.csv\", index=False)\n",
    "\n",
    "print(\"\\nFeature and label separation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_label_correlations(X, y, target_names, filename):\n",
    "    \"\"\"Create a heatmap of correlations between features and labels\"\"\"\n",
    "    # Calculate correlations\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    correlation = combined.corr()\n",
    "\n",
    "    # Extract only the correlations between features and labels\n",
    "    feature_target_corr = correlation.loc[X.columns, target_names]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(feature_target_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Feature-Target Correlations\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return feature_target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s11_labels = [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"]\n",
    "s11_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s11_labels], s11_labels, \"s11_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S11 parameters:\")\n",
    "for label in s11_labels:\n",
    "    top_features = s11_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s12_labels = [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"]\n",
    "s12_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s12_labels], s12_labels, \"s12_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S12 parameters:\")\n",
    "for label in s12_labels:\n",
    "    top_features = s12_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s21_labels = [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"]\n",
    "s21_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s21_labels], s21_labels, \"s21_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S21 parameters:\")\n",
    "for label in s21_labels:\n",
    "    top_features = s21_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s22_labels = [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"]\n",
    "s22_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s22_labels], s22_labels, \"s22_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S22 parameters:\")\n",
    "for label in s22_labels:\n",
    "    top_features = s22_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extrapolation_split(\n",
    "    df,\n",
    "    train_threshold=40e9,\n",
    "    test_freqs=[\n",
    "        41,\n",
    "        42,\n",
    "        43,\n",
    "        44,\n",
    "        45,\n",
    "        46,\n",
    "        47,\n",
    "        48,\n",
    "        49,\n",
    "        50,\n",
    "        51,\n",
    "        52,\n",
    "        53,\n",
    "        54,\n",
    "        55,\n",
    "        56,\n",
    "        57,\n",
    "        58,\n",
    "        59,\n",
    "        60,\n",
    "        61,\n",
    "        62,\n",
    "        63,\n",
    "        64,\n",
    "        65,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a train-test split for extrapolation where:\n",
    "    - Training set contains all data with frequency <= train_threshold\n",
    "    - Test set contains specified frequencies above the threshold\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing a 'freq' column in Hz\n",
    "    train_threshold : float, default=60e9\n",
    "        Maximum frequency (in Hz) to include in training data\n",
    "    test_freqs : list, default=[61, 62, 63, 64, 65]\n",
    "        Specific frequencies (in GHz) to include in test set\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_mask : numpy array\n",
    "        Boolean mask for training data\n",
    "    test_mask : numpy array\n",
    "        Boolean mask for test data\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert test frequencies from GHz to Hz\n",
    "    test_freqs_hz = np.array(test_freqs) * 1e9\n",
    "\n",
    "    # Get all unique frequency values\n",
    "    unique_freqs = np.sort(df[\"freq\"].unique())\n",
    "\n",
    "    # Create training mask: include all frequencies <= threshold\n",
    "    train_mask = df[\"freq\"] <= train_threshold\n",
    "\n",
    "    # Create test mask: include only the specified test frequencies\n",
    "    test_mask = df[\"freq\"].isin(test_freqs_hz)\n",
    "\n",
    "    # Print dataset information\n",
    "    train_freqs = unique_freqs[unique_freqs <= train_threshold]\n",
    "    test_freqs_found = np.intersect1d(unique_freqs, test_freqs_hz)\n",
    "    test_freqs_missing = np.setdiff1d(test_freqs_hz, unique_freqs)\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(unique_freqs)} unique frequency values from {unique_freqs.min() / 1e9:.2f} GHz to {unique_freqs.max() / 1e9:.2f} GHz\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Training on {len(train_freqs)} unique frequencies from {train_freqs.min() / 1e9:.2f} GHz to {train_freqs.max() / 1e9:.2f} GHz\"\n",
    "    )\n",
    "    print(f\"Training set: {train_mask.sum()} samples\")\n",
    "\n",
    "    print(\n",
    "        f\"Testing on {len(test_freqs_found)} unique frequencies: {test_freqs_found / 1e9} GHz\"\n",
    "    )\n",
    "    print(f\"Test set: {test_mask.sum()} samples\")\n",
    "\n",
    "    if len(test_freqs_missing) > 0:\n",
    "        print(\n",
    "            f\"Warning: {len(test_freqs_missing)} requested test frequencies not found in data: {test_freqs_missing / 1e9} GHz\"\n",
    "        )\n",
    "\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = create_extrapolation_split(df)\n",
    "\n",
    "# Use the masks to split features and labels\n",
    "X_raw_train = X[train_mask].copy()\n",
    "X_raw_test = X[test_mask].copy()\n",
    "Y_raw_train = Y[train_mask].copy()\n",
    "Y_raw_test = Y[test_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train[\"vb_is_zero\"] = (X_train[\"vb\"] == 0).astype(int)\n",
    "X_train[\"vb_is_high\"] = ((X_train[\"vb\"] >= 0.7) & (X_train[\"vb\"] <= 0.9)).astype(int)\n",
    "X_train[\"vc_is_zero\"] = (X_train[\"vc\"] == 0).astype(int)\n",
    "X_train[\"vc_is_1_2V\"] = ((X_train[\"vc\"] >= 1.1) & (X_train[\"vc\"] <= 1.3)).astype(int)\n",
    "X_train[\"vc_is_1_5V\"] = ((X_train[\"vc\"] >= 1.4) & (X_train[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# For test data\n",
    "X_test = X_raw_test.copy()\n",
    "X_test[\"vb_is_zero\"] = (X_test[\"vb\"] == 0).astype(int)\n",
    "X_test[\"vb_is_high\"] = ((X_test[\"vb\"] >= 0.7) & (X_test[\"vb\"] <= 0.9)).astype(int)\n",
    "X_test[\"vc_is_zero\"] = (X_test[\"vc\"] == 0).astype(int)\n",
    "X_test[\"vc_is_1_2V\"] = ((X_test[\"vc\"] >= 1.1) & (X_test[\"vc\"] <= 1.3)).astype(int)\n",
    "X_test[\"vc_is_1_5V\"] = ((X_test[\"vc\"] >= 1.4) & (X_test[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# STEP 3: Initialize and fit scaler ONLY on training data\n",
    "voltage_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "voltage_scaler.fit(X_train[[\"vb\", \"vc\"]])  # Fit only on training data\n",
    "\n",
    "# STEP 4: Transform both datasets using the fitted scaler\n",
    "X_train[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_train[[\"vb\", \"vc\"]])\n",
    "X_test[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_test[[\"vb\", \"vc\"]])\n",
    "\n",
    "# STEP 5: Save the scaler for future use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(voltage_scaler, \"voltage_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"DEV_L_0_9um\"] = (X_train[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_train.loc[:, \"DEV_L_2_5um\"] = (X_train[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_train.loc[:, \"DEV_L_5_0um\"] = (X_train[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"DEV_GEOM_L\", axis=1)\n",
    "\n",
    "# Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"DEV_L_0_9um\"] = (X_test[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_test.loc[:, \"DEV_L_2_5um\"] = (X_test[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_test.loc[:, \"DEV_L_5_0um\"] = (X_test[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"DEV_GEOM_L\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"TRANS_1\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_train.loc[:, \"TRANS_2\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_train.loc[:, \"TRANS_4\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"NUM_OF_TRANS_RF\", axis=1)\n",
    "\n",
    "# STEP 3: Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"TRANS_1\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_test.loc[:, \"TRANS_2\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_test.loc[:, \"TRANS_4\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"NUM_OF_TRANS_RF\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import frequency_preprocessing\n",
    "\n",
    "importlib.reload(frequency_preprocessing)\n",
    "from frequency_preprocessing import preprocess_frequency\n",
    "\n",
    "# Then try using it\n",
    "X_train, X_test = preprocess_frequency(X_train, X_test, fit_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if columns exist, create them if they don't\n",
    "for i in range(1, 6):\n",
    "    if f\"freq_pos_in_band_{i}\" not in X_train.columns:\n",
    "        X_train[f\"freq_pos_in_band_{i}\"] = 0\n",
    "    else:\n",
    "        X_train[f\"freq_pos_in_band_{i}\"] = X_train[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "\n",
    "    if X_test is not None:\n",
    "        if f\"freq_pos_in_band_{i}\" not in X_test.columns:\n",
    "            X_test[f\"freq_pos_in_band_{i}\"] = 0\n",
    "        else:\n",
    "            X_test[f\"freq_pos_in_band_{i}\"] = X_test[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "\n",
    "# Fill any remaining NaN values in other columns\n",
    "X_train = X_train.fillna(0)\n",
    "if X_test is not None:\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "# ADD THIS CODE HERE - Make sure X_test has same columns in same order as X_train\n",
    "X_test = X_test.reindex(columns=X_train.columns)\n",
    "print(\"Column order match:\", list(X_train.columns) == list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "# Create directory for results\n",
    "os.makedirs(\"freq_aware_results\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Define SMAPE function for better handling of small values\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate SMAPE with protection against division by zero.\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + epsilon\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    smape = numerator / denominator\n",
    "    return np.mean(smape) * 100\n",
    "\n",
    "\n",
    "# Define mean absolute percentage error function\n",
    "def mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate MAPE with protection against division by zero.\"\"\"\n",
    "    non_zero = np.abs(y_true) > epsilon\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan\n",
    "    percentage_errors = (\n",
    "        np.abs(\n",
    "            (y_true[non_zero] - y_pred[non_zero]) / (np.abs(y_true[non_zero]) + epsilon)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "    return np.mean(percentage_errors)\n",
    "\n",
    "\n",
    "# Frequency-aware neural network\n",
    "class FrequencyAwareNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq_features,\n",
    "        other_features,\n",
    "        hidden_sizes=[64, 128, 256],\n",
    "        dropout_rate=0.2,\n",
    "        activation=\"silu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == \"silu\":\n",
    "            activation_fn = nn.SiLU()\n",
    "        elif activation == \"relu\":\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            activation_fn = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        # Frequency-specific processing branch\n",
    "        freq_layers = []\n",
    "        prev_size = freq_features\n",
    "        for h_size in hidden_sizes[:2]:  # First two hidden sizes for branches\n",
    "            freq_layers.append(nn.Linear(prev_size, h_size))\n",
    "            freq_layers.append(\n",
    "                activation_fn\n",
    "            )  # Using SiLU (Swish) activation for better performance\n",
    "            freq_layers.append(nn.BatchNorm1d(h_size))\n",
    "            freq_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.freq_branch = nn.Sequential(*freq_layers)\n",
    "\n",
    "        # Other parameters branch\n",
    "        other_layers = []\n",
    "        prev_size = other_features\n",
    "        for h_size in hidden_sizes[:2]:\n",
    "            other_layers.append(nn.Linear(prev_size, h_size))\n",
    "            other_layers.append(activation_fn)\n",
    "            other_layers.append(nn.BatchNorm1d(h_size))\n",
    "            other_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.other_branch = nn.Sequential(*other_layers)\n",
    "\n",
    "        # Combined processing with residual connections\n",
    "        combined_layers = []\n",
    "        prev_size = hidden_sizes[1] * 2  # Output size from both branches combined\n",
    "\n",
    "        for h_size in hidden_sizes[2:]:\n",
    "            combined_layers.append(nn.Linear(prev_size, h_size))\n",
    "            combined_layers.append(activation_fn)\n",
    "            combined_layers.append(nn.BatchNorm1d(h_size))\n",
    "            combined_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        # Final output layer for real and imaginary components\n",
    "        combined_layers.append(nn.Linear(prev_size, 2))\n",
    "\n",
    "        self.combined = nn.Sequential(*combined_layers)\n",
    "\n",
    "        # Store feature indices for processing\n",
    "        self.freq_indices = None\n",
    "        self.other_indices = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into frequency and other features\n",
    "        if self.freq_indices is None or self.other_indices is None:\n",
    "            raise ValueError(\n",
    "                \"Feature indices not set. Call set_feature_indices() first.\"\n",
    "            )\n",
    "\n",
    "        freq_input = x[:, self.freq_indices]\n",
    "        other_input = x[:, self.other_indices]\n",
    "\n",
    "        # Process through branches\n",
    "        freq_features = self.freq_branch(freq_input)\n",
    "        other_features = self.other_branch(other_input)\n",
    "\n",
    "        # Combine and output\n",
    "        combined = torch.cat([freq_features, other_features], dim=1)\n",
    "        return self.combined(combined)\n",
    "\n",
    "    def set_feature_indices(self, freq_indices, other_indices):\n",
    "        \"\"\"Set indices for frequency and other features.\"\"\"\n",
    "        self.freq_indices = freq_indices\n",
    "        self.other_indices = other_indices\n",
    "\n",
    "\n",
    "# Helper function to identify frequency-related features\n",
    "def identify_frequency_features(X_columns):\n",
    "    \"\"\"Identify frequency-related features in the dataset.\"\"\"\n",
    "    freq_features = [\n",
    "        i\n",
    "        for i, col in enumerate(X_columns)\n",
    "        if \"freq\" in col.lower() or \"band\" in col.lower()\n",
    "    ]\n",
    "    other_features = [i for i in range(len(X_columns)) if i not in freq_features]\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(freq_features)} frequency-related features and {len(other_features)} other features\"\n",
    "    )\n",
    "    return freq_features, other_features\n",
    "\n",
    "\n",
    "# Modified prepare_data_for_pytorch to handle scaling\n",
    "def prepare_data_for_pytorch_with_scaling(\n",
    "    X_train, Y_train, X_test, Y_test, components, batch_size=128, scale_y=True\n",
    "):\n",
    "    \"\"\"Prepare data for PyTorch models with optional Y-scaling.\"\"\"\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "    # Handle Y data scaling if requested\n",
    "    if scale_y:\n",
    "        # Create scaler for Y values\n",
    "        y_scaler = StandardScaler()\n",
    "        Y_train_values = Y_train[components].values\n",
    "        Y_test_values = Y_test[components].values\n",
    "\n",
    "        # Fit scaler and transform data\n",
    "        Y_train_scaled = y_scaler.fit_transform(Y_train_values)\n",
    "        Y_test_scaled = y_scaler.transform(Y_test_values)\n",
    "\n",
    "        # Convert to tensors\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train_scaled)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test_scaled)\n",
    "\n",
    "        # Save scaler for later use\n",
    "        component_str = \"_\".join(components)\n",
    "        joblib.dump(y_scaler, f\"freq_aware_results/{component_str}_scaler.pkl\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            y_scaler,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # No scaling\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train[components].values)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test[components].values)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    X_test_tensor,\n",
    "    Y_test_tensor,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=15,\n",
    "    verbose=True,\n",
    "    lr_scheduler_type=\"reduce_on_plateau\",\n",
    "    warmup_epochs=5,\n",
    "):\n",
    "    \"\"\"Train a PyTorch model with early stopping and learning rate scheduling.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up learning rate scheduler based on specified type\n",
    "    if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.85, patience=5, verbose=verbose, min_lr=5e-7\n",
    "        )\n",
    "    elif lr_scheduler_type == \"cosine_annealing\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif lr_scheduler_type == \"one_cycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=optimizer.param_groups[0][\"lr\"],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # For early stopping\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Track losses and learning rates for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Apply learning rate warmup if needed\n",
    "        if warmup_epochs > 0 and epoch < warmup_epochs and scheduler is None:\n",
    "            lr_multiplier = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = optimizer.param_groups[0][\"lr\"] * lr_multiplier\n",
    "\n",
    "        # Record current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step OneCycleLR scheduler here if being used\n",
    "            if lr_scheduler_type == \"one_cycle\":\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, Y_test_tensor.to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {current_lr:.8f}\"\n",
    "            )\n",
    "\n",
    "        # Learning rate scheduler step (except for OneCycleLR which is done per iteration)\n",
    "        if scheduler is not None:\n",
    "            if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            elif lr_scheduler_type == \"cosine_annealing\":\n",
    "                scheduler.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot learning rate schedule\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(\"freq_aware_results/learning_rate_schedule.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "# Modified evaluate_model function to handle scaling\n",
    "def evaluate_model_with_scaling(\n",
    "    model, X_test_tensor, Y_test_tensor, Y_test, components, device, y_scaler=None\n",
    "):\n",
    "    \"\"\"Evaluate a trained model and calculate performance metrics.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform if scaler was used\n",
    "    if y_scaler is not None:\n",
    "        predictions_original = y_scaler.inverse_transform(predictions)\n",
    "        y_test_original = Y_test[components].values\n",
    "    else:\n",
    "        predictions_original = predictions\n",
    "        y_test_original = Y_test[components].values\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        y_true = y_test_original[:, i]\n",
    "        y_pred = predictions_original[:, i]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Use SMAPE instead of MAPE for S12\n",
    "        if \"S12\" in component or \"S_deemb(1,2)\" in component:\n",
    "            smape_val = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"smape\": smape_val,\n",
    "            }\n",
    "        else:\n",
    "            # Regular MAPE for other S-parameters\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "            }\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        \"rmse\": np.mean([metrics[comp][\"rmse\"] for comp in components]),\n",
    "        \"r2\": np.mean([metrics[comp][\"r2\"] for comp in components]),\n",
    "        \"mae\": np.mean([metrics[comp][\"mae\"] for comp in components]),\n",
    "    }\n",
    "\n",
    "    # Add SMAPE or MAPE average depending on which components were evaluated\n",
    "    if any(\"S12\" in comp or \"S_deemb(1,2)\" in comp for comp in components):\n",
    "        avg_metrics[\"smape\"] = np.mean([metrics[comp][\"smape\"] for comp in components])\n",
    "    else:\n",
    "        avg_metrics[\"mape\"] = np.mean([metrics[comp][\"mape\"] for comp in components])\n",
    "\n",
    "    return metrics, avg_metrics, predictions_original\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses, model_name):\n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Learning Curves for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"freq_aware_results/learning_curves_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_predictions(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot predictions vs actual values.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        ax.scatter(y_true, y_pred, alpha=0.3)\n",
    "        ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"{component}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/predictions_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_distribution(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot error distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        errors = y_pred - y_true\n",
    "\n",
    "        sns.histplot(errors, kde=True, ax=ax)\n",
    "        ax.set_xlabel(\"Prediction Error\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{component} Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/error_dist_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Modified train_frequency_aware_models function\n",
    "def train_frequency_aware_models(\n",
    "    X_train, X_test, Y_train, Y_test, hyperparameters=None, selected_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train frequency-aware models for each S-parameter with conditional scaling.\n",
    "    \"\"\"\n",
    "    # S-parameter definitions\n",
    "    s_parameter_models = {\"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"]}\n",
    "\n",
    "    # 'S12': ['S_deemb(1,2)_real', 'S_deemb(1,2)_imag']\n",
    "\n",
    "    # Set default hyperparameters if not provided\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {\n",
    "            \"hidden_sizes\": [64, 128, 256],\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 150,\n",
    "            \"early_stopping_patience\": 15,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"lr_scheduler_type\": \"one_cycle\",\n",
    "        }\n",
    "\n",
    "    # Filter features if requested\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train.columns)\n",
    "\n",
    "    # Store results and models\n",
    "    models = {}\n",
    "    all_results = {}\n",
    "    all_predictions = {}\n",
    "    scalers = {}  # Store scalers for each model\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train a model for each S-parameter\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training frequency-aware model for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Decide whether to scale Y data (only for S12)\n",
    "        scale_y = model_name == \"S12\" or model_name == \"S21\"\n",
    "\n",
    "        # Prepare data with conditional scaling\n",
    "        prep_results = prepare_data_for_pytorch_with_scaling(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            components,\n",
    "            hyperparameters[\"batch_size\"],\n",
    "            scale_y=scale_y,\n",
    "        )\n",
    "\n",
    "        if scale_y:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                y_scaler,\n",
    "            ) = prep_results\n",
    "            scalers[model_name] = y_scaler\n",
    "            print(\"Applied StandardScaler to Y values for S12\")\n",
    "        else:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                _,\n",
    "            ) = prep_results\n",
    "\n",
    "        # Initialize model\n",
    "        model = FrequencyAwareNetwork(\n",
    "            len(freq_indices),\n",
    "            len(other_indices),\n",
    "            hyperparameters[\"hidden_sizes\"],\n",
    "            hyperparameters[\"dropout_rate\"],\n",
    "            hyperparameters.get(\"activation\", \"gelu\"),\n",
    "        )\n",
    "        model.set_feature_indices(freq_indices, other_indices)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        # Loss and optimizer - Use Huber Loss for S21 to handle outliers better\n",
    "        if model_name == \"S21\":\n",
    "            criterion = nn.HuberLoss(delta=0.1)  # Less sensitive to outliers\n",
    "            print(\"Using Huber Loss for S21\")\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        # Train model (use your existing train_model function)\n",
    "        trained_model, train_losses, val_losses = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            hyperparameters[\"epochs\"],\n",
    "            hyperparameters[\"early_stopping_patience\"],\n",
    "            lr_scheduler_type=hyperparameters.get(\"lr_scheduler_type\", \"one_cycle\"),\n",
    "        )\n",
    "\n",
    "        # Plot learning curves\n",
    "        plot_learning_curves(train_losses, val_losses, model_name)\n",
    "\n",
    "        # Evaluate model with proper scaling handling\n",
    "        metrics, avg_metrics, predictions = evaluate_model_with_scaling(\n",
    "            trained_model,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            Y_test,\n",
    "            components,\n",
    "            device,\n",
    "            scalers.get(model_name),\n",
    "        )\n",
    "\n",
    "        # Plot predictions and error distributions\n",
    "        plot_predictions(Y_test, predictions, components, model_name)\n",
    "        plot_error_distribution(Y_test, predictions, components, model_name)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nPerformance metrics for {model_name}:\")\n",
    "        for component, metric in metrics.items():\n",
    "            print(f\"  {component}:\")\n",
    "            print(f\"    RMSE: {metric['rmse']:.6f}\")\n",
    "            print(f\"    R²: {metric['r2']:.6f}\")\n",
    "            print(f\"    MAE: {metric['mae']:.6f}\")\n",
    "            if \"smape\" in metric:\n",
    "                print(f\"    SMAPE: {metric['smape']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"    MAPE: {metric['mape']:.2f}%\")\n",
    "\n",
    "        print(f\"\\nAverage metrics for {model_name}:\")\n",
    "        print(f\"  R²: {avg_metrics['r2']:.6f}\")\n",
    "        print(f\"  RMSE: {avg_metrics['rmse']:.6f}\")\n",
    "        print(f\"  MAE: {avg_metrics['mae']:.6f}\")\n",
    "        if \"smape\" in avg_metrics:\n",
    "            print(f\"  SMAPE: {avg_metrics['smape']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  MAPE: {avg_metrics['mape']:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        models[model_name] = trained_model\n",
    "        all_results[model_name] = {\n",
    "            \"component_metrics\": metrics,\n",
    "            \"avg_metrics\": avg_metrics,\n",
    "        }\n",
    "        all_predictions[model_name] = predictions\n",
    "\n",
    "    # Record total training time\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Save models\n",
    "    for model_name, model in models.items():\n",
    "        torch.save(model.state_dict(), f\"freq_aware_results/{model_name}_model.pth\")\n",
    "\n",
    "    print(\"Models and results saved to freq_aware_results/\")\n",
    "\n",
    "    return models, all_results, all_predictions, scalers\n",
    "\n",
    "\n",
    "# Function to experiment with different hyperparameters\n",
    "def hyperparameter_tuning(X_train, X_test, Y_train, Y_test, param_grid):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning by training models with different configurations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values to try\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each configuration\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    def generate_combinations(index, current_params):\n",
    "        if index == len(param_keys):\n",
    "            # Train model with current parameter combination\n",
    "            config_name = \"_\".join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "            print(f\"\\n\\n{'#' * 70}\")\n",
    "            print(f\"# Testing configuration: {config_name}\")\n",
    "            print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "            # Train models\n",
    "            _, all_results, _ = train_frequency_aware_models(\n",
    "                X_train, X_test, Y_train, Y_test, hyperparameters=current_params\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            avg_r2 = np.mean(\n",
    "                [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "            )\n",
    "            results[config_name] = {\n",
    "                \"params\": current_params.copy(),\n",
    "                \"avg_r2\": avg_r2,\n",
    "                \"detailed_results\": all_results,\n",
    "            }\n",
    "            return\n",
    "\n",
    "        # Recursive exploration of parameter combinations\n",
    "        for value in param_values[index]:\n",
    "            current_params[param_keys[index]] = value\n",
    "            generate_combinations(index + 1, current_params)\n",
    "\n",
    "    # Start generating combinations\n",
    "    generate_combinations(0, {})\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (config_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Configuration: {config_name}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "        print(f\"   Parameters: {result['params']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to test different feature subsets\n",
    "def feature_selection_experiment(X_train, X_test, Y_train, Y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Test different feature subsets to find optimal combinations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Complete feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    feature_sets : dict\n",
    "        Dictionary mapping set names to lists of feature columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each feature set\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for set_name, features in feature_sets.items():\n",
    "        print(f\"\\n\\n{'#' * 70}\")\n",
    "        print(f\"# Testing feature set: {set_name} ({len(features)} features)\")\n",
    "        print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "        # Train models with this feature set\n",
    "        _, all_results, _ = train_frequency_aware_models(\n",
    "            X_train, X_test, Y_train, Y_test, selected_features=features\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        avg_r2 = np.mean(\n",
    "            [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "        )\n",
    "        results[set_name] = {\n",
    "            \"features\": features,\n",
    "            \"feature_count\": len(features),\n",
    "            \"avg_r2\": avg_r2,\n",
    "            \"detailed_results\": all_results,\n",
    "        }\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE SELECTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (set_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Feature Set: {set_name}\")\n",
    "        print(f\"   Features: {len(result['features'])}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Example of running with all features and default hyperparameters\n",
    "# models, results, predictions = train_frequency_aware_models(\n",
    "#     X_train, X_test, Y_raw_train, Y_raw_test,\n",
    "#     hyperparameters=default_hyperparameters\n",
    "# )\n",
    "\n",
    "# Example of hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'dropout_rate': [0.1, 0.2, 0.3],\n",
    "#     'batch_size': [128, 256, 512]\n",
    "# }\n",
    "# tuning_results = hyperparameter_tuning(X_train, X_test, Y_raw_train, Y_raw_test, param_grid)\n",
    "\n",
    "# Example of feature selection experiment\n",
    "# core_features = ['freq', 'vb', 'vc', 'gm_abs_log']\n",
    "# freq_features = [col for col in X_train.columns if 'freq' in col]\n",
    "# impedance_features = [col for col in X_train.columns if 'Zin' in col or 'Zout' in col]\n",
    "\n",
    "# feature_sets = {\n",
    "#     'all_features': X_train.columns.tolist(),\n",
    "#     'frequency_only': freq_features,\n",
    "#     'core_plus_frequency': core_features + freq_features,\n",
    "#     'core_plus_impedance': core_features + impedance_features,\n",
    "#     'optimized_set': ['freq', 'freq_log', 'freq_log_norm', 'vb', 'vc', 'gm_abs_log',\n",
    "#                       'Zin_real_log', 'Zin_imag_log', 'Zout_real_log']\n",
    "# }\n",
    "# feature_results = feature_selection_experiment(X_train, X_test, Y_raw_train, Y_raw_test, feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training S(2,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frequency_aware_models(\n",
    "    X_train, X_test, Y_train, Y_test, hyperparameters=None, selected_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train frequency-aware models for each S-parameter with separate models for real/imaginary,\n",
    "    and different architectures for frequency vs other parameters.\n",
    "    \"\"\"\n",
    "    # S-parameter definitions\n",
    "    s_parameter_components = {\n",
    "        \"S21_real\": \"S_deemb(2,1)_real\",\n",
    "        \"S21_imag\": \"S_deemb(2,1)_imag\",\n",
    "    }\n",
    "\n",
    "    # Set default hyperparameters if not provided\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {\n",
    "            \"hidden_sizes\": [64, 128, 256],\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 150,\n",
    "            \"early_stopping_patience\": 15,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"lr_scheduler_type\": \"one_cycle\",\n",
    "            \"weight_decay\": 1e-5,\n",
    "        }\n",
    "\n",
    "    # Extract architecture-specific parameters if provided\n",
    "    freq_hidden_sizes = hyperparameters.get(\"freq_hidden_sizes\", [64, 128])\n",
    "    other_hidden_sizes = hyperparameters.get(\"other_hidden_sizes\", [64, 128])\n",
    "\n",
    "    # Use component-specific network sizes if provided\n",
    "    real_hidden_sizes = hyperparameters.get(\n",
    "        \"real_hidden_sizes\", hyperparameters[\"hidden_sizes\"]\n",
    "    )\n",
    "    imag_hidden_sizes = hyperparameters.get(\n",
    "        \"imag_hidden_sizes\", hyperparameters[\"hidden_sizes\"]\n",
    "    )\n",
    "\n",
    "    # Filter features if requested\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create a copy of data and add normalized frequency for better extrapolation\n",
    "    X_train_enhanced = X_train.copy()\n",
    "    X_test_enhanced = X_test.copy()\n",
    "\n",
    "    # Add normalized frequency (critical for extrapolation)\n",
    "    max_freq = X_train[\"freq\"].max()\n",
    "    X_train_enhanced[\"freq_ratio\"] = X_train[\"freq\"] / max_freq\n",
    "    X_test_enhanced[\"freq_ratio\"] = X_test[\"freq\"] / max_freq\n",
    "\n",
    "    # Also add inverse freq to capture low-frequency behavior\n",
    "    X_train_enhanced[\"freq_inv\"] = 1.0 / (\n",
    "        X_train[\"freq\"] + 1e5\n",
    "    )  # Avoid division by zero\n",
    "    X_test_enhanced[\"freq_inv\"] = 1.0 / (X_test[\"freq\"] + 1e5)\n",
    "\n",
    "    # Add log frequency to better capture wide frequency ranges\n",
    "    X_train_enhanced[\"freq_log\"] = np.log(X_train[\"freq\"] + 1e5)\n",
    "    X_test_enhanced[\"freq_log\"] = np.log(X_test[\"freq\"] + 1e5)\n",
    "\n",
    "    # Add squared frequency for high-frequency behavior\n",
    "    X_train_enhanced[\"freq_squared\"] = (X_train[\"freq\"] / max_freq) ** 2\n",
    "    X_test_enhanced[\"freq_squared\"] = (X_test[\"freq\"] / max_freq) ** 2\n",
    "\n",
    "    # Analyze S21 real/imag training data\n",
    "    s21_real_train = Y_train[\"S_deemb(2,1)_real\"].values\n",
    "    s21_imag_train = Y_train[\"S_deemb(2,1)_imag\"].values\n",
    "\n",
    "    print(\n",
    "        f\"S21 real training range: {s21_real_train.min():.6f} to {s21_real_train.max():.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"S21 imag training range: {s21_imag_train.min():.6f} to {s21_imag_train.max():.6f}\"\n",
    "    )\n",
    "\n",
    "    # Get high-frequency training statistics\n",
    "    X_train_s21 = X_train.copy()\n",
    "    X_train_s21[\"S21_real\"] = s21_real_train\n",
    "    X_train_s21[\"S21_imag\"] = s21_imag_train\n",
    "\n",
    "    high_freq_threshold = np.percentile(X_train_s21[\"freq\"], 80)\n",
    "    high_freq_data = X_train_s21[X_train_s21[\"freq\"] >= high_freq_threshold]\n",
    "\n",
    "    print(\n",
    "        f\"Using high-frequency training data (>{high_freq_threshold / 1e9:.1f} GHz) for stabilization\"\n",
    "    )\n",
    "\n",
    "    high_freq_real_mean = high_freq_data[\"S21_real\"].mean()\n",
    "    high_freq_imag_mean = high_freq_data[\"S21_imag\"].mean()\n",
    "\n",
    "    print(\n",
    "        f\"High-frequency S21 means: real={high_freq_real_mean:.6f}, imag={high_freq_imag_mean:.6f}\"\n",
    "    )\n",
    "\n",
    "    # Calculate different bounds for real and imaginary parts\n",
    "    # For real part - tighter bounds due to problems with this component\n",
    "    real_p10 = np.percentile(s21_real_train, 10)\n",
    "    real_p90 = np.percentile(s21_real_train, 90)\n",
    "    real_range = real_p90 - real_p10\n",
    "    real_min = real_p10 - 0.2 * real_range  # Tighter bound for real\n",
    "    real_max = real_p90 + 0.2 * real_range\n",
    "\n",
    "    # For imaginary part - more relaxed bounds since it's behaving better\n",
    "    imag_p05 = np.percentile(s21_imag_train, 5)\n",
    "    imag_p95 = np.percentile(s21_imag_train, 95)\n",
    "    imag_range = imag_p95 - imag_p05\n",
    "    imag_min = imag_p05 - 0.3 * imag_range  # More relaxed bound\n",
    "    imag_max = imag_p95 + 0.3 * imag_range\n",
    "\n",
    "    print(\"Setting component-specific bounds:\")\n",
    "    print(f\"  Real: [{real_min:.6f}, {real_max:.6f}]\")\n",
    "    print(f\"  Imaginary: [{imag_min:.6f}, {imag_max:.6f}]\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train_enhanced.columns)\n",
    "\n",
    "    # Store results and models\n",
    "    models = {}\n",
    "    all_results = {}\n",
    "    all_predictions = {}\n",
    "    scalers = {}  # Store scalers for each model\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Final predictions\n",
    "    s21_predictions = np.zeros((len(X_test), 2))\n",
    "\n",
    "    # Log-cosh loss function for robust training\n",
    "    def log_cosh_loss(y_pred, y_true):\n",
    "        return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "\n",
    "    # Train a separate model for each component\n",
    "    for model_name, component in s_parameter_components.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training model for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Component-specific hyperparameters\n",
    "        if model_name == \"S21_real\":\n",
    "            # More aggressive regularization for real component\n",
    "            comp_hyperparams = hyperparameters.copy()\n",
    "            comp_hyperparams[\"dropout_rate\"] = (\n",
    "                0.3  # Higher dropout but not as aggressive\n",
    "            )\n",
    "            comp_hyperparams[\"learning_rate\"] = 0.001  # Lower learning rate\n",
    "            comp_hyperparams[\"weight_decay\"] = (\n",
    "                hyperparameters.get(\"weight_decay\", 1e-5) * 2\n",
    "            )  # Higher L2 regularization\n",
    "            comp_hidden_sizes = real_hidden_sizes\n",
    "            scheduler_type = hyperparameters.get(\"real_scheduler\", \"cosine_annealing\")\n",
    "        else:\n",
    "            # Standard hyperparameters for imaginary component\n",
    "            comp_hyperparams = hyperparameters.copy()\n",
    "            comp_hyperparams[\"dropout_rate\"] = 0.2\n",
    "            comp_hyperparams[\"weight_decay\"] = hyperparameters.get(\"weight_decay\", 1e-5)\n",
    "            comp_hidden_sizes = imag_hidden_sizes\n",
    "            scheduler_type = hyperparameters.get(\"imag_scheduler\", \"reduce_on_plateau\")\n",
    "\n",
    "        # Prepare data for this component\n",
    "        Y_train_comp = Y_train[[component]].copy()\n",
    "        Y_test_comp = Y_test[[component]].copy()\n",
    "\n",
    "        # Scale inputs and outputs\n",
    "        x_scaler = StandardScaler()\n",
    "        y_scaler = StandardScaler()\n",
    "\n",
    "        # Store the scalers\n",
    "        scalers[model_name] = {\"x\": x_scaler, \"y\": y_scaler}\n",
    "\n",
    "        # Scale X data - only scale non-frequency features\n",
    "        X_train_scaled = X_train_enhanced.copy()\n",
    "        X_test_scaled = X_test_enhanced.copy()\n",
    "\n",
    "        # Get non-frequency columns to scale\n",
    "        non_freq_cols = [\n",
    "            col for col in X_train_enhanced.columns if \"freq\" not in col.lower()\n",
    "        ]\n",
    "\n",
    "        # Scale those columns\n",
    "        X_train_scaled[non_freq_cols] = x_scaler.fit_transform(\n",
    "            X_train_enhanced[non_freq_cols]\n",
    "        )\n",
    "        X_test_scaled[non_freq_cols] = x_scaler.transform(\n",
    "            X_test_enhanced[non_freq_cols]\n",
    "        )\n",
    "\n",
    "        # Scale Y data\n",
    "        Y_train_scaled = y_scaler.fit_transform(Y_train_comp)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train_scaled.values).to(device)\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train_scaled).to(device)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled.values).to(device)\n",
    "\n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=comp_hyperparams[\"batch_size\"], shuffle=True\n",
    "        )\n",
    "\n",
    "        # Define a focused model specifically for this component with different branch architectures\n",
    "        class ComponentModel(nn.Module):\n",
    "            def __init__(\n",
    "                self,\n",
    "                input_size,\n",
    "                hidden_sizes,\n",
    "                dropout_rate,\n",
    "                freq_indices,\n",
    "                other_indices,\n",
    "            ):\n",
    "                super().__init__()\n",
    "                self.freq_indices = freq_indices\n",
    "                self.other_indices = other_indices\n",
    "\n",
    "                # Frequency branch with custom architecture\n",
    "                freq_layers = []\n",
    "                input_size_freq = len(freq_indices)\n",
    "                for i in range(len(freq_hidden_sizes)):\n",
    "                    out_size = freq_hidden_sizes[i]\n",
    "                    freq_layers.append(nn.Linear(input_size_freq, out_size))\n",
    "                    freq_layers.append(nn.BatchNorm1d(out_size))\n",
    "                    freq_layers.append(nn.GELU())\n",
    "                    freq_layers.append(nn.Dropout(dropout_rate))\n",
    "                    input_size_freq = out_size\n",
    "                self.freq_layers = nn.Sequential(*freq_layers)\n",
    "\n",
    "                # Other parameters branch with custom architecture\n",
    "                other_layers = []\n",
    "                input_size_other = len(other_indices)\n",
    "                for i in range(len(other_hidden_sizes)):\n",
    "                    out_size = other_hidden_sizes[i]\n",
    "                    other_layers.append(nn.Linear(input_size_other, out_size))\n",
    "                    other_layers.append(nn.BatchNorm1d(out_size))\n",
    "                    other_layers.append(nn.GELU())\n",
    "                    other_layers.append(nn.Dropout(dropout_rate))\n",
    "                    input_size_other = out_size\n",
    "                self.other_layers = nn.Sequential(*other_layers)\n",
    "\n",
    "                # Attention mechanism for better integration of branches\n",
    "                self.attention = nn.Sequential(\n",
    "                    nn.Linear(freq_hidden_sizes[-1] + other_hidden_sizes[-1], 64),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(64, 2),\n",
    "                    nn.Softmax(dim=1),\n",
    "                )\n",
    "\n",
    "                # Combined layers\n",
    "                combined_size = freq_hidden_sizes[-1] + other_hidden_sizes[-1]\n",
    "\n",
    "                combined_layers = []\n",
    "                input_size_combined = combined_size\n",
    "                for i in range(len(hidden_sizes) - 1):\n",
    "                    combined_layers.append(\n",
    "                        nn.Linear(input_size_combined, hidden_sizes[i])\n",
    "                    )\n",
    "                    combined_layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
    "                    combined_layers.append(nn.GELU())\n",
    "                    combined_layers.append(nn.Dropout(dropout_rate))\n",
    "                    input_size_combined = hidden_sizes[i]\n",
    "\n",
    "                # Output layer\n",
    "                combined_layers.append(\n",
    "                    nn.Linear(\n",
    "                        hidden_sizes[-2]\n",
    "                        if len(hidden_sizes) > 1\n",
    "                        else input_size_combined,\n",
    "                        1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Apply tanh only to real component to constrain outputs\n",
    "                if model_name == \"S21_real\":\n",
    "                    combined_layers.append(nn.Tanh())\n",
    "\n",
    "                self.combined_layers = nn.Sequential(*combined_layers)\n",
    "\n",
    "            def forward(self, x):\n",
    "                # Extract frequency and other inputs\n",
    "                freq_input = x[:, self.freq_indices]\n",
    "                other_input = x[:, self.other_indices]\n",
    "\n",
    "                # Process through respective branches\n",
    "                freq_features = self.freq_layers(freq_input)\n",
    "                other_features = self.other_layers(other_input)\n",
    "\n",
    "                # Combine features\n",
    "                combined = torch.cat([freq_features, other_features], dim=1)\n",
    "\n",
    "                # Apply attention mechanism\n",
    "                attention_weights = self.attention(combined)\n",
    "                weighted_freq = freq_features * attention_weights[:, 0].unsqueeze(1)\n",
    "                weighted_other = other_features * attention_weights[:, 1].unsqueeze(1)\n",
    "\n",
    "                # New combined features with attention\n",
    "                combined_attention = torch.cat([weighted_freq, weighted_other], dim=1)\n",
    "\n",
    "                # Final processing\n",
    "                return self.combined_layers(combined_attention)\n",
    "\n",
    "        # Initialize model\n",
    "        model = ComponentModel(\n",
    "            X_train_scaled.shape[1],\n",
    "            comp_hidden_sizes,\n",
    "            comp_hyperparams[\"dropout_rate\"],\n",
    "            freq_indices,\n",
    "            other_indices,\n",
    "        ).to(device)\n",
    "\n",
    "        # Use different loss functions and optimizers for real vs imaginary\n",
    "        if model_name == \"S21_real\":\n",
    "            # Real component is more problematic - use more robust loss\n",
    "            criterion = nn.SmoothL1Loss(beta=0.05)\n",
    "            print(f\"Using SmoothL1Loss with beta=0.05 for {model_name}\")\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=comp_hyperparams[\"learning_rate\"],\n",
    "                weight_decay=comp_hyperparams[\"weight_decay\"],\n",
    "            )\n",
    "        else:\n",
    "            # Imaginary component - custom loss\n",
    "            def custom_loss(pred, target):\n",
    "                # Combined loss: 70% Huber, 30% Log-cosh\n",
    "                huber = nn.HuberLoss(delta=0.1)(pred, target)\n",
    "                logcosh = log_cosh_loss(pred, target)\n",
    "                return 0.7 * huber + 0.3 * logcosh\n",
    "\n",
    "            criterion = custom_loss\n",
    "            print(f\"Using custom combined loss (Huber + Log-cosh) for {model_name}\")\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=comp_hyperparams[\"learning_rate\"],\n",
    "                weight_decay=comp_hyperparams[\"weight_decay\"],\n",
    "            )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        # Use appropriate scheduler based on component and settings\n",
    "        if scheduler_type == \"cosine_annealing\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=comp_hyperparams[\"epochs\"], eta_min=1e-6\n",
    "            )\n",
    "            print(f\"Using CosineAnnealingLR scheduler for {model_name}\")\n",
    "        elif scheduler_type == \"one_cycle\":\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=comp_hyperparams[\"learning_rate\"] * 10,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=comp_hyperparams[\"epochs\"],\n",
    "            )\n",
    "            print(f\"Using OneCycleLR scheduler for {model_name}\")\n",
    "        else:  # reduce_on_plateau default\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    "            )\n",
    "            print(f\"Using ReduceLROnPlateau scheduler for {model_name}\")\n",
    "\n",
    "        print(f\"Starting training for {model_name}...\")\n",
    "\n",
    "        for epoch in range(comp_hyperparams[\"epochs\"]):\n",
    "            # Training\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Add L2 regularization for real component (in addition to optimizer's weight_decay)\n",
    "                if model_name == \"S21_real\":\n",
    "                    l2_reg = 0\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if \"weight\" in name:  # Apply only to weights, not biases\n",
    "                            l2_reg += torch.norm(param, 2)\n",
    "                    loss += 1e-5 * l2_reg\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Component-specific gradient clipping\n",
    "                if model_name == \"S21_real\":\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update OneCycleLR scheduler if used\n",
    "                if scheduler_type == \"one_cycle\":\n",
    "                    scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Calculate average loss\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test_tensor)\n",
    "                # Use test labels if available, otherwise just monitor outputs\n",
    "                if Y_test_comp is not None:\n",
    "                    Y_test_tensor = torch.FloatTensor(\n",
    "                        y_scaler.transform(Y_test_comp)\n",
    "                    ).to(device)\n",
    "                    val_loss = criterion(test_outputs, Y_test_tensor).item()\n",
    "                else:\n",
    "                    val_loss = criterion(\n",
    "                        test_outputs, torch.zeros_like(test_outputs)\n",
    "                    ).item()\n",
    "\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{comp_hyperparams['epochs']}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\"\n",
    "                )\n",
    "\n",
    "            # Update scheduler\n",
    "            if scheduler_type == \"cosine_annealing\":\n",
    "                scheduler.step()\n",
    "            elif scheduler_type == \"reduce_on_plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            # one_cycle scheduler is updated after each batch\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= comp_hyperparams[\"early_stopping_patience\"]:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "        # Get predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "        # Inverse transform\n",
    "        predictions = y_scaler.inverse_transform(predictions)\n",
    "\n",
    "        # Apply component-specific clipping\n",
    "        if model_name == \"S21_real\":\n",
    "            predictions = np.clip(predictions, real_min, real_max)\n",
    "            component_idx = 0\n",
    "        else:\n",
    "            predictions = np.clip(predictions, imag_min, imag_max)\n",
    "            component_idx = 1\n",
    "\n",
    "        # Store in final predictions array\n",
    "        s21_predictions[:, component_idx] = predictions.flatten()\n",
    "\n",
    "        # Calculate metrics\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions.flatten()\n",
    "\n",
    "        metrics = {\n",
    "            \"mse\": mean_squared_error(y_true, y_pred),\n",
    "            \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            \"r2\": r2_score(y_true, y_pred),\n",
    "            \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "            \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        }\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nPerformance metrics for {model_name}:\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"  R²: {metrics['r2']:.6f}\")\n",
    "        print(f\"  MAE: {metrics['mae']:.6f}\")\n",
    "        print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "\n",
    "        # Store model and results\n",
    "        models[model_name] = model\n",
    "        all_results[model_name] = metrics\n",
    "\n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Learning Curves for {model_name}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"freq_aware_results/learning_curves_{model_name}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot actual vs predicted values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_true, y_pred, alpha=0.3)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"r--\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.title(f\"Actual vs Predicted for {model_name}\")\n",
    "        plt.savefig(f\"freq_aware_results/actual_vs_predicted_{model_name}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Calculate average metrics for S21 (combining real and imaginary)\n",
    "    s21_metrics = {\n",
    "        \"S_deemb(2,1)_real\": all_results[\"S21_real\"],\n",
    "        \"S_deemb(2,1)_imag\": all_results[\"S21_imag\"],\n",
    "    }\n",
    "\n",
    "    s21_avg_metrics = {\n",
    "        \"rmse\": (all_results[\"S21_real\"][\"rmse\"] + all_results[\"S21_imag\"][\"rmse\"]) / 2,\n",
    "        \"r2\": (all_results[\"S21_real\"][\"r2\"] + all_results[\"S21_imag\"][\"r2\"]) / 2,\n",
    "        \"mae\": (all_results[\"S21_real\"][\"mae\"] + all_results[\"S21_imag\"][\"mae\"]) / 2,\n",
    "        \"mape\": (all_results[\"S21_real\"][\"mape\"] + all_results[\"S21_imag\"][\"mape\"]) / 2,\n",
    "    }\n",
    "\n",
    "    # Print combined results\n",
    "    print(\"\\nCombined metrics for S21:\")\n",
    "    print(f\"  R²: {s21_avg_metrics['r2']:.6f}\")\n",
    "    print(f\"  RMSE: {s21_avg_metrics['rmse']:.6f}\")\n",
    "    print(f\"  MAE: {s21_avg_metrics['mae']:.6f}\")\n",
    "    print(f\"  MAPE: {s21_avg_metrics['mape']:.2f}%\")\n",
    "\n",
    "    # Record total training time\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Save models\n",
    "    for model_name, model in models.items():\n",
    "        torch.save(model.state_dict(), f\"freq_aware_results/{model_name}_model.pth\")\n",
    "\n",
    "    # Save scalers\n",
    "    joblib.dump(scalers, \"freq_aware_results/s21_component_scalers.pkl\")\n",
    "\n",
    "    print(\"Models and results saved to freq_aware_results/\")\n",
    "\n",
    "    # The following is needed to match the expected return format\n",
    "    combined_results = {\n",
    "        \"S21\": {\"component_metrics\": s21_metrics, \"avg_metrics\": s21_avg_metrics}\n",
    "    }\n",
    "    combined_predictions = {\"S21\": s21_predictions}\n",
    "\n",
    "    return models, combined_results, combined_predictions, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_predictions_as_features(\n",
    "    models,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    scalers=None,\n",
    "    output_path=\"freq_aware_results/prediction_features\",\n",
    "    target_sparameter=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions using trained models and save them as features for training other models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of trained models from train_frequency_aware_models\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Training and test feature data\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Training and test target data\n",
    "    scalers : dict, optional\n",
    "        Dictionary of scalers used for Y values during training\n",
    "    output_path : str\n",
    "        Path to save the features\n",
    "    target_sparameter : str, optional\n",
    "        S-parameter we're targeting next (e.g., 'S12'). Its actual values will be excluded\n",
    "        to prevent data leakage\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_features, test_features : pd.DataFrame\n",
    "        DataFrames containing prediction features for training and test data\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Define component mapping for each S-parameter\n",
    "    component_mapping = {\n",
    "        \"S11\": [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"],\n",
    "        \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "        \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "        \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "    }\n",
    "\n",
    "    # Initialize DataFrames to store prediction features\n",
    "    train_features = pd.DataFrame(index=Y_train.index)\n",
    "    test_features = pd.DataFrame(index=Y_test.index)\n",
    "\n",
    "    # Determine which components to exclude from actual values to prevent data leakage\n",
    "    excluded_components = []\n",
    "    if target_sparameter and target_sparameter in component_mapping:\n",
    "        excluded_components = component_mapping[target_sparameter]\n",
    "        print(\n",
    "            f\"Excluding actual values of {target_sparameter} components to prevent data leakage\"\n",
    "        )\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train.columns)\n",
    "\n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Generating {model_name} predictions...\")\n",
    "\n",
    "        # Get components for this model\n",
    "        components = component_mapping.get(model_name, [])\n",
    "        if not components:\n",
    "            print(f\"Warning: Unknown model name {model_name}\")\n",
    "            continue\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Convert input data to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "        X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "        # Set feature indices in the model\n",
    "        model.set_feature_indices(freq_indices, other_indices)\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            train_preds = model(X_train_tensor.to(device)).cpu().numpy()\n",
    "            test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "        # Apply inverse transform if scaler was used\n",
    "        if scalers and model_name in scalers and scalers[model_name] is not None:\n",
    "            train_preds = scalers[model_name].inverse_transform(train_preds)\n",
    "            test_preds = scalers[model_name].inverse_transform(test_preds)\n",
    "\n",
    "        # Add predictions as features with appropriate names\n",
    "        for i, component in enumerate(components):\n",
    "            feature_name = f\"{model_name}_pred_{component}\"\n",
    "            train_features[feature_name] = train_preds[:, i]\n",
    "            test_features[feature_name] = test_preds[:, i]\n",
    "\n",
    "    # Save the prediction features\n",
    "    train_features.to_csv(f\"{output_path}/train_prediction_features.csv\", index=True)\n",
    "    test_features.to_csv(f\"{output_path}/test_prediction_features.csv\", index=True)\n",
    "\n",
    "    print(f\"Saved prediction features to {output_path}\")\n",
    "    print(f\"Train features shape: {train_features.shape}\")\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_prediction_features(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    prediction_path=\"freq_aware_results/prediction_features\",\n",
    "    append_mode=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load prediction features and combine them with original features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Original or previously combined training and test feature data\n",
    "    prediction_path : str\n",
    "        Path where prediction features are saved\n",
    "    append_mode : bool, default=True\n",
    "        If True, appends new prediction features without overwriting existing ones\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_combined, X_test_combined : pd.DataFrame\n",
    "        Combined original and prediction features\n",
    "    \"\"\"\n",
    "    # Load prediction features\n",
    "    train_predictions = pd.read_csv(\n",
    "        f\"{prediction_path}/train_prediction_features.csv\", index_col=0\n",
    "    )\n",
    "    test_predictions = pd.read_csv(\n",
    "        f\"{prediction_path}/test_prediction_features.csv\", index_col=0\n",
    "    )\n",
    "\n",
    "    # Ensure indices match\n",
    "    train_predictions.index = X_train.index\n",
    "    test_predictions.index = X_test.index\n",
    "\n",
    "    # Check for existing prediction columns to avoid duplication\n",
    "    if append_mode:\n",
    "        # Get list of new prediction columns\n",
    "        pred_cols = train_predictions.columns.tolist()\n",
    "\n",
    "        # Filter out columns that already exist in X_train/X_test\n",
    "        existing_cols = [col for col in pred_cols if col in X_train.columns]\n",
    "        new_cols = [col for col in pred_cols if col not in X_train.columns]\n",
    "\n",
    "        if existing_cols:\n",
    "            print(\n",
    "                f\"Found {len(existing_cols)} prediction columns already in dataset, preserving existing values\"\n",
    "            )\n",
    "\n",
    "        if new_cols:\n",
    "            # Add only new columns to avoid overwriting\n",
    "            X_train_combined = X_train.copy()\n",
    "            X_test_combined = X_test.copy()\n",
    "\n",
    "            for col in new_cols:\n",
    "                X_train_combined[col] = train_predictions[col]\n",
    "                X_test_combined[col] = test_predictions[col]\n",
    "\n",
    "            print(f\"Added {len(new_cols)} new prediction columns to existing features\")\n",
    "        else:\n",
    "            # No new columns to add\n",
    "            X_train_combined = X_train.copy()\n",
    "            X_test_combined = X_test.copy()\n",
    "            print(\"No new prediction columns to add - all already exist in dataset\")\n",
    "    else:\n",
    "        # Original behavior - add all columns (potentially overwriting)\n",
    "        X_train_combined = pd.concat([X_train, train_predictions], axis=1)\n",
    "        X_test_combined = pd.concat([X_test, test_predictions], axis=1)\n",
    "        print(\n",
    "            f\"Combined {X_train.shape[1]} original features with {train_predictions.shape[1]} prediction features\"\n",
    "        )\n",
    "\n",
    "    # Report final feature counts\n",
    "    print(\n",
    "        f\"Final feature counts - Train: {X_train_combined.shape[1]}, Test: {X_test_combined.shape[1]}\"\n",
    "    )\n",
    "\n",
    "    return X_train_combined, X_test_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(1,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train your S11 model as you've been doing\n",
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [256, 512, 1024, 512],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# After training your S11 model\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# Generate S11 predictions to use as features for S12 model\n",
    "# Specify S12 as the target to ensure we're not leaking its actual values\n",
    "train_features, test_features = generate_and_save_predictions_as_features(\n",
    "    models,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    scalers=scalers,\n",
    "    target_sparameter=\"S21\",  # This prevents S12 actual values from being included\n",
    ")\n",
    "\n",
    "# Combine with original features\n",
    "X_train_combined, X_test_combined = load_and_combine_prediction_features(\n",
    "    X_train, X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(1,2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [384, 768, 1536, 768, 384],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# After training your S11 model\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train_combined,\n",
    "    X_test_combined,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    ")\n",
    "\n",
    "# Generate S11 predictions to use as features for S12 model\n",
    "# Specify S12 as the target to ensure we're not leaking its actual values\n",
    "train_features, test_features = generate_and_save_predictions_as_features(\n",
    "    models,\n",
    "    X_train_combined,\n",
    "    X_test_combined,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    scalers=scalers,\n",
    "    target_sparameter=\"S21\",  # This prevents S12 actual values from being included\n",
    ")\n",
    "\n",
    "# Combine with original features\n",
    "X_train_combined, X_test_combined = load_and_combine_prediction_features(\n",
    "    X_train_combined, X_test_combined\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(2,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    # Core hyperparameters - keeping stable for now\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 350,  # Slightly increase epochs\n",
    "    \"early_stopping_patience\": 40,  # Slightly increase patience\n",
    "    \"activation\": \"gelu\",\n",
    "    \"hidden_sizes\": [1024, 512, 256, 128],\n",
    "    # Architecture improvements for both components\n",
    "    # S21_real - deeper and wider to improve R²\n",
    "    \"real_hidden_sizes\": [2048, 2048, 2048, 1024, 512, 256],\n",
    "    # S21_imag - improved architecture to reduce MAE\n",
    "    \"imag_hidden_sizes\": [2048, 2048, 2048, 2048, 1024, 512],  # Wider and deeper\n",
    "    # Enhanced branch architectures for both components\n",
    "    \"freq_hidden_sizes\": [512, 1024, 2048, 1024],  # Deeper frequency branch\n",
    "    \"other_hidden_sizes\": [1024, 2048, 2048, 1024],  # Deeper parameter branch\n",
    "    # Maintain other parameters\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"real_scheduler\": \"cosine_annealing\",\n",
    "    \"imag_scheduler\": \"one_cycle\",\n",
    "}\n",
    "\n",
    "s21_model, s21_metrics, s21_avg_metrics, s21_predictions = train_frequency_aware_models(\n",
    "    X_train_combined, X_test_combined, Y_raw_train, Y_raw_test, best_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(2,2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [1024, 1536, 2048, 1536, 1024],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# After training your S11 model\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train_combined,\n",
    "    X_test_combined,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    ")\n",
    "\n",
    "# Generate S11 predictions to use as features for S12 model\n",
    "# Specify S12 as the target to ensure we're not leaking its actual values\n",
    "train_features, test_features = generate_and_save_predictions_as_features(\n",
    "    models,\n",
    "    X_train_combined,\n",
    "    X_test_combined,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    scalers=scalers,\n",
    "    target_sparameter=\"S21\",  # This prevents S12 actual values from being included\n",
    ")\n",
    "\n",
    "# Combine with original features\n",
    "X_train_combined, X_test_combined = load_and_combine_prediction_features(\n",
    "    X_train_combined, X_test_combined\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
