{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "file_path = \"dataset.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define features and labels\n",
    "# Features - based on your provided images (excluding ib and ic which are now labels)\n",
    "feature_columns = [\n",
    "    \"freq\",  # Frequency\n",
    "    \"vb\",\n",
    "    \"vc\",  # Voltage parameters\n",
    "    \"DEV_GEOM_L\",\n",
    "    \"NUM_OF_TRANS_RF\",  # Device geometry\n",
    "]\n",
    "\n",
    "# Labels - de-embedded S-parameters\n",
    "\n",
    "s_parameter_labels = [\n",
    "    \"S_deemb(1,1)_real\",\n",
    "    \"S_deemb(1,1)_imag\",\n",
    "    \"S_deemb(1,2)_real\",\n",
    "    \"S_deemb(1,2)_imag\",\n",
    "    \"S_deemb(2,1)_real\",\n",
    "    \"S_deemb(2,1)_imag\",\n",
    "    \"S_deemb(2,2)_real\",\n",
    "    \"S_deemb(2,2)_imag\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for null values in features:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Checking for null values in labels:\n",
      "S_deemb(1,1)_real    39886\n",
      "S_deemb(1,1)_imag    39886\n",
      "S_deemb(1,2)_real    39886\n",
      "S_deemb(1,2)_imag    39886\n",
      "S_deemb(2,1)_real    39886\n",
      "S_deemb(2,1)_imag    39886\n",
      "S_deemb(2,2)_real    39886\n",
      "S_deemb(2,2)_imag    39886\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Check for null values in both features and labels\n",
    "print(\"Checking for null values in features:\")\n",
    "feature_nulls = df[feature_columns].isnull().sum()\n",
    "print(feature_nulls[feature_nulls > 0])  # Only show features with nulls\n",
    "\n",
    "print(\"\\nChecking for null values in labels:\")\n",
    "label_nulls = df[s_parameter_labels].isnull().sum()\n",
    "print(label_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset shape: (196100, 49)\n",
      "Cleaned dataset shape: (156214, 49)\n",
      "Removed 39886 rows with null values\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Filter rows with any null values in features or labels\n",
    "df_clean = df.dropna(subset=feature_columns + s_parameter_labels)\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows with null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset shape: (196100, 49)\n",
      "Cleaned dataset shape: (156214, 49)\n",
      "Removed 39886 rows with null values\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Filter rows with any null values in features or labels\n",
    "df_clean = df.dropna(subset=feature_columns + s_parameter_labels)\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows with null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature dataset shape: (156214, 5)\n",
      "S-parameter labels shape: (156214, 8)\n",
      "\n",
      "Feature statistics (first 5 columns):\n",
      "               freq             vb             vc     DEV_GEOM_L  \\\n",
      "count  1.562140e+05  156214.000000  156214.000000  156214.000000   \n",
      "mean   2.904730e+10       0.462686       0.859285       2.833728   \n",
      "std    2.054359e+10       0.640656       0.646469       1.751900   \n",
      "min    1.000000e+08      -1.800000      -0.600000       0.900000   \n",
      "25%    1.000000e+10       0.000000       0.280000       0.900000   \n",
      "50%    2.850000e+10       0.770000       1.000000       2.500000   \n",
      "75%    4.700000e+10       0.860000       1.310000       5.000000   \n",
      "max    6.500000e+10       1.040000       2.370000       5.000000   \n",
      "\n",
      "       NUM_OF_TRANS_RF  \n",
      "count    156214.000000  \n",
      "mean          1.476551  \n",
      "std           0.775818  \n",
      "min           1.000000  \n",
      "25%           1.000000  \n",
      "50%           1.000000  \n",
      "75%           2.000000  \n",
      "max           4.000000  \n",
      "\n",
      "S-parameter statistics (first 4 columns):\n",
      "       S_deemb(1,1)_real  S_deemb(1,1)_imag  S_deemb(1,2)_real  \\\n",
      "count      156214.000000      156214.000000      156214.000000   \n",
      "mean            0.245853          -0.462666           0.125194   \n",
      "std             0.605869           0.265956           0.118055   \n",
      "min            -0.855559          -0.890505          -0.003813   \n",
      "25%            -0.364094          -0.715049           0.033752   \n",
      "50%             0.345364          -0.511300           0.092598   \n",
      "75%             0.841138          -0.221676           0.184632   \n",
      "max             1.000610          -0.000857           0.616569   \n",
      "\n",
      "       S_deemb(1,2)_imag  \n",
      "count      156214.000000  \n",
      "mean            0.092756  \n",
      "std             0.082146  \n",
      "min            -0.110548  \n",
      "25%             0.022753  \n",
      "50%             0.061644  \n",
      "75%             0.163502  \n",
      "max             0.300715  \n",
      "\n",
      "Feature and label separation complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create separate dataframes for features and labels\n",
    "X = df_clean[feature_columns].copy()\n",
    "Y = df_clean[s_parameter_labels].copy()\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"\\nFeature dataset shape: {X.shape}\")\n",
    "print(f\"S-parameter labels shape: {Y.shape}\")\n",
    "\n",
    "# Step 6: Basic statistics for all datasets\n",
    "print(\"\\nFeature statistics (first 5 columns):\")\n",
    "print(X.iloc[:, :5].describe())\n",
    "\n",
    "\n",
    "print(\"\\nS-parameter statistics (first 4 columns):\")\n",
    "print(Y.iloc[:, :4].describe())\n",
    "\n",
    "# Optional: Save cleaned datasets to files\n",
    "# X.to_csv(\"hbt_features.csv\", index=False)\n",
    "# Y.to_csv(\"hbt_sparam_labels.csv\", index=False)\n",
    "\n",
    "print(\"\\nFeature and label separation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_label_correlations(X, y, target_names, filename):\n",
    "    \"\"\"Create a heatmap of correlations between features and labels\"\"\"\n",
    "    # Calculate correlations\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    correlation = combined.corr()\n",
    "\n",
    "    # Extract only the correlations between features and labels\n",
    "    feature_target_corr = correlation.loc[X.columns, target_names]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(feature_target_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Feature-Target Correlations\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return feature_target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features correlated with S11 parameters:\n",
      "\n",
      "Top features for S_deemb(1,1)_real:\n",
      "freq               0.650297\n",
      "vb                 0.443714\n",
      "vc                 0.306241\n",
      "NUM_OF_TRANS_RF    0.092477\n",
      "DEV_GEOM_L         0.077022\n",
      "Name: S_deemb(1,1)_real, dtype: float64\n",
      "\n",
      "Top features for S_deemb(1,1)_imag:\n",
      "freq               0.451187\n",
      "vb                 0.082900\n",
      "vc                 0.042034\n",
      "DEV_GEOM_L         0.021004\n",
      "NUM_OF_TRANS_RF    0.000268\n",
      "Name: S_deemb(1,1)_imag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s11_labels = [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"]\n",
    "s11_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s11_labels], s11_labels, \"s11_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S11 parameters:\")\n",
    "for label in s11_labels:\n",
    "    top_features = s11_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_based_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a train-test split where:\n",
    "    1. No two consecutive frequency values are in the test set\n",
    "    2. Test set frequencies are evenly distributed across frequency bands\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing a 'freq' column\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of unique frequency values to include in test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_mask : numpy array\n",
    "        Boolean mask for training data\n",
    "    test_mask : numpy array\n",
    "        Boolean mask for test data\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Get sorted unique frequency values\n",
    "    unique_freqs = np.sort(df[\"freq\"].unique())\n",
    "    n_freqs = len(unique_freqs)\n",
    "    print(f\"Found {n_freqs} unique frequency values\")\n",
    "\n",
    "    # Define band boundaries\n",
    "    band_boundaries = [\n",
    "        (0, 1e9),  # Band 1: < 1 GHz\n",
    "        (1e9, 6e9),  # Band 2: 1-6 GHz\n",
    "        (6e9, 20e9),  # Band 3: 6-20 GHz\n",
    "        (20e9, 40e9),  # Band 4: 20-40 GHz\n",
    "        (40e9, float(\"inf\")),  # Band 5: > 40 GHz\n",
    "    ]\n",
    "\n",
    "    # Assign frequencies to bands\n",
    "    freq_bands = np.zeros(n_freqs, dtype=int)\n",
    "\n",
    "    for i, freq in enumerate(unique_freqs):\n",
    "        for band_idx, (lower, upper) in enumerate(band_boundaries):\n",
    "            if lower <= freq < upper or (band_idx == 4 and freq >= lower):\n",
    "                freq_bands[i] = band_idx\n",
    "                break\n",
    "\n",
    "    # Count frequencies in each band\n",
    "    band_counts = np.zeros(5, dtype=int)\n",
    "    for band in freq_bands:\n",
    "        band_counts[band] += 1\n",
    "\n",
    "    for band_idx, count in enumerate(band_counts):\n",
    "        print(f\"Band {band_idx + 1}: {count} frequency values\")\n",
    "\n",
    "    # Simple but effective approach: select every k-th frequency as test set\n",
    "    # This guarantees no consecutive frequencies in test set\n",
    "    k = int(1 / test_size)  # If test_size is 0.2, k=5 means select every 5th frequency\n",
    "\n",
    "    # Start with a base selection\n",
    "    test_indices = np.arange(0, n_freqs, k)\n",
    "    print(f\"Base selection gives {len(test_indices)} test frequencies (every {k}th)\")\n",
    "\n",
    "    # Calculate target test frequencies per band\n",
    "    target_per_band = np.zeros(5, dtype=int)\n",
    "    for i, count in enumerate(band_counts):\n",
    "        target_per_band[i] = max(1, int(round(count * test_size)))\n",
    "\n",
    "    print(\"Target test frequencies per band:\")\n",
    "    for i, target in enumerate(target_per_band):\n",
    "        print(f\"Band {i + 1}: {target}\")\n",
    "\n",
    "    # Calculate how many frequencies we actually selected per band\n",
    "    actual_per_band = np.zeros(5, dtype=int)\n",
    "    for idx in test_indices:\n",
    "        band = freq_bands[idx]\n",
    "        actual_per_band[band] += 1\n",
    "\n",
    "    print(\"Actual initial test frequencies per band:\")\n",
    "    for i, actual in enumerate(actual_per_band):\n",
    "        print(f\"Band {i + 1}: {actual}\")\n",
    "\n",
    "    # Adjust selection to better match target distribution\n",
    "    # First, identify bands that need more frequencies\n",
    "    for band in range(5):\n",
    "        if actual_per_band[band] < target_per_band[band]:\n",
    "            # Get candidate indices in this band that aren't already selected\n",
    "            band_candidates = [\n",
    "                i\n",
    "                for i in range(n_freqs)\n",
    "                if freq_bands[i] == band\n",
    "                and i not in test_indices\n",
    "                and i - 1 not in test_indices\n",
    "                and i + 1 not in test_indices\n",
    "            ]\n",
    "\n",
    "            # How many more do we need?\n",
    "            n_needed = target_per_band[band] - actual_per_band[band]\n",
    "\n",
    "            # Select additional frequencies if we have enough candidates\n",
    "            if len(band_candidates) >= n_needed:\n",
    "                # Choose candidates with roughly equal spacing\n",
    "                step = max(1, len(band_candidates) // n_needed)\n",
    "                selected = band_candidates[::step][:n_needed]\n",
    "                test_indices = np.append(test_indices, selected)\n",
    "                actual_per_band[band] += len(selected)\n",
    "\n",
    "    # If we over-selected in some bands, remove frequencies to match target\n",
    "    for band in range(5):\n",
    "        if actual_per_band[band] > target_per_band[band]:\n",
    "            # How many to remove\n",
    "            n_remove = actual_per_band[band] - target_per_band[band]\n",
    "\n",
    "            # Get indices in this band that were selected\n",
    "            band_selected = [i for i in test_indices if freq_bands[i] == band]\n",
    "\n",
    "            # Choose which ones to remove (spaced out)\n",
    "            if band_selected:\n",
    "                step = max(1, len(band_selected) // n_remove)\n",
    "                to_remove = band_selected[::step][:n_remove]\n",
    "                test_indices = np.array([i for i in test_indices if i not in to_remove])\n",
    "                actual_per_band[band] -= len(to_remove)\n",
    "\n",
    "    print(\"Final test frequencies per band after adjustment:\")\n",
    "    for i, actual in enumerate(actual_per_band):\n",
    "        print(f\"Band {i + 1}: {actual} (target: {target_per_band[i]})\")\n",
    "\n",
    "    # Sort the indices\n",
    "    test_indices = np.sort(test_indices)\n",
    "\n",
    "    # Verify that no consecutive frequencies are in test set\n",
    "    for i in range(len(test_indices) - 1):\n",
    "        if test_indices[i + 1] - test_indices[i] == 1:\n",
    "            print(\n",
    "                f\"WARNING: Consecutive frequencies in test set: {unique_freqs[test_indices[i]]} and {unique_freqs[test_indices[i + 1]]}\"\n",
    "            )\n",
    "\n",
    "    # Create test frequencies set\n",
    "    test_freqs = unique_freqs[test_indices]\n",
    "\n",
    "    # Create train and test masks\n",
    "    test_mask = df[\"freq\"].isin(test_freqs)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    print(f\"Final training set: {train_mask.sum()} samples\")\n",
    "    print(f\"Final test set: {test_mask.sum()} samples\")\n",
    "\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 unique frequency values\n",
      "Band 1: 9 frequency values\n",
      "Band 2: 5 frequency values\n",
      "Band 3: 14 frequency values\n",
      "Band 4: 20 frequency values\n",
      "Band 5: 26 frequency values\n",
      "Base selection gives 15 test frequencies (every 5th)\n",
      "Target test frequencies per band:\n",
      "Band 1: 2\n",
      "Band 2: 1\n",
      "Band 3: 3\n",
      "Band 4: 4\n",
      "Band 5: 5\n",
      "Actual initial test frequencies per band:\n",
      "Band 1: 2\n",
      "Band 2: 1\n",
      "Band 3: 3\n",
      "Band 4: 4\n",
      "Band 5: 5\n",
      "Final test frequencies per band after adjustment:\n",
      "Band 1: 2 (target: 2)\n",
      "Band 2: 1 (target: 1)\n",
      "Band 3: 3 (target: 3)\n",
      "Band 4: 4 (target: 4)\n",
      "Band 5: 5 (target: 5)\n",
      "Final training set: 124549 samples\n",
      "Final test set: 31665 samples\n"
     ]
    }
   ],
   "source": [
    "# Replace your current train-test split with the frequency-based approach\n",
    "train_mask, test_mask = create_frequency_based_split(\n",
    "    df_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use the masks to split features and labels\n",
    "X_raw_train = X[train_mask].copy()\n",
    "X_raw_test = X[test_mask].copy()\n",
    "Y_raw_train = Y[train_mask].copy()\n",
    "Y_raw_test = Y[test_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voltage_scaler.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train[\"vb_is_zero\"] = (X_train[\"vb\"] == 0).astype(int)\n",
    "X_train[\"vb_is_high\"] = ((X_train[\"vb\"] >= 0.7) & (X_train[\"vb\"] <= 0.9)).astype(int)\n",
    "X_train[\"vc_is_zero\"] = (X_train[\"vc\"] == 0).astype(int)\n",
    "X_train[\"vc_is_1_2V\"] = ((X_train[\"vc\"] >= 1.1) & (X_train[\"vc\"] <= 1.3)).astype(int)\n",
    "X_train[\"vc_is_1_5V\"] = ((X_train[\"vc\"] >= 1.4) & (X_train[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# For test data\n",
    "X_test = X_raw_test.copy()\n",
    "X_test[\"vb_is_zero\"] = (X_test[\"vb\"] == 0).astype(int)\n",
    "X_test[\"vb_is_high\"] = ((X_test[\"vb\"] >= 0.7) & (X_test[\"vb\"] <= 0.9)).astype(int)\n",
    "X_test[\"vc_is_zero\"] = (X_test[\"vc\"] == 0).astype(int)\n",
    "X_test[\"vc_is_1_2V\"] = ((X_test[\"vc\"] >= 1.1) & (X_test[\"vc\"] <= 1.3)).astype(int)\n",
    "X_test[\"vc_is_1_5V\"] = ((X_test[\"vc\"] >= 1.4) & (X_test[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# STEP 3: Initialize and fit scaler ONLY on training data\n",
    "voltage_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "voltage_scaler.fit(X_train[[\"vb\", \"vc\"]])  # Fit only on training data\n",
    "\n",
    "# STEP 4: Transform both datasets using the fitted scaler\n",
    "X_train[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_train[[\"vb\", \"vc\"]])\n",
    "X_test[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_test[[\"vb\", \"vc\"]])\n",
    "\n",
    "# STEP 5: Save the scaler for future use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(voltage_scaler, \"voltage_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"DEV_L_0_9um\"] = (X_train[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_train.loc[:, \"DEV_L_2_5um\"] = (X_train[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_train.loc[:, \"DEV_L_5_0um\"] = (X_train[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"DEV_GEOM_L\", axis=1)\n",
    "\n",
    "# Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"DEV_L_0_9um\"] = (X_test[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_test.loc[:, \"DEV_L_2_5um\"] = (X_test[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_test.loc[:, \"DEV_L_5_0um\"] = (X_test[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"DEV_GEOM_L\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"TRANS_1\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_train.loc[:, \"TRANS_2\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_train.loc[:, \"TRANS_4\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"NUM_OF_TRANS_RF\", axis=1)\n",
    "\n",
    "# STEP 3: Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"TRANS_1\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_test.loc[:, \"TRANS_2\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_test.loc[:, \"TRANS_4\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"NUM_OF_TRANS_RF\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import frequency_preprocessing\n",
    "\n",
    "importlib.reload(frequency_preprocessing)\n",
    "from frequency_preprocessing import preprocess_frequency\n",
    "\n",
    "# Then try using it\n",
    "X_train, X_test = preprocess_frequency(X_train, X_test, fit_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 for freq_pos_in_band columns\n",
    "for i in range(1, 6):\n",
    "    X_train[f\"freq_pos_in_band_{i}\"] = X_train[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "    if X_test is not None:\n",
    "        X_test[f\"freq_pos_in_band_{i}\"] = X_test[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "\n",
    "# Fill any remaining NaN values in other columns\n",
    "X_train = X_train.fillna(0)\n",
    "if X_test is not None:\n",
    "    X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "# Create directory for results\n",
    "os.makedirs(\"freq_aware_results\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Define SMAPE function for better handling of small values\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate SMAPE with protection against division by zero.\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + epsilon\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    smape = numerator / denominator\n",
    "    return np.mean(smape) * 100\n",
    "\n",
    "\n",
    "# Define mean absolute percentage error function\n",
    "def mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate MAPE with protection against division by zero.\"\"\"\n",
    "    non_zero = np.abs(y_true) > epsilon\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan\n",
    "    percentage_errors = (\n",
    "        np.abs(\n",
    "            (y_true[non_zero] - y_pred[non_zero]) / (np.abs(y_true[non_zero]) + epsilon)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "    return np.mean(percentage_errors)\n",
    "\n",
    "\n",
    "# Frequency-aware neural network\n",
    "class FrequencyAwareNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq_features,\n",
    "        other_features,\n",
    "        hidden_sizes=[64, 128, 256],\n",
    "        dropout_rate=0.2,\n",
    "        activation=\"silu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == \"silu\":\n",
    "            activation_fn = nn.SiLU()\n",
    "        elif activation == \"relu\":\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            activation_fn = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        # Frequency-specific processing branch\n",
    "        freq_layers = []\n",
    "        prev_size = freq_features\n",
    "        for h_size in hidden_sizes[:2]:  # First two hidden sizes for branches\n",
    "            freq_layers.append(nn.Linear(prev_size, h_size))\n",
    "            freq_layers.append(\n",
    "                activation_fn\n",
    "            )  # Using SiLU (Swish) activation for better performance\n",
    "            freq_layers.append(nn.BatchNorm1d(h_size))\n",
    "            freq_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.freq_branch = nn.Sequential(*freq_layers)\n",
    "\n",
    "        # Other parameters branch\n",
    "        other_layers = []\n",
    "        prev_size = other_features\n",
    "        for h_size in hidden_sizes[:2]:\n",
    "            other_layers.append(nn.Linear(prev_size, h_size))\n",
    "            other_layers.append(activation_fn)\n",
    "            other_layers.append(nn.BatchNorm1d(h_size))\n",
    "            other_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.other_branch = nn.Sequential(*other_layers)\n",
    "\n",
    "        # Combined processing with residual connections\n",
    "        combined_layers = []\n",
    "        prev_size = hidden_sizes[1] * 2  # Output size from both branches combined\n",
    "\n",
    "        for h_size in hidden_sizes[2:]:\n",
    "            combined_layers.append(nn.Linear(prev_size, h_size))\n",
    "            combined_layers.append(activation_fn)\n",
    "            combined_layers.append(nn.BatchNorm1d(h_size))\n",
    "            combined_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        # Final output layer for real and imaginary components\n",
    "        combined_layers.append(nn.Linear(prev_size, 2))\n",
    "\n",
    "        self.combined = nn.Sequential(*combined_layers)\n",
    "\n",
    "        # Store feature indices for processing\n",
    "        self.freq_indices = None\n",
    "        self.other_indices = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into frequency and other features\n",
    "        if self.freq_indices is None or self.other_indices is None:\n",
    "            raise ValueError(\n",
    "                \"Feature indices not set. Call set_feature_indices() first.\"\n",
    "            )\n",
    "\n",
    "        freq_input = x[:, self.freq_indices]\n",
    "        other_input = x[:, self.other_indices]\n",
    "\n",
    "        # Process through branches\n",
    "        freq_features = self.freq_branch(freq_input)\n",
    "        other_features = self.other_branch(other_input)\n",
    "\n",
    "        # Combine and output\n",
    "        combined = torch.cat([freq_features, other_features], dim=1)\n",
    "        return self.combined(combined)\n",
    "\n",
    "    def set_feature_indices(self, freq_indices, other_indices):\n",
    "        \"\"\"Set indices for frequency and other features.\"\"\"\n",
    "        self.freq_indices = freq_indices\n",
    "        self.other_indices = other_indices\n",
    "\n",
    "\n",
    "# Helper function to identify frequency-related features\n",
    "def identify_frequency_features(X_columns):\n",
    "    \"\"\"Identify frequency-related features in the dataset.\"\"\"\n",
    "    freq_features = [\n",
    "        i\n",
    "        for i, col in enumerate(X_columns)\n",
    "        if \"freq\" in col.lower() or \"band\" in col.lower()\n",
    "    ]\n",
    "    other_features = [i for i in range(len(X_columns)) if i not in freq_features]\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(freq_features)} frequency-related features and {len(other_features)} other features\"\n",
    "    )\n",
    "    return freq_features, other_features\n",
    "\n",
    "\n",
    "# Modified prepare_data_for_pytorch to handle scaling\n",
    "def prepare_data_for_pytorch_with_scaling(\n",
    "    X_train, Y_train, X_test, Y_test, components, batch_size=128, scale_y=True\n",
    "):\n",
    "    \"\"\"Prepare data for PyTorch models with optional Y-scaling.\"\"\"\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "    # Handle Y data scaling if requested\n",
    "    if scale_y:\n",
    "        # Create scaler for Y values\n",
    "        y_scaler = StandardScaler()\n",
    "        Y_train_values = Y_train[components].values\n",
    "        Y_test_values = Y_test[components].values\n",
    "\n",
    "        # Fit scaler and transform data\n",
    "        Y_train_scaled = y_scaler.fit_transform(Y_train_values)\n",
    "        Y_test_scaled = y_scaler.transform(Y_test_values)\n",
    "\n",
    "        # Convert to tensors\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train_scaled)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test_scaled)\n",
    "\n",
    "        # Save scaler for later use\n",
    "        component_str = \"_\".join(components)\n",
    "        joblib.dump(y_scaler, f\"freq_aware_results/{component_str}_scaler.pkl\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            y_scaler,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # No scaling\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train[components].values)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test[components].values)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    X_test_tensor,\n",
    "    Y_test_tensor,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=15,\n",
    "    verbose=True,\n",
    "    lr_scheduler_type=\"reduce_on_plateau\",\n",
    "    warmup_epochs=5,\n",
    "):\n",
    "    \"\"\"Train a PyTorch model with early stopping and learning rate scheduling.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up learning rate scheduler based on specified type\n",
    "    if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.85, patience=5, min_lr=5e-7\n",
    "        )\n",
    "    elif lr_scheduler_type == \"cosine_annealing\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif lr_scheduler_type == \"one_cycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=optimizer.param_groups[0][\"lr\"],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # For early stopping\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Track losses and learning rates for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Apply learning rate warmup if needed\n",
    "        if warmup_epochs > 0 and epoch < warmup_epochs and scheduler is None:\n",
    "            lr_multiplier = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = optimizer.param_groups[0][\"lr\"] * lr_multiplier\n",
    "\n",
    "        # Record current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step OneCycleLR scheduler here if being used\n",
    "            if lr_scheduler_type == \"one_cycle\":\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, Y_test_tensor.to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {current_lr:.8f}\"\n",
    "            )\n",
    "\n",
    "        # Learning rate scheduler step (except for OneCycleLR which is done per iteration)\n",
    "        if scheduler is not None:\n",
    "            if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            elif lr_scheduler_type == \"cosine_annealing\":\n",
    "                scheduler.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot learning rate schedule\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(\"freq_aware_results/learning_rate_schedule.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "# Modified evaluate_model function to handle scaling\n",
    "def evaluate_model_with_scaling(\n",
    "    model, X_test_tensor, Y_test_tensor, Y_test, components, device, y_scaler=None\n",
    "):\n",
    "    \"\"\"Evaluate a trained model and calculate performance metrics.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform if scaler was used\n",
    "    if y_scaler is not None:\n",
    "        predictions_original = y_scaler.inverse_transform(predictions)\n",
    "        y_test_original = Y_test[components].values\n",
    "    else:\n",
    "        predictions_original = predictions\n",
    "        y_test_original = Y_test[components].values\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        y_true = y_test_original[:, i]\n",
    "        y_pred = predictions_original[:, i]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Use SMAPE instead of MAPE for S12\n",
    "        if \"S12\" in component or \"S_deemb(1,2)\" in component:\n",
    "            smape_val = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"smape\": smape_val,\n",
    "            }\n",
    "        else:\n",
    "            # Regular MAPE for other S-parameters\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "            }\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        \"rmse\": np.mean([metrics[comp][\"rmse\"] for comp in components]),\n",
    "        \"r2\": np.mean([metrics[comp][\"r2\"] for comp in components]),\n",
    "        \"mae\": np.mean([metrics[comp][\"mae\"] for comp in components]),\n",
    "    }\n",
    "\n",
    "    # Add SMAPE or MAPE average depending on which components were evaluated\n",
    "    if any(\"S12\" in comp or \"S_deemb(1,2)\" in comp for comp in components):\n",
    "        avg_metrics[\"smape\"] = np.mean([metrics[comp][\"smape\"] for comp in components])\n",
    "    else:\n",
    "        avg_metrics[\"mape\"] = np.mean([metrics[comp][\"mape\"] for comp in components])\n",
    "\n",
    "    return metrics, avg_metrics, predictions_original\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses, model_name):\n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Learning Curves for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"freq_aware_results/learning_curves_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_predictions(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot predictions vs actual values.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        ax.scatter(y_true, y_pred, alpha=0.3)\n",
    "        ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"{component}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/predictions_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_distribution(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot error distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        errors = y_pred - y_true\n",
    "\n",
    "        sns.histplot(errors, kde=True, ax=ax)\n",
    "        ax.set_xlabel(\"Prediction Error\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{component} Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/error_dist_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Modified train_frequency_aware_models function\n",
    "def train_frequency_aware_models(\n",
    "    X_train, X_test, Y_train, Y_test, hyperparameters=None, selected_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train frequency-aware models for each S-parameter with conditional scaling.\n",
    "    \"\"\"\n",
    "    # S-parameter definitions\n",
    "    s_parameter_models = {\n",
    "        \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "    }\n",
    "\n",
    "    # 'S12': ['S_deemb(1,2)_real', 'S_deemb(1,2)_imag']\n",
    "\n",
    "    # Set default hyperparameters if not provided\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {\n",
    "            \"hidden_sizes\": [64, 128, 256],\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 150,\n",
    "            \"early_stopping_patience\": 15,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"lr_scheduler_type\": \"one_cycle\",\n",
    "        }\n",
    "\n",
    "    # Filter features if requested\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train.columns)\n",
    "\n",
    "    # Store results and models\n",
    "    models = {}\n",
    "    all_results = {}\n",
    "    all_predictions = {}\n",
    "    scalers = {}  # Store scalers for each model\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train a model for each S-parameter\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training frequency-aware model for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Decide whether to scale Y data (only for S12)\n",
    "        scale_y = model_name == \"S12\"\n",
    "\n",
    "        # Prepare data with conditional scaling\n",
    "        prep_results = prepare_data_for_pytorch_with_scaling(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            components,\n",
    "            hyperparameters[\"batch_size\"],\n",
    "            scale_y=scale_y,\n",
    "        )\n",
    "\n",
    "        if scale_y:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                y_scaler,\n",
    "            ) = prep_results\n",
    "            scalers[model_name] = y_scaler\n",
    "            print(\"Applied StandardScaler to Y values for S12\")\n",
    "        else:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                _,\n",
    "            ) = prep_results\n",
    "\n",
    "        # Initialize model\n",
    "        model = FrequencyAwareNetwork(\n",
    "            len(freq_indices),\n",
    "            len(other_indices),\n",
    "            hyperparameters[\"hidden_sizes\"],\n",
    "            hyperparameters[\"dropout_rate\"],\n",
    "            hyperparameters.get(\"activation\", \"gelu\"),\n",
    "        )\n",
    "        model.set_feature_indices(freq_indices, other_indices)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        # Train model (use your existing train_model function)\n",
    "        trained_model, train_losses, val_losses = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            hyperparameters[\"epochs\"],\n",
    "            hyperparameters[\"early_stopping_patience\"],\n",
    "            lr_scheduler_type=hyperparameters.get(\"lr_scheduler_type\", \"one_cycle\"),\n",
    "        )\n",
    "\n",
    "        # Plot learning curves\n",
    "        plot_learning_curves(train_losses, val_losses, model_name)\n",
    "\n",
    "        # Evaluate model with proper scaling handling\n",
    "        metrics, avg_metrics, predictions = evaluate_model_with_scaling(\n",
    "            trained_model,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            Y_test,\n",
    "            components,\n",
    "            device,\n",
    "            scalers.get(model_name),\n",
    "        )\n",
    "\n",
    "        # Plot predictions and error distributions\n",
    "        plot_predictions(Y_test, predictions, components, model_name)\n",
    "        plot_error_distribution(Y_test, predictions, components, model_name)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nPerformance metrics for {model_name}:\")\n",
    "        for component, metric in metrics.items():\n",
    "            print(f\"  {component}:\")\n",
    "            print(f\"    RMSE: {metric['rmse']:.6f}\")\n",
    "            print(f\"    R²: {metric['r2']:.6f}\")\n",
    "            print(f\"    MAE: {metric['mae']:.6f}\")\n",
    "            if \"smape\" in metric:\n",
    "                print(f\"    SMAPE: {metric['smape']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"    MAPE: {metric['mape']:.2f}%\")\n",
    "\n",
    "        print(f\"\\nAverage metrics for {model_name}:\")\n",
    "        print(f\"  R²: {avg_metrics['r2']:.6f}\")\n",
    "        print(f\"  RMSE: {avg_metrics['rmse']:.6f}\")\n",
    "        print(f\"  MAE: {avg_metrics['mae']:.6f}\")\n",
    "        if \"smape\" in avg_metrics:\n",
    "            print(f\"  SMAPE: {avg_metrics['smape']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  MAPE: {avg_metrics['mape']:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        models[model_name] = trained_model\n",
    "        all_results[model_name] = {\n",
    "            \"component_metrics\": metrics,\n",
    "            \"avg_metrics\": avg_metrics,\n",
    "        }\n",
    "        all_predictions[model_name] = predictions\n",
    "\n",
    "    # Record total training time\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Save models\n",
    "    for model_name, model in models.items():\n",
    "        torch.save(model.state_dict(), f\"freq_aware_results/{model_name}_model.pth\")\n",
    "\n",
    "    print(\"Models and results saved to freq_aware_results/\")\n",
    "\n",
    "    return models, all_results, all_predictions, scalers\n",
    "\n",
    "\n",
    "# Function to experiment with different hyperparameters\n",
    "def hyperparameter_tuning(X_train, X_test, Y_train, Y_test, param_grid):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning by training models with different configurations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values to try\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each configuration\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    def generate_combinations(index, current_params):\n",
    "        if index == len(param_keys):\n",
    "            # Train model with current parameter combination\n",
    "            config_name = \"_\".join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "            print(f\"\\n\\n{'#' * 70}\")\n",
    "            print(f\"# Testing configuration: {config_name}\")\n",
    "            print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "            # Train models\n",
    "            _, all_results, _ = train_frequency_aware_models(\n",
    "                X_train, X_test, Y_train, Y_test, hyperparameters=current_params\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            avg_r2 = np.mean(\n",
    "                [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "            )\n",
    "            results[config_name] = {\n",
    "                \"params\": current_params.copy(),\n",
    "                \"avg_r2\": avg_r2,\n",
    "                \"detailed_results\": all_results,\n",
    "            }\n",
    "            return\n",
    "\n",
    "        # Recursive exploration of parameter combinations\n",
    "        for value in param_values[index]:\n",
    "            current_params[param_keys[index]] = value\n",
    "            generate_combinations(index + 1, current_params)\n",
    "\n",
    "    # Start generating combinations\n",
    "    generate_combinations(0, {})\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (config_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Configuration: {config_name}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "        print(f\"   Parameters: {result['params']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to test different feature subsets\n",
    "def feature_selection_experiment(X_train, X_test, Y_train, Y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Test different feature subsets to find optimal combinations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Complete feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    feature_sets : dict\n",
    "        Dictionary mapping set names to lists of feature columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each feature set\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for set_name, features in feature_sets.items():\n",
    "        print(f\"\\n\\n{'#' * 70}\")\n",
    "        print(f\"# Testing feature set: {set_name} ({len(features)} features)\")\n",
    "        print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "        # Train models with this feature set\n",
    "        _, all_results, _ = train_frequency_aware_models(\n",
    "            X_train, X_test, Y_train, Y_test, selected_features=features\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        avg_r2 = np.mean(\n",
    "            [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "        )\n",
    "        results[set_name] = {\n",
    "            \"features\": features,\n",
    "            \"feature_count\": len(features),\n",
    "            \"avg_r2\": avg_r2,\n",
    "            \"detailed_results\": all_results,\n",
    "        }\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE SELECTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (set_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Feature Set: {set_name}\")\n",
    "        print(f\"   Features: {len(result['features'])}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Example of running with all features and default hyperparameters\n",
    "# models, results, predictions = train_frequency_aware_models(\n",
    "#     X_train, X_test, Y_raw_train, Y_raw_test,\n",
    "#     hyperparameters=default_hyperparameters\n",
    "# )\n",
    "\n",
    "# Example of hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'dropout_rate': [0.1, 0.2, 0.3],\n",
    "#     'batch_size': [128, 256, 512]\n",
    "# }\n",
    "# tuning_results = hyperparameter_tuning(X_train, X_test, Y_raw_train, Y_raw_test, param_grid)\n",
    "\n",
    "# Example of feature selection experiment\n",
    "# core_features = ['freq', 'vb', 'vc', 'gm_abs_log']\n",
    "# freq_features = [col for col in X_train.columns if 'freq' in col]\n",
    "# impedance_features = [col for col in X_train.columns if 'Zin' in col or 'Zout' in col]\n",
    "\n",
    "# feature_sets = {\n",
    "#     'all_features': X_train.columns.tolist(),\n",
    "#     'frequency_only': freq_features,\n",
    "#     'core_plus_frequency': core_features + freq_features,\n",
    "#     'core_plus_impedance': core_features + impedance_features,\n",
    "#     'optimized_set': ['freq', 'freq_log', 'freq_log_norm', 'vb', 'vc', 'gm_abs_log',\n",
    "#                       'Zin_real_log', 'Zin_imag_log', 'Zout_real_log']\n",
    "# }\n",
    "# feature_results = feature_selection_experiment(X_train, X_test, Y_raw_train, Y_raw_test, feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(1,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Identified 15 frequency-related features and 6 other features\n",
      "\n",
      "==================================================\n",
      "Training frequency-aware model for S11\n",
      "==================================================\n",
      "Epoch 10/200, Train Loss: 0.009793, Val Loss: 0.007758, LR: 0.00100000\n",
      "Epoch 20/200, Train Loss: 0.008762, Val Loss: 0.006603, LR: 0.00085000\n",
      "Epoch 30/200, Train Loss: 0.008016, Val Loss: 0.006164, LR: 0.00072250\n",
      "Epoch 40/200, Train Loss: 0.007579, Val Loss: 0.006691, LR: 0.00061412\n",
      "Epoch 50/200, Train Loss: 0.007467, Val Loss: 0.006433, LR: 0.00052201\n",
      "Epoch 60/200, Train Loss: 0.007675, Val Loss: 0.006039, LR: 0.00044371\n",
      "Epoch 70/200, Train Loss: 0.007300, Val Loss: 0.006003, LR: 0.00037715\n",
      "Epoch 80/200, Train Loss: 0.007039, Val Loss: 0.006051, LR: 0.00032058\n",
      "Epoch 90/200, Train Loss: 0.007144, Val Loss: 0.006029, LR: 0.00027249\n",
      "Epoch 100/200, Train Loss: 0.007060, Val Loss: 0.006259, LR: 0.00023162\n",
      "Epoch 110/200, Train Loss: 0.006876, Val Loss: 0.006006, LR: 0.00019687\n",
      "Epoch 120/200, Train Loss: 0.006805, Val Loss: 0.005912, LR: 0.00014224\n",
      "Epoch 130/200, Train Loss: 0.006751, Val Loss: 0.005923, LR: 0.00012091\n",
      "Epoch 140/200, Train Loss: 0.006755, Val Loss: 0.006100, LR: 0.00010277\n",
      "Epoch 150/200, Train Loss: 0.006697, Val Loss: 0.005772, LR: 0.00007425\n",
      "Epoch 160/200, Train Loss: 0.006593, Val Loss: 0.005958, LR: 0.00006311\n",
      "Epoch 170/200, Train Loss: 0.006707, Val Loss: 0.006021, LR: 0.00004560\n",
      "Epoch 180/200, Train Loss: 0.006607, Val Loss: 0.005953, LR: 0.00003876\n",
      "Early stopping at epoch 180\n",
      "\n",
      "Performance metrics for S11:\n",
      "  S_deemb(1,1)_real:\n",
      "    RMSE: 0.096536\n",
      "    R²: 0.974789\n",
      "    MAE: 0.053960\n",
      "    MAPE: 153.10%\n",
      "  S_deemb(1,1)_imag:\n",
      "    RMSE: 0.050858\n",
      "    R²: 0.965046\n",
      "    MAE: 0.031263\n",
      "    MAPE: 23.48%\n",
      "\n",
      "Average metrics for S11:\n",
      "  R²: 0.969917\n",
      "  RMSE: 0.073697\n",
      "  MAE: 0.042611\n",
      "  MAPE: 88.29%\n",
      "\n",
      "Total training time: 168.92 seconds\n",
      "Models and results saved to freq_aware_results/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['freq_aware_results/all_scalers.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [256, 512, 1024, 512],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S11\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(1,2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Identified 15 frequency-related features and 6 other features\n",
      "\n",
      "==================================================\n",
      "Training frequency-aware model for S12\n",
      "==================================================\n",
      "Applied StandardScaler to Y values for S12\n",
      "Epoch 10/300, Train Loss: 0.083003, Val Loss: 0.071906, LR: 0.00200000\n",
      "Epoch 20/300, Train Loss: 0.079795, Val Loss: 0.070786, LR: 0.00170000\n",
      "Epoch 30/300, Train Loss: 0.078014, Val Loss: 0.069310, LR: 0.00144500\n",
      "Epoch 40/300, Train Loss: 0.077573, Val Loss: 0.073279, LR: 0.00122825\n",
      "Epoch 50/300, Train Loss: 0.077067, Val Loss: 0.067784, LR: 0.00088741\n",
      "Epoch 60/300, Train Loss: 0.075787, Val Loss: 0.067225, LR: 0.00088741\n",
      "Epoch 70/300, Train Loss: 0.075418, Val Loss: 0.067471, LR: 0.00064115\n",
      "Epoch 80/300, Train Loss: 0.075541, Val Loss: 0.066763, LR: 0.00054498\n",
      "Epoch 90/300, Train Loss: 0.075101, Val Loss: 0.068795, LR: 0.00054498\n",
      "Epoch 100/300, Train Loss: 0.074955, Val Loss: 0.067769, LR: 0.00046323\n",
      "Epoch 110/300, Train Loss: 0.074161, Val Loss: 0.066910, LR: 0.00033469\n",
      "Epoch 120/300, Train Loss: 0.074433, Val Loss: 0.066592, LR: 0.00024181\n",
      "Epoch 130/300, Train Loss: 0.073858, Val Loss: 0.066290, LR: 0.00024181\n",
      "Epoch 140/300, Train Loss: 0.073598, Val Loss: 0.066312, LR: 0.00020554\n",
      "Epoch 150/300, Train Loss: 0.074111, Val Loss: 0.066203, LR: 0.00017471\n",
      "Epoch 160/300, Train Loss: 0.073878, Val Loss: 0.066435, LR: 0.00012623\n",
      "Epoch 170/300, Train Loss: 0.073831, Val Loss: 0.066431, LR: 0.00010729\n",
      "Epoch 180/300, Train Loss: 0.073764, Val Loss: 0.066189, LR: 0.00009120\n",
      "Epoch 190/300, Train Loss: 0.073723, Val Loss: 0.066287, LR: 0.00007752\n",
      "Epoch 200/300, Train Loss: 0.073579, Val Loss: 0.066128, LR: 0.00005601\n",
      "Epoch 210/300, Train Loss: 0.073542, Val Loss: 0.066125, LR: 0.00004047\n",
      "Epoch 220/300, Train Loss: 0.073280, Val Loss: 0.066085, LR: 0.00003440\n",
      "Epoch 230/300, Train Loss: 0.073111, Val Loss: 0.066147, LR: 0.00002485\n",
      "Epoch 240/300, Train Loss: 0.073580, Val Loss: 0.066255, LR: 0.00001795\n",
      "Early stopping at epoch 241\n",
      "\n",
      "Performance metrics for S12:\n",
      "  S_deemb(1,2)_real:\n",
      "    RMSE: 0.031139\n",
      "    R²: 0.928663\n",
      "    MAE: 0.013735\n",
      "    SMAPE: 28.56%\n",
      "  S_deemb(1,2)_imag:\n",
      "    RMSE: 0.020586\n",
      "    R²: 0.937461\n",
      "    MAE: 0.010712\n",
      "    SMAPE: 21.48%\n",
      "\n",
      "Average metrics for S12:\n",
      "  R²: 0.933062\n",
      "  RMSE: 0.025863\n",
      "  MAE: 0.012224\n",
      "  SMAPE: 25.02%\n",
      "\n",
      "Total training time: 234.71 seconds\n",
      "Models and results saved to freq_aware_results/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['freq_aware_results/all_scalers.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [384, 768, 1536, 768, 384],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S12\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(2,1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Identified 15 frequency-related features and 6 other features\n",
      "\n",
      "==================================================\n",
      "Training frequency-aware model for S21\n",
      "==================================================\n",
      "Epoch 10/200, Train Loss: 0.572542, Val Loss: 0.491979, LR: 0.00200000\n",
      "Epoch 20/200, Train Loss: 0.491452, Val Loss: 0.390267, LR: 0.00200000\n",
      "Epoch 30/200, Train Loss: 0.448655, Val Loss: 5.632973, LR: 0.00200000\n",
      "Epoch 40/200, Train Loss: 0.443983, Val Loss: 0.370182, LR: 0.00170000\n",
      "Epoch 50/200, Train Loss: 0.387322, Val Loss: 0.387145, LR: 0.00144500\n",
      "Epoch 60/200, Train Loss: 0.378890, Val Loss: 0.413280, LR: 0.00144500\n",
      "Epoch 70/200, Train Loss: 0.360920, Val Loss: 0.341764, LR: 0.00104401\n",
      "Epoch 80/200, Train Loss: 0.353688, Val Loss: 0.367151, LR: 0.00104401\n",
      "Epoch 90/200, Train Loss: 0.352128, Val Loss: 0.335613, LR: 0.00088741\n",
      "Epoch 100/200, Train Loss: 0.349183, Val Loss: 0.339924, LR: 0.00064115\n",
      "Epoch 110/200, Train Loss: 0.335221, Val Loss: 0.303835, LR: 0.00054498\n",
      "Epoch 120/200, Train Loss: 0.329720, Val Loss: 0.333137, LR: 0.00046323\n",
      "Epoch 130/200, Train Loss: 0.328165, Val Loss: 0.311658, LR: 0.00039375\n",
      "Epoch 140/200, Train Loss: 0.323303, Val Loss: 0.308199, LR: 0.00033469\n",
      "Epoch 150/200, Train Loss: 0.322347, Val Loss: 0.315513, LR: 0.00028448\n",
      "Epoch 160/200, Train Loss: 0.323557, Val Loss: 0.317878, LR: 0.00020554\n",
      "Epoch 170/200, Train Loss: 0.317168, Val Loss: 0.298245, LR: 0.00017471\n",
      "Epoch 180/200, Train Loss: 0.314854, Val Loss: 0.295877, LR: 0.00012623\n",
      "Early stopping at epoch 182\n",
      "\n",
      "Performance metrics for S21:\n",
      "  S_deemb(2,1)_real:\n",
      "    RMSE: 0.654196\n",
      "    R²: 0.975982\n",
      "    MAE: 0.175562\n",
      "    MAPE: 4102.12%\n",
      "  S_deemb(2,1)_imag:\n",
      "    RMSE: 0.415885\n",
      "    R²: 0.965996\n",
      "    MAE: 0.126722\n",
      "    MAPE: 112.72%\n",
      "\n",
      "Average metrics for S21:\n",
      "  R²: 0.970989\n",
      "  RMSE: 0.535041\n",
      "  MAE: 0.151142\n",
      "  MAPE: 2107.42%\n",
      "\n",
      "Total training time: 193.67 seconds\n",
      "Models and results saved to freq_aware_results/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['freq_aware_results/all_scalers.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [1024, 2048, 2048, 1024],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S11\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For S(2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Identified 15 frequency-related features and 6 other features\n",
      "\n",
      "==================================================\n",
      "Training frequency-aware model for S22\n",
      "==================================================\n",
      "Epoch 10/200, Train Loss: 0.007110, Val Loss: 0.006922, LR: 0.00200000\n",
      "Epoch 20/200, Train Loss: 0.005822, Val Loss: 0.005030, LR: 0.00200000\n",
      "Epoch 30/200, Train Loss: 0.005383, Val Loss: 0.005435, LR: 0.00170000\n",
      "Epoch 40/200, Train Loss: 0.004940, Val Loss: 0.004581, LR: 0.00144500\n",
      "Epoch 50/200, Train Loss: 0.004763, Val Loss: 0.004272, LR: 0.00144500\n",
      "Epoch 60/200, Train Loss: 0.004666, Val Loss: 0.004687, LR: 0.00104401\n",
      "Epoch 70/200, Train Loss: 0.004576, Val Loss: 0.004091, LR: 0.00088741\n",
      "Epoch 80/200, Train Loss: 0.004447, Val Loss: 0.003861, LR: 0.00075430\n",
      "Epoch 90/200, Train Loss: 0.004437, Val Loss: 0.003941, LR: 0.00054498\n",
      "Epoch 100/200, Train Loss: 0.004373, Val Loss: 0.003898, LR: 0.00046323\n",
      "Epoch 110/200, Train Loss: 0.004296, Val Loss: 0.003755, LR: 0.00039375\n",
      "Epoch 120/200, Train Loss: 0.004243, Val Loss: 0.003723, LR: 0.00028448\n",
      "Epoch 130/200, Train Loss: 0.004231, Val Loss: 0.003805, LR: 0.00024181\n",
      "Epoch 140/200, Train Loss: 0.004236, Val Loss: 0.003828, LR: 0.00024181\n",
      "Epoch 150/200, Train Loss: 0.004226, Val Loss: 0.003703, LR: 0.00020554\n",
      "Epoch 160/200, Train Loss: 0.004174, Val Loss: 0.003696, LR: 0.00017471\n",
      "Epoch 170/200, Train Loss: 0.004132, Val Loss: 0.003684, LR: 0.00014850\n",
      "Epoch 180/200, Train Loss: 0.004113, Val Loss: 0.003690, LR: 0.00012623\n",
      "Epoch 190/200, Train Loss: 0.004085, Val Loss: 0.003643, LR: 0.00010729\n",
      "Epoch 200/200, Train Loss: 0.004089, Val Loss: 0.003636, LR: 0.00009120\n",
      "\n",
      "Performance metrics for S22:\n",
      "  S_deemb(2,2)_real:\n",
      "    RMSE: 0.076050\n",
      "    R²: 0.969359\n",
      "    MAE: 0.038662\n",
      "    MAPE: 22.64%\n",
      "  S_deemb(2,2)_imag:\n",
      "    RMSE: 0.038587\n",
      "    R²: 0.959097\n",
      "    MAE: 0.022081\n",
      "    MAPE: 49.40%\n",
      "\n",
      "Average metrics for S22:\n",
      "  R²: 0.964228\n",
      "  RMSE: 0.057319\n",
      "  MAE: 0.030372\n",
      "  MAPE: 36.02%\n",
      "\n",
      "Total training time: 222.35 seconds\n",
      "Models and results saved to freq_aware_results/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['freq_aware_results/all_scalers.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [1024, 1536, 2048, 1536, 1024],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S11\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
