{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "file_path = \"combined_spar_data_full_parameters_split.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define features and labels\n",
    "# Features - based on your provided images (excluding ib and ic which are now labels)\n",
    "feature_columns = [\n",
    "    \"freq\",  # Frequency\n",
    "    \"vb\",\n",
    "    \"vc\",  # Voltage parameters\n",
    "    \"DEV_GEOM_L\",\n",
    "    \"NUM_OF_TRANS_RF\",  # Device geometry\n",
    "    # \"gm\",\n",
    "    # \"Cpi\",\n",
    "    # \"Cmu\",  # Device parameters\n",
    "    # \"Zin_real\",\n",
    "    # \"Zin_imag\",\n",
    "    # \"Zout_real\",  # Impedance components\n",
    "]\n",
    "\n",
    "# Labels - de-embedded S-parameters\n",
    "\n",
    "s_parameter_labels = [\n",
    "    \"S_deemb(1,1)_real\",\n",
    "    \"S_deemb(1,1)_imag\",\n",
    "    \"S_deemb(1,2)_real\",\n",
    "    \"S_deemb(1,2)_imag\",\n",
    "    \"S_deemb(2,1)_real\",\n",
    "    \"S_deemb(2,1)_imag\",\n",
    "    \"S_deemb(2,2)_real\",\n",
    "    \"S_deemb(2,2)_imag\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check for null values in both features and labels\n",
    "print(\"Checking for null values in features:\")\n",
    "feature_nulls = df[feature_columns].isnull().sum()\n",
    "print(feature_nulls[feature_nulls > 0])  # Only show features with nulls\n",
    "\n",
    "print(\"\\nChecking for null values in labels:\")\n",
    "label_nulls = df[s_parameter_labels].isnull().sum()\n",
    "print(label_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter rows with any null values in features or labels\n",
    "df_clean = df.dropna(subset=feature_columns + s_parameter_labels)\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows with null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create separate dataframes for features and labels\n",
    "X = df_clean[feature_columns].copy()\n",
    "Y = df_clean[s_parameter_labels].copy()\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f\"\\nFeature dataset shape: {X.shape}\")\n",
    "print(f\"S-parameter labels shape: {Y.shape}\")\n",
    "\n",
    "# Step 6: Basic statistics for all datasets\n",
    "print(\"\\nFeature statistics (first 5 columns):\")\n",
    "print(X.iloc[:, :5].describe())\n",
    "\n",
    "\n",
    "print(\"\\nS-parameter statistics (first 4 columns):\")\n",
    "print(Y.iloc[:, :4].describe())\n",
    "\n",
    "# Optional: Save cleaned datasets to files\n",
    "# X.to_csv(\"hbt_features.csv\", index=False)\n",
    "# Y.to_csv(\"hbt_sparam_labels.csv\", index=False)\n",
    "\n",
    "print(\"\\nFeature and label separation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_vs_label_correlations(X, y, target_names, filename):\n",
    "    \"\"\"Create a heatmap of correlations between features and labels\"\"\"\n",
    "    # Calculate correlations\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    correlation = combined.corr()\n",
    "\n",
    "    # Extract only the correlations between features and labels\n",
    "    feature_target_corr = correlation.loc[X.columns, target_names]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(feature_target_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Feature-Target Correlations\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return feature_target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations for selected S-parameters (using just S11 as example)\n",
    "s11_labels = [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"]\n",
    "s11_corr = plot_feature_vs_label_correlations(\n",
    "    X, Y[s11_labels], s11_labels, \"s11_correlations.png\"\n",
    ")\n",
    "print(\"\\nTop 5 features correlated with S11 parameters:\")\n",
    "for label in s11_labels:\n",
    "    top_features = s11_corr[label].abs().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop features for {label}:\")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_based_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a train-test split where:\n",
    "    1. No two consecutive frequency values are in the test set\n",
    "    2. Test set frequencies are evenly distributed across frequency bands\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing a 'freq' column\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of unique frequency values to include in test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_mask : numpy array\n",
    "        Boolean mask for training data\n",
    "    test_mask : numpy array\n",
    "        Boolean mask for test data\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Get sorted unique frequency values\n",
    "    unique_freqs = np.sort(df[\"freq\"].unique())\n",
    "    n_freqs = len(unique_freqs)\n",
    "    print(f\"Found {n_freqs} unique frequency values\")\n",
    "\n",
    "    # Define band boundaries\n",
    "    band_boundaries = [\n",
    "        (0, 1e9),  # Band 1: < 1 GHz\n",
    "        (1e9, 6e9),  # Band 2: 1-6 GHz\n",
    "        (6e9, 20e9),  # Band 3: 6-20 GHz\n",
    "        (20e9, 40e9),  # Band 4: 20-40 GHz\n",
    "        (40e9, float(\"inf\")),  # Band 5: > 40 GHz\n",
    "    ]\n",
    "\n",
    "    # Assign frequencies to bands\n",
    "    freq_bands = np.zeros(n_freqs, dtype=int)\n",
    "\n",
    "    for i, freq in enumerate(unique_freqs):\n",
    "        for band_idx, (lower, upper) in enumerate(band_boundaries):\n",
    "            if lower <= freq < upper or (band_idx == 4 and freq >= lower):\n",
    "                freq_bands[i] = band_idx\n",
    "                break\n",
    "\n",
    "    # Count frequencies in each band\n",
    "    band_counts = np.zeros(5, dtype=int)\n",
    "    for band in freq_bands:\n",
    "        band_counts[band] += 1\n",
    "\n",
    "    for band_idx, count in enumerate(band_counts):\n",
    "        print(f\"Band {band_idx + 1}: {count} frequency values\")\n",
    "\n",
    "    # Simple but effective approach: select every k-th frequency as test set\n",
    "    # This guarantees no consecutive frequencies in test set\n",
    "    k = int(1 / test_size)  # If test_size is 0.2, k=5 means select every 5th frequency\n",
    "\n",
    "    # Start with a base selection\n",
    "    test_indices = np.arange(0, n_freqs, k)\n",
    "    print(f\"Base selection gives {len(test_indices)} test frequencies (every {k}th)\")\n",
    "\n",
    "    # Calculate target test frequencies per band\n",
    "    target_per_band = np.zeros(5, dtype=int)\n",
    "    for i, count in enumerate(band_counts):\n",
    "        target_per_band[i] = max(1, int(round(count * test_size)))\n",
    "\n",
    "    print(\"Target test frequencies per band:\")\n",
    "    for i, target in enumerate(target_per_band):\n",
    "        print(f\"Band {i + 1}: {target}\")\n",
    "\n",
    "    # Calculate how many frequencies we actually selected per band\n",
    "    actual_per_band = np.zeros(5, dtype=int)\n",
    "    for idx in test_indices:\n",
    "        band = freq_bands[idx]\n",
    "        actual_per_band[band] += 1\n",
    "\n",
    "    print(\"Actual initial test frequencies per band:\")\n",
    "    for i, actual in enumerate(actual_per_band):\n",
    "        print(f\"Band {i + 1}: {actual}\")\n",
    "\n",
    "    # Adjust selection to better match target distribution\n",
    "    # First, identify bands that need more frequencies\n",
    "    for band in range(5):\n",
    "        if actual_per_band[band] < target_per_band[band]:\n",
    "            # Get candidate indices in this band that aren't already selected\n",
    "            band_candidates = [\n",
    "                i\n",
    "                for i in range(n_freqs)\n",
    "                if freq_bands[i] == band\n",
    "                and i not in test_indices\n",
    "                and i - 1 not in test_indices\n",
    "                and i + 1 not in test_indices\n",
    "            ]\n",
    "\n",
    "            # How many more do we need?\n",
    "            n_needed = target_per_band[band] - actual_per_band[band]\n",
    "\n",
    "            # Select additional frequencies if we have enough candidates\n",
    "            if len(band_candidates) >= n_needed:\n",
    "                # Choose candidates with roughly equal spacing\n",
    "                step = max(1, len(band_candidates) // n_needed)\n",
    "                selected = band_candidates[::step][:n_needed]\n",
    "                test_indices = np.append(test_indices, selected)\n",
    "                actual_per_band[band] += len(selected)\n",
    "\n",
    "    # If we over-selected in some bands, remove frequencies to match target\n",
    "    for band in range(5):\n",
    "        if actual_per_band[band] > target_per_band[band]:\n",
    "            # How many to remove\n",
    "            n_remove = actual_per_band[band] - target_per_band[band]\n",
    "\n",
    "            # Get indices in this band that were selected\n",
    "            band_selected = [i for i in test_indices if freq_bands[i] == band]\n",
    "\n",
    "            # Choose which ones to remove (spaced out)\n",
    "            if band_selected:\n",
    "                step = max(1, len(band_selected) // n_remove)\n",
    "                to_remove = band_selected[::step][:n_remove]\n",
    "                test_indices = np.array([i for i in test_indices if i not in to_remove])\n",
    "                actual_per_band[band] -= len(to_remove)\n",
    "\n",
    "    print(\"Final test frequencies per band after adjustment:\")\n",
    "    for i, actual in enumerate(actual_per_band):\n",
    "        print(f\"Band {i + 1}: {actual} (target: {target_per_band[i]})\")\n",
    "\n",
    "    # Sort the indices\n",
    "    test_indices = np.sort(test_indices)\n",
    "\n",
    "    # Verify that no consecutive frequencies are in test set\n",
    "    for i in range(len(test_indices) - 1):\n",
    "        if test_indices[i + 1] - test_indices[i] == 1:\n",
    "            print(\n",
    "                f\"WARNING: Consecutive frequencies in test set: {unique_freqs[test_indices[i]]} and {unique_freqs[test_indices[i + 1]]}\"\n",
    "            )\n",
    "\n",
    "    # Create test frequencies set\n",
    "    test_freqs = unique_freqs[test_indices]\n",
    "\n",
    "    # Create train and test masks\n",
    "    test_mask = df[\"freq\"].isin(test_freqs)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    print(f\"Final training set: {train_mask.sum()} samples\")\n",
    "    print(f\"Final test set: {test_mask.sum()} samples\")\n",
    "\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_frequency_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create an improved frequency-based train-test split that ensures:\n",
    "    1. No frequency overlap between train and test\n",
    "    2. Even distribution of device parameters between train and test\n",
    "    3. Balanced representation of different device geometries and parameters\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing dataset with 'freq' and device parameters\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of unique frequency values to include in test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_mask, test_mask : numpy arrays\n",
    "        Boolean masks for train and test data\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Get sorted unique frequency values\n",
    "    unique_freqs = np.sort(df[\"freq\"].unique())\n",
    "    n_freqs = len(unique_freqs)\n",
    "\n",
    "    # Define frequency bands\n",
    "    band_boundaries = [\n",
    "        (0, 1e9),  # < 1 GHz\n",
    "        (1e9, 6e9),  # 1-6 GHz\n",
    "        (6e9, 20e9),  # 6-20 GHz\n",
    "        (20e9, 40e9),  # 20-40 GHz\n",
    "        (40e9, float(\"inf\")),  # > 40 GHz\n",
    "    ]\n",
    "\n",
    "    # Assign frequencies to bands\n",
    "    freq_bands = {}\n",
    "    band_freqs = {i: [] for i in range(len(band_boundaries))}\n",
    "\n",
    "    for freq in unique_freqs:\n",
    "        for band_idx, (lower, upper) in enumerate(band_boundaries):\n",
    "            if lower <= freq < upper or (\n",
    "                band_idx == len(band_boundaries) - 1 and freq >= lower\n",
    "            ):\n",
    "                freq_bands[freq] = band_idx\n",
    "                band_freqs[band_idx].append(freq)\n",
    "                break\n",
    "\n",
    "    # Select test frequencies ensuring no consecutive frequencies\n",
    "    test_freqs = []\n",
    "\n",
    "    # Calculate target number of test frequencies per band\n",
    "    target_per_band = {\n",
    "        band: max(1, int(len(freqs) * test_size)) for band, freqs in band_freqs.items()\n",
    "    }\n",
    "\n",
    "    # Randomly select frequencies from each band\n",
    "    for band, freqs in band_freqs.items():\n",
    "        if len(freqs) > 0:\n",
    "            # Sort frequencies within band\n",
    "            sorted_freqs = np.sort(freqs)\n",
    "\n",
    "            # Select frequencies with spacing to avoid consecutive selections\n",
    "            n_select = target_per_band[band]\n",
    "            step = max(1, len(sorted_freqs) // (n_select + 1))\n",
    "\n",
    "            # Jitter indices to avoid selecting frequencies at exact intervals\n",
    "            indices = np.arange(step, len(sorted_freqs), step)[:n_select]\n",
    "            indices = np.clip(\n",
    "                indices + np.random.randint(-step // 4, step // 4, size=len(indices)),\n",
    "                0,\n",
    "                len(sorted_freqs) - 1,\n",
    "            )\n",
    "\n",
    "            # Ensure unique indices\n",
    "            indices = np.unique(indices)\n",
    "            selected_freqs = sorted_freqs[indices]\n",
    "\n",
    "            test_freqs.extend(selected_freqs)\n",
    "\n",
    "    # Create train and test masks\n",
    "    test_mask = df[\"freq\"].isin(test_freqs)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    # Check for balanced distribution of device parameters\n",
    "    dev_params = [\"DEV_GEOM_L\", \"NUM_OF_TRANS_RF\", \"vb\", \"vc\"]\n",
    "    for param in dev_params:\n",
    "        if param in df.columns:\n",
    "            train_dist = df.loc[train_mask, param].value_counts(normalize=True)\n",
    "            test_dist = df.loc[test_mask, param].value_counts(normalize=True)\n",
    "\n",
    "            # If distributions are very different, adjust selection\n",
    "            if np.abs(train_dist.values - test_dist.values).max() > 0.2:\n",
    "                print(\n",
    "                    f\"Warning: Unbalanced distribution detected for {param}. Adjusting split...\"\n",
    "                )\n",
    "                # This could be expanded with a rebalancing algorithm\n",
    "\n",
    "    # Verify no frequency overlap\n",
    "    train_freqs = df.loc[train_mask, \"freq\"].unique()\n",
    "    test_freqs = df.loc[test_mask, \"freq\"].unique()\n",
    "    overlap = np.intersect1d(train_freqs, test_freqs)\n",
    "    assert len(overlap) == 0, \"Frequency overlap detected in split!\"\n",
    "\n",
    "    print(f\"Train set: {train_mask.sum()} samples ({train_mask.mean():.2%})\")\n",
    "    print(f\"Test set: {test_mask.sum()} samples ({test_mask.mean():.2%})\")\n",
    "\n",
    "    return train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your current train-test split with the frequency-based approach\n",
    "train_mask, test_mask = create_frequency_based_split(\n",
    "    df_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use the masks to split features and labels\n",
    "X_raw_train = X[train_mask].copy()\n",
    "X_raw_test = X[test_mask].copy()\n",
    "Y_raw_train = Y[train_mask].copy()\n",
    "Y_raw_test = Y[test_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train[\"vb_is_zero\"] = (X_train[\"vb\"] == 0).astype(int)\n",
    "X_train[\"vb_is_high\"] = ((X_train[\"vb\"] >= 0.7) & (X_train[\"vb\"] <= 0.9)).astype(int)\n",
    "X_train[\"vc_is_zero\"] = (X_train[\"vc\"] == 0).astype(int)\n",
    "X_train[\"vc_is_1_2V\"] = ((X_train[\"vc\"] >= 1.1) & (X_train[\"vc\"] <= 1.3)).astype(int)\n",
    "X_train[\"vc_is_1_5V\"] = ((X_train[\"vc\"] >= 1.4) & (X_train[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# For test data\n",
    "X_test = X_raw_test.copy()\n",
    "X_test[\"vb_is_zero\"] = (X_test[\"vb\"] == 0).astype(int)\n",
    "X_test[\"vb_is_high\"] = ((X_test[\"vb\"] >= 0.7) & (X_test[\"vb\"] <= 0.9)).astype(int)\n",
    "X_test[\"vc_is_zero\"] = (X_test[\"vc\"] == 0).astype(int)\n",
    "X_test[\"vc_is_1_2V\"] = ((X_test[\"vc\"] >= 1.1) & (X_test[\"vc\"] <= 1.3)).astype(int)\n",
    "X_test[\"vc_is_1_5V\"] = ((X_test[\"vc\"] >= 1.4) & (X_test[\"vc\"] <= 1.6)).astype(int)\n",
    "\n",
    "# STEP 3: Initialize and fit scaler ONLY on training data\n",
    "voltage_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "voltage_scaler.fit(X_train[[\"vb\", \"vc\"]])  # Fit only on training data\n",
    "\n",
    "# STEP 4: Transform both datasets using the fitted scaler\n",
    "X_train[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_train[[\"vb\", \"vc\"]])\n",
    "X_test[[\"vb\", \"vc\"]] = voltage_scaler.transform(X_test[[\"vb\", \"vc\"]])\n",
    "\n",
    "# STEP 5: Save the scaler for future use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(voltage_scaler, \"voltage_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"DEV_L_0_9um\"] = (X_train[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_train.loc[:, \"DEV_L_2_5um\"] = (X_train[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_train.loc[:, \"DEV_L_5_0um\"] = (X_train[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"DEV_GEOM_L\", axis=1)\n",
    "\n",
    "# Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"DEV_L_0_9um\"] = (X_test[\"DEV_GEOM_L\"] == 0.9).astype(int)\n",
    "X_test.loc[:, \"DEV_L_2_5um\"] = (X_test[\"DEV_GEOM_L\"] == 2.5).astype(int)\n",
    "X_test.loc[:, \"DEV_L_5_0um\"] = (X_test[\"DEV_GEOM_L\"] == 5.0).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"DEV_GEOM_L\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "X_train.loc[:, \"TRANS_1\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_train.loc[:, \"TRANS_2\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_train.loc[:, \"TRANS_4\"] = (X_train[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from training data\n",
    "X_train = X_train.drop(\"NUM_OF_TRANS_RF\", axis=1)\n",
    "\n",
    "# STEP 3: Process test data with the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "X_test.loc[:, \"TRANS_1\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 1).astype(int)\n",
    "X_test.loc[:, \"TRANS_2\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 2).astype(int)\n",
    "X_test.loc[:, \"TRANS_4\"] = (X_test[\"NUM_OF_TRANS_RF\"] == 4).astype(int)\n",
    "\n",
    "# Drop the original column from test data\n",
    "X_test = X_test.drop(\"NUM_OF_TRANS_RF\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "\n",
    "# Create binary features for physically questionable negative values\n",
    "X_train.loc[:, \"gm_is_neg\"] = (X_train[\"gm\"] < 0).astype(int)\n",
    "X_train.loc[:, \"Cpi_is_neg\"] = (X_train[\"Cpi\"] < 0).astype(int)\n",
    "X_train.loc[:, \"Cmu_is_neg\"] = (X_train[\"Cmu\"] < 0).astype(int)\n",
    "\n",
    "# For highly concentrated distributions, use log transform on absolute values\n",
    "epsilon = 1e-20  # Small value to prevent log(0)\n",
    "X_train.loc[:, \"gm_abs_log\"] = np.log10(np.abs(X_train[\"gm\"]) + epsilon)\n",
    "X_train.loc[:, \"Cpi_abs_log\"] = np.log10(np.abs(X_train[\"Cpi\"]) + epsilon)\n",
    "X_train.loc[:, \"Cmu_abs_log\"] = np.log10(np.abs(X_train[\"Cmu\"]) + epsilon)\n",
    "\n",
    "# STEP 3: Fit the scaler ONLY on training data\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_train[[\"gm_abs_log\", \"Cpi_abs_log\", \"Cmu_abs_log\"]])\n",
    "\n",
    "# Apply the fitted scaler to training data\n",
    "X_train.loc[:, [\"gm_abs_log\", \"Cpi_abs_log\", \"Cmu_abs_log\"]] = robust_scaler.transform(\n",
    "    X_train[[\"gm_abs_log\", \"Cpi_abs_log\", \"Cmu_abs_log\"]]\n",
    ")\n",
    "\n",
    "# Drop original columns from training data\n",
    "X_train = X_train.drop([\"gm\", \"Cpi\", \"Cmu\"], axis=1)\n",
    "\n",
    "# STEP 4: Process test data using same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "# Create binary features for physically questionable negative values\n",
    "X_test.loc[:, \"gm_is_neg\"] = (X_test[\"gm\"] < 0).astype(int)\n",
    "X_test.loc[:, \"Cpi_is_neg\"] = (X_test[\"Cpi\"] < 0).astype(int)\n",
    "X_test.loc[:, \"Cmu_is_neg\"] = (X_test[\"Cmu\"] < 0).astype(int)\n",
    "\n",
    "# Apply same log transform with same epsilon\n",
    "X_test.loc[:, \"gm_abs_log\"] = np.log10(np.abs(X_test[\"gm\"]) + epsilon)\n",
    "X_test.loc[:, \"Cpi_abs_log\"] = np.log10(np.abs(X_test[\"Cpi\"]) + epsilon)\n",
    "X_test.loc[:, \"Cmu_abs_log\"] = np.log10(np.abs(X_test[\"Cmu\"]) + epsilon)\n",
    "\n",
    "# Apply the previously fitted scaler to test data\n",
    "X_test.loc[:, [\"gm_abs_log\", \"Cpi_abs_log\", \"Cmu_abs_log\"]] = robust_scaler.transform(\n",
    "    X_test[[\"gm_abs_log\", \"Cpi_abs_log\", \"Cmu_abs_log\"]]\n",
    ")\n",
    "\n",
    "# Drop original columns from test data\n",
    "X_test = X_test.drop([\"gm\", \"Cpi\", \"Cmu\"], axis=1)\n",
    "\n",
    "# STEP 5: Save the scaler for future use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(robust_scaler, \"device_params_robust_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation function\n",
    "def signed_log_transform(x, epsilon=1e-20):\n",
    "    \"\"\"Apply log transform that preserves sign of original value\"\"\"\n",
    "    return np.sign(x) * np.log1p(np.abs(x) + epsilon)\n",
    "\n",
    "\n",
    "# STEP 2: Process training data\n",
    "X_train = X_raw_train.copy()\n",
    "\n",
    "# Create binary indicators for physically significant states\n",
    "X_train[\"Zin_real_negative\"] = (X_train[\"Zin_real\"] < 0).astype(int)\n",
    "X_train[\"Zout_real_negative\"] = (X_train[\"Zout_real\"] < 0).astype(int)\n",
    "\n",
    "# Apply signed log transformation to handle extreme values while preserving sign\n",
    "for col in [\"Zin_real\", \"Zin_imag\", \"Zout_real\"]:\n",
    "    X_train[f\"{col}_log\"] = signed_log_transform(X_train[col])\n",
    "\n",
    "# Initialize and fit scaler ONLY on training data\n",
    "impedance_scaler = RobustScaler()\n",
    "log_cols = [col + \"_log\" for col in [\"Zin_real\", \"Zin_imag\", \"Zout_real\"]]\n",
    "impedance_scaler.fit(X_train[log_cols])\n",
    "\n",
    "# Apply the fitted scaler to training data\n",
    "X_train[log_cols] = impedance_scaler.transform(X_train[log_cols])\n",
    "\n",
    "# Create interaction feature\n",
    "X_train[\"Zin_real_imag_interaction\"] = X_train[\"Zin_real_log\"] * X_train[\"Zin_imag_log\"]\n",
    "\n",
    "# Drop original features from training data\n",
    "X_train = X_train.drop([\"Zin_real\", \"Zin_imag\", \"Zout_real\"], axis=1)\n",
    "\n",
    "# STEP 3: Process test data using the same transformations\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "# Create binary indicators for test data\n",
    "X_test[\"Zin_real_negative\"] = (X_test[\"Zin_real\"] < 0).astype(int)\n",
    "X_test[\"Zout_real_negative\"] = (X_test[\"Zout_real\"] < 0).astype(int)\n",
    "\n",
    "# Apply same log transformation with same epsilon\n",
    "for col in [\"Zin_real\", \"Zin_imag\", \"Zout_real\"]:\n",
    "    X_test[f\"{col}_log\"] = signed_log_transform(X_test[col])\n",
    "\n",
    "# Apply the PREVIOUSLY FITTED scaler to test data\n",
    "X_test[log_cols] = impedance_scaler.transform(X_test[log_cols])\n",
    "\n",
    "# Create interaction feature for test data\n",
    "X_test[\"Zin_real_imag_interaction\"] = X_test[\"Zin_real_log\"] * X_test[\"Zin_imag_log\"]\n",
    "\n",
    "# Drop original features from test data\n",
    "X_test = X_test.drop([\"Zin_real\", \"Zin_imag\", \"Zout_real\"], axis=1)\n",
    "\n",
    "# STEP 4: Save the scaler for future use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(impedance_scaler, \"impedance_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import frequency_preprocessing\n",
    "\n",
    "importlib.reload(frequency_preprocessing)\n",
    "from frequency_preprocessing import preprocess_frequency\n",
    "\n",
    "# Then try using it\n",
    "X_train, X_test = preprocess_frequency(X_train, X_test, fit_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 for freq_pos_in_band columns\n",
    "for i in range(1, 6):\n",
    "    X_train[f\"freq_pos_in_band_{i}\"] = X_train[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "    if X_test is not None:\n",
    "        X_test[f\"freq_pos_in_band_{i}\"] = X_test[f\"freq_pos_in_band_{i}\"].fillna(0)\n",
    "\n",
    "# Fill any remaining NaN values in other columns\n",
    "X_train = X_train.fillna(0)\n",
    "if X_test is not None:\n",
    "    X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 4 S-parameter pairs (each will be a separate model)\n",
    "s_parameter_models = {\n",
    "    \"S11\": [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"],\n",
    "    \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "    \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "    \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "}\n",
    "\n",
    "# Dictionary to store best features for each model\n",
    "best_features = {}\n",
    "importances_by_model = {}\n",
    "\n",
    "# Analyze each S-parameter model independently\n",
    "for model_name, components in s_parameter_models.items():\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Analyzing feature importance for {model_name} model\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # Get targets for this S-parameter model (both real and imaginary)\n",
    "    Y_model = Y_raw_train[components]\n",
    "\n",
    "    # Train a model for each component to get feature importance\n",
    "    importances = {}\n",
    "\n",
    "    # Calculate importance scores for both components and combine them\n",
    "    for component in components:\n",
    "        print(f\"Training model for {component}...\")\n",
    "\n",
    "        # Train RF model for this component\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, Y_raw_train[component])\n",
    "\n",
    "        # Get feature importances\n",
    "        for feature, importance in zip(X_train.columns, rf.feature_importances_):\n",
    "            if feature in importances:\n",
    "                # Average importance across components\n",
    "                importances[feature] = (importances[feature] + importance) / 2\n",
    "            else:\n",
    "                importances[feature] = importance\n",
    "\n",
    "    # Convert to DataFrame and sort\n",
    "    importance_df = pd.DataFrame(\n",
    "        {\"feature\": list(importances.keys()), \"importance\": list(importances.values())}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # Store the importance dataframe\n",
    "    importances_by_model[model_name] = importance_df\n",
    "\n",
    "    # Print top 30 features\n",
    "    print(f\"\\nTop 30 Features for {model_name}:\")\n",
    "    print(importance_df.head(30))\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.title(f\"Top 25 Features for {model_name} Model\")\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=importance_df.head(25))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"feature_importance_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot features by category\n",
    "    # Create a mapping of features to categories\n",
    "    feature_categories = {}\n",
    "    for feature in X_train.columns:\n",
    "        if \"freq_band_\" in feature or \"band_\" in feature:\n",
    "            feature_categories[feature] = \"Frequency Band\"\n",
    "        elif \"freq_\" in feature:\n",
    "            feature_categories[feature] = \"Frequency Feature\"\n",
    "        elif feature in [\"vb\", \"vc\"]:\n",
    "            feature_categories[feature] = \"Voltage\"\n",
    "        elif feature in [\"gm\", \"Cpi\", \"Cmu\"] or any(\n",
    "            x in feature for x in [\"gm_\", \"Cpi_\", \"Cmu_\"]\n",
    "        ):\n",
    "            feature_categories[feature] = \"Device Parameter\"\n",
    "        elif \"Zin_\" in feature or \"Zout_\" in feature:\n",
    "            feature_categories[feature] = \"Impedance\"\n",
    "        elif \"DEV_\" in feature or \"TRANS_\" in feature:\n",
    "            feature_categories[feature] = \"Device Geometry\"\n",
    "        else:\n",
    "            feature_categories[feature] = \"Other\"\n",
    "\n",
    "    # Add category column\n",
    "    importance_df[\"category\"] = importance_df[\"feature\"].map(feature_categories)\n",
    "\n",
    "    # Plot importance by category\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.suptitle(f\"Feature Importance by Category for {model_name}\", fontsize=16)\n",
    "\n",
    "    for i, cat in enumerate(sorted(importance_df[\"category\"].unique())):\n",
    "        # Get features in this category\n",
    "        cat_features = importance_df[importance_df[\"category\"] == cat]\n",
    "        if len(cat_features) > 0:\n",
    "            plt.subplot(len(importance_df[\"category\"].unique()), 1, i + 1)\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=cat_features.head(10))\n",
    "            plt.title(f\"{cat} Features\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
    "    plt.savefig(f\"feature_importance_by_category_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Evaluate baseline performance with all features\n",
    "    def evaluate_model(features, y_true_components):\n",
    "        # Create model\n",
    "        base_model = RandomForestRegressor(n_estimators=75, random_state=42, n_jobs=-1)\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "\n",
    "        # Train on specified features\n",
    "        model.fit(X_train[features], Y_raw_train[y_true_components])\n",
    "\n",
    "        # Predict\n",
    "        preds = model.predict(X_test[features])\n",
    "\n",
    "        # Calculate R² for each component\n",
    "        r2_scores = [\n",
    "            r2_score(Y_raw_test[comp], preds[:, i])\n",
    "            for i, comp in enumerate(y_true_components)\n",
    "        ]\n",
    "\n",
    "        return np.mean(r2_scores), r2_scores\n",
    "\n",
    "    # Evaluate performance with different feature sets\n",
    "    full_perf, full_component_perf = evaluate_model(X_train.columns, components)\n",
    "    print(f\"\\nFull model performance (all {len(X_train.columns)} features):\")\n",
    "    print(f\"  Average R²: {full_perf:.4f}\")\n",
    "    print(f\"  {components[0]} R²: {full_component_perf[0]:.4f}\")\n",
    "    print(f\"  {components[1]} R²: {full_component_perf[1]:.4f}\")\n",
    "\n",
    "    # Define different feature set sizes to test\n",
    "    feature_counts = [10, 15, 20, 25, 30, 38]\n",
    "    results = []\n",
    "\n",
    "    # Test performance with different feature set sizes\n",
    "    for n_features in feature_counts:\n",
    "        # Get top N features\n",
    "        top_features = importance_df.head(n_features)[\"feature\"].tolist()\n",
    "\n",
    "        # Evaluate\n",
    "        avg_perf, component_perf = evaluate_model(top_features, components)\n",
    "\n",
    "        # Save results\n",
    "        results.append(\n",
    "            {\n",
    "                \"n_features\": n_features,\n",
    "                \"avg_r2\": avg_perf,\n",
    "                \"real_r2\": component_perf[0],\n",
    "                \"imag_r2\": component_perf[1],\n",
    "                \"vs_full\": avg_perf - full_perf,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nPerformance with different feature set sizes:\")\n",
    "    print(results_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "    # Plot performance vs feature count\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[\"n_features\"], results_df[\"avg_r2\"], \"o-\", label=\"Average R²\")\n",
    "    plt.plot(\n",
    "        results_df[\"n_features\"],\n",
    "        results_df[\"real_r2\"],\n",
    "        \"s-\",\n",
    "        label=f\"{components[0]} R²\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        results_df[\"n_features\"],\n",
    "        results_df[\"imag_r2\"],\n",
    "        \"^-\",\n",
    "        label=f\"{components[1]} R²\",\n",
    "    )\n",
    "    plt.axhline(\n",
    "        y=full_perf,\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Full model ({len(X_train.columns)} features)\",\n",
    "    )\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"R² Score\")\n",
    "    plt.title(f\"Model Performance vs Feature Count for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"performance_vs_features_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Find optimal feature set\n",
    "    # Use the smallest feature set that achieves at least 99% of full model performance\n",
    "    target_performance = full_perf * 0.99\n",
    "    optimal_row = results_df[results_df[\"avg_r2\"] >= target_performance].iloc[0]\n",
    "    optimal_feature_count = int(optimal_row[\"n_features\"])\n",
    "\n",
    "    print(f\"\\nRecommended feature set for {model_name}:\")\n",
    "    print(f\"  Use top {optimal_feature_count} features\")\n",
    "    print(\n",
    "        f\"  Expected performance: {optimal_row['avg_r2']:.4f} R² \"\n",
    "        + f\"({optimal_row['avg_r2'] / full_perf * 100:.1f}% of full model)\"\n",
    "    )\n",
    "\n",
    "    # Store the best features\n",
    "    best_features[model_name] = importance_df.head(optimal_feature_count)[\n",
    "        \"feature\"\n",
    "    ].tolist()\n",
    "\n",
    "    # Print the specific features\n",
    "    print(\"\\nFeatures to use:\")\n",
    "    for i, feature in enumerate(best_features[model_name]):\n",
    "        print(f\"  {i + 1}. {feature}\")\n",
    "\n",
    "# Create a comparison of feature sets\n",
    "comparison = pd.DataFrame(index=set().union(*best_features.values()))\n",
    "\n",
    "# For each model, mark which features are used\n",
    "for model_name, features in best_features.items():\n",
    "    comparison[model_name] = [1 if f in features else 0 for f in comparison.index]\n",
    "\n",
    "# Add a \"used in X models\" column\n",
    "comparison[\"used_in_models\"] = comparison.sum(axis=1)\n",
    "comparison = comparison.sort_values(\"used_in_models\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature usage across models:\")\n",
    "print(comparison)\n",
    "\n",
    "# Create a visualization of feature overlap\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.title(\"Feature Usage Across S-Parameter Models\")\n",
    "sns.heatmap(\n",
    "    comparison.drop(\"used_in_models\", axis=1), cmap=\"YlGnBu\", annot=True, cbar=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_usage_across_models.png\")\n",
    "plt.close()\n",
    "\n",
    "# Print final summary of recommended feature sets\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RECOMMENDED FEATURE SETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in s_parameter_models.keys():\n",
    "    print(f\"\\n{model_name} Model: {len(best_features[model_name])} features\")\n",
    "    for feature in best_features[model_name]:\n",
    "        print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error with protection against division by zero.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual target values\n",
    "    y_pred : array-like\n",
    "        Predicted target values\n",
    "    epsilon : float, default=1e-10\n",
    "        Small constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mape : float\n",
    "        Mean Absolute Percentage Error value\n",
    "    \"\"\"\n",
    "    # Handle cases where y_true is close to zero\n",
    "    # We add epsilon to denominator to avoid division by zero\n",
    "    non_zero = np.abs(y_true) > epsilon\n",
    "\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan  # Return NaN if all values are too close to zero\n",
    "\n",
    "    # Calculate percentage errors only for non-zero values\n",
    "    percentage_errors = (\n",
    "        np.abs(\n",
    "            (y_true[non_zero] - y_pred[non_zero]) / (np.abs(y_true[non_zero]) + epsilon)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Return the mean\n",
    "    return np.mean(percentage_errors)\n",
    "\n",
    "\n",
    "def train_evaluate_linear_regression_baseline(\n",
    "    X_train, X_test, Y_train, Y_test, s_parameter_models\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate linear regression models for each S-parameter component.\n",
    "    Includes RMSE, R², MAE, and MAPE metrics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    s_parameter_models : dict\n",
    "        Dictionary mapping S-parameter names to their component columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with model performance metrics\n",
    "    models : dict\n",
    "        Dictionary with trained linear regression models\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    models = {}\n",
    "\n",
    "    # For comparison with your RF models\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Summary tables for each metric\n",
    "    summary_r2 = {}\n",
    "    summary_rmse = {}\n",
    "    summary_mae = {}\n",
    "    summary_mape = {}\n",
    "\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training linear regression for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Train separate models for real and imaginary components\n",
    "        model_results = {}\n",
    "        model_dict = {}\n",
    "\n",
    "        # Store metrics for this S-parameter\n",
    "        r2_values = []\n",
    "        rmse_values = []\n",
    "        mae_values = []\n",
    "        mape_values = []\n",
    "\n",
    "        for component in components:\n",
    "            print(f\"Training model for {component}...\")\n",
    "\n",
    "            # Create and train linear model\n",
    "            lr_model = LinearRegression()\n",
    "            lr_model.fit(X_train, Y_train[component])\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = lr_model.predict(X_test)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(Y_test[component], y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(Y_test[component], y_pred)\n",
    "            mae = mean_absolute_error(Y_test[component], y_pred)\n",
    "            mape = mean_absolute_percentage_error(Y_test[component].values, y_pred)\n",
    "\n",
    "            print(f\"  RMSE: {rmse:.6f}\")\n",
    "            print(f\"  R²: {r2:.6f}\")\n",
    "            print(f\"  MAE: {mae:.6f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "            # Store results\n",
    "            model_results[component] = {\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"mape\": mape,\n",
    "            }\n",
    "            model_dict[component] = lr_model\n",
    "\n",
    "            # Store for averaging\n",
    "            r2_values.append(r2)\n",
    "            rmse_values.append(rmse)\n",
    "            mae_values.append(mae)\n",
    "            mape_values.append(mape)\n",
    "\n",
    "            # Visualize predictions vs actual\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(Y_test[component], y_pred, alpha=0.3)\n",
    "            plt.plot(\n",
    "                [Y_test[component].min(), Y_test[component].max()],\n",
    "                [Y_test[component].min(), Y_test[component].max()],\n",
    "                \"r--\",\n",
    "            )\n",
    "            plt.xlabel(\"Actual\")\n",
    "            plt.ylabel(\"Predicted\")\n",
    "            plt.title(f\"Linear Regression: {component} Predictions vs Actual\")\n",
    "            plt.savefig(f\"linear_regression_{component}_pred_vs_actual.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # Calculate average metrics for this S-parameter\n",
    "        avg_r2 = np.mean(r2_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        avg_mae = np.mean(mae_values)\n",
    "        avg_mape = np.mean(mape_values)\n",
    "\n",
    "        print(f\"\\nAverage metrics for {model_name}:\")\n",
    "        print(f\"  R²: {avg_r2:.6f}\")\n",
    "        print(f\"  RMSE: {avg_rmse:.6f}\")\n",
    "        print(f\"  MAE: {avg_mae:.6f}\")\n",
    "        print(f\"  MAPE: {avg_mape:.2f}%\")\n",
    "\n",
    "        # Store for summary\n",
    "        summary_r2[model_name] = avg_r2\n",
    "        summary_rmse[model_name] = avg_rmse\n",
    "        summary_mae[model_name] = avg_mae\n",
    "        summary_mape[model_name] = avg_mape\n",
    "\n",
    "        # Store model and results\n",
    "        results[model_name] = model_results\n",
    "        models[model_name] = model_dict\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Print detailed summary table\n",
    "    print(\"\\nDetailed Performance Summary:\")\n",
    "    print(\n",
    "        f\"{'S-Parameter':<10} {'Component':<20} {'RMSE':<10} {'R²':<10} {'MAE':<10} {'MAPE':<10}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for model_name, model_results in results.items():\n",
    "        for component, metrics in model_results.items():\n",
    "            print(\n",
    "                f\"{model_name:<10} {component:<20} {metrics['rmse']:<10.6f} {metrics['r2']:<10.6f} {metrics['mae']:<10.6f} {metrics['mape']:<10.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Print overall summary table\n",
    "    print(\"\\nOverall S-Parameter Performance:\")\n",
    "    print(f\"{'S-Parameter':<10} {'R²':<10} {'RMSE':<10} {'MAE':<10} {'MAPE':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for model_name in s_parameter_models.keys():\n",
    "        print(\n",
    "            f\"{model_name:<10} {summary_r2[model_name]:<10.6f} {summary_rmse[model_name]:<10.6f} {summary_mae[model_name]:<10.6f} {summary_mape[model_name]:<10.2f}%\"\n",
    "        )\n",
    "\n",
    "    # Calculate and print overall average\n",
    "    overall_r2 = np.mean(list(summary_r2.values()))\n",
    "    overall_rmse = np.mean(list(summary_rmse.values()))\n",
    "    overall_mae = np.mean(list(summary_mae.values()))\n",
    "    overall_mape = np.mean(list(summary_mape.values()))\n",
    "\n",
    "    print(\"-\" * 55)\n",
    "    print(\n",
    "        f\"{'AVERAGE':<10} {overall_r2:<10.6f} {overall_rmse:<10.6f} {overall_mae:<10.6f} {overall_mape:<10.2f}%\"\n",
    "    )\n",
    "\n",
    "    return results, models\n",
    "\n",
    "\n",
    "# Feature importance analysis function remains unchanged\n",
    "def analyze_linear_regression_coefficients(models, X_train, s_parameter_models):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the coefficients of linear regression models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary with trained linear regression models\n",
    "    X_train : pd.DataFrame\n",
    "        Preprocessed feature dataset to get column names\n",
    "    s_parameter_models : dict\n",
    "        Dictionary mapping S-parameter names to their component columns\n",
    "    \"\"\"\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Linear Regression Coefficients for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Create a DataFrame to store coefficients for both components\n",
    "        coef_df = pd.DataFrame(index=feature_names)\n",
    "\n",
    "        for component in components:\n",
    "            model = models[model_name][component]\n",
    "            coef_df[component] = model.coef_\n",
    "\n",
    "        # Add absolute importance (average of absolute coefficients)\n",
    "        coef_df[\"abs_importance\"] = coef_df.abs().mean(axis=1)\n",
    "\n",
    "        # Sort by absolute importance\n",
    "        coef_df = coef_df.sort_values(\"abs_importance\", ascending=False)\n",
    "\n",
    "        # Print top 20 most important features\n",
    "        print(f\"\\nTop 20 features for {model_name}:\")\n",
    "        print(coef_df.head(20))\n",
    "\n",
    "        # Visualize top 15 coefficients\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        top_features = coef_df.head(15).index\n",
    "\n",
    "        for i, component in enumerate(components):\n",
    "            plt.subplot(len(components), 1, i + 1)\n",
    "            plt.barh(top_features, coef_df.loc[top_features, component])\n",
    "            plt.title(f\"{component} Coefficients\")\n",
    "            plt.xlabel(\"Coefficient Value\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f\"linear_regression_{model_name}_coefficients.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "s_parameter_models = {\n",
    "    \"S11\": [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"],\n",
    "    \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "    \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "    \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results, models = train_evaluate_linear_regression_baseline(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, s_parameter_models\n",
    ")\n",
    "\n",
    "# Analyze coefficients\n",
    "analyze_linear_regression_coefficients(models, X_train, s_parameter_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with default hyperparameters\n",
    "models, results, predictions = \n",
    "(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [\n",
    "        0.002,\n",
    "    ],\n",
    "    \"dropout_rate\": [0.1],\n",
    "    \"batch_size\": [512],\n",
    "    \"epochs\": [150],\n",
    "    \"early_stopping_patience\": [20],\n",
    "    \"hidden_sizes\": [[256, 512, 1024, 512]],\n",
    "    \"lr_scheduler_type\": [\"one_cycle\"],\n",
    "    \"activation\": [\"gelu\"],\n",
    "}\n",
    "\n",
    "# Run tuning experiment\n",
    "tuning_results = hyperparameter_tuning(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.002],\n",
    "    \"dropout_rate\": [0.1],\n",
    "    \"batch_size\": [512],\n",
    "    \"epochs\": [200],\n",
    "    \"early_stopping_patience\": [30],\n",
    "    \"hidden_sizes\": [[256, 512, 1024, 512]],\n",
    "    \"lr_scheduler_type\": [\"one_cycle\"],\n",
    "    \"activation\": [\"gelu\"],\n",
    "}\n",
    "\n",
    "# Run tuning experiment\n",
    "tuning_results = hyperparameter_tuning(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.002],\n",
    "    \"dropout_rate\": [0.1],\n",
    "    \"batch_size\": [512],\n",
    "    \"epochs\": [300],\n",
    "    \"early_stopping_patience\": [40],\n",
    "    \"hidden_sizes\": [\n",
    "        [64, 128, 256],\n",
    "        [128, 256, 512],\n",
    "        [256, 512, 1024],\n",
    "        [256, 512, 1024, 512],\n",
    "        [512, 1024, 2048, 1024],\n",
    "        [1024, 2048, 4096, 2048],\n",
    "        [256, 512, 1024, 512, 256],\n",
    "        [512, 1024, 2048, 1024, 512],\n",
    "        [1024, 2048, 4096, 2048, 1024],\n",
    "        [256, 512, 1024, 512, 256, 128],\n",
    "        [512, 1024, 2048, 1024, 512, 256],\n",
    "        [1024, 2048, 4096, 2048, 1024, 512],\n",
    "    ],\n",
    "    \"lr_scheduler_type\": [\"one_cycle\"],\n",
    "    \"activation\": [\"gelu\"],\n",
    "}\n",
    "\n",
    "# Run tuning experiment\n",
    "tuning_results = hyperparameter_tuning(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing check\n",
    "print(\"S12 data statistics:\")\n",
    "print(\n",
    "    \"Mean:\",\n",
    "    Y_raw_train[\"S_deemb(1,2)_real\"].mean(),\n",
    "    Y_raw_train[\"S_deemb(1,2)_imag\"].mean(),\n",
    ")\n",
    "print(\n",
    "    \"Std:\",\n",
    "    Y_raw_train[\"S_deemb(1,2)_real\"].std(),\n",
    "    Y_raw_train[\"S_deemb(1,2)_imag\"].std(),\n",
    ")\n",
    "print(\n",
    "    \"Min:\",\n",
    "    Y_raw_train[\"S_deemb(1,2)_real\"].min(),\n",
    "    Y_raw_train[\"S_deemb(1,2)_imag\"].min(),\n",
    ")\n",
    "print(\n",
    "    \"Max:\",\n",
    "    Y_raw_train[\"S_deemb(1,2)_real\"].max(),\n",
    "    Y_raw_train[\"S_deemb(1,2)_imag\"].max(),\n",
    ")\n",
    "\n",
    "# Consider log scaling for S12 if values are very small\n",
    "# Y_train_log = np.log10(np.abs(Y_train) + 1e-10) * np.sign(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "# Create directory for results\n",
    "os.makedirs(\"freq_aware_results\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Define SMAPE function for better handling of small values\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate SMAPE with protection against division by zero.\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + epsilon\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    smape = numerator / denominator\n",
    "    return np.mean(smape) * 100\n",
    "\n",
    "\n",
    "# Define mean absolute percentage error function\n",
    "def mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate MAPE with protection against division by zero.\"\"\"\n",
    "    non_zero = np.abs(y_true) > epsilon\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan\n",
    "    percentage_errors = (\n",
    "        np.abs(\n",
    "            (y_true[non_zero] - y_pred[non_zero]) / (np.abs(y_true[non_zero]) + epsilon)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "    return np.mean(percentage_errors)\n",
    "\n",
    "\n",
    "# Frequency-aware neural network\n",
    "class FrequencyAwareNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq_features,\n",
    "        other_features,\n",
    "        hidden_sizes=[64, 128, 256],\n",
    "        dropout_rate=0.2,\n",
    "        activation=\"silu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == \"silu\":\n",
    "            activation_fn = nn.SiLU()\n",
    "        elif activation == \"relu\":\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            activation_fn = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        # Frequency-specific processing branch\n",
    "        freq_layers = []\n",
    "        prev_size = freq_features\n",
    "        for h_size in hidden_sizes[:2]:  # First two hidden sizes for branches\n",
    "            freq_layers.append(nn.Linear(prev_size, h_size))\n",
    "            freq_layers.append(\n",
    "                activation_fn\n",
    "            )  # Using SiLU (Swish) activation for better performance\n",
    "            freq_layers.append(nn.BatchNorm1d(h_size))\n",
    "            freq_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.freq_branch = nn.Sequential(*freq_layers)\n",
    "\n",
    "        # Other parameters branch\n",
    "        other_layers = []\n",
    "        prev_size = other_features\n",
    "        for h_size in hidden_sizes[:2]:\n",
    "            other_layers.append(nn.Linear(prev_size, h_size))\n",
    "            other_layers.append(activation_fn)\n",
    "            other_layers.append(nn.BatchNorm1d(h_size))\n",
    "            other_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.other_branch = nn.Sequential(*other_layers)\n",
    "\n",
    "        # Combined processing with residual connections\n",
    "        combined_layers = []\n",
    "        prev_size = hidden_sizes[1] * 2  # Output size from both branches combined\n",
    "\n",
    "        for h_size in hidden_sizes[2:]:\n",
    "            combined_layers.append(nn.Linear(prev_size, h_size))\n",
    "            combined_layers.append(activation_fn)\n",
    "            combined_layers.append(nn.BatchNorm1d(h_size))\n",
    "            combined_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        # Final output layer for real and imaginary components\n",
    "        combined_layers.append(nn.Linear(prev_size, 2))\n",
    "\n",
    "        self.combined = nn.Sequential(*combined_layers)\n",
    "\n",
    "        # Store feature indices for processing\n",
    "        self.freq_indices = None\n",
    "        self.other_indices = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into frequency and other features\n",
    "        if self.freq_indices is None or self.other_indices is None:\n",
    "            raise ValueError(\n",
    "                \"Feature indices not set. Call set_feature_indices() first.\"\n",
    "            )\n",
    "\n",
    "        freq_input = x[:, self.freq_indices]\n",
    "        other_input = x[:, self.other_indices]\n",
    "\n",
    "        # Process through branches\n",
    "        freq_features = self.freq_branch(freq_input)\n",
    "        other_features = self.other_branch(other_input)\n",
    "\n",
    "        # Combine and output\n",
    "        combined = torch.cat([freq_features, other_features], dim=1)\n",
    "        return self.combined(combined)\n",
    "\n",
    "    def set_feature_indices(self, freq_indices, other_indices):\n",
    "        \"\"\"Set indices for frequency and other features.\"\"\"\n",
    "        self.freq_indices = freq_indices\n",
    "        self.other_indices = other_indices\n",
    "\n",
    "\n",
    "# Helper function to identify frequency-related features\n",
    "def identify_frequency_features(X_columns):\n",
    "    \"\"\"Identify frequency-related features in the dataset.\"\"\"\n",
    "    freq_features = [\n",
    "        i\n",
    "        for i, col in enumerate(X_columns)\n",
    "        if \"freq\" in col.lower() or \"band\" in col.lower()\n",
    "    ]\n",
    "    other_features = [i for i in range(len(X_columns)) if i not in freq_features]\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(freq_features)} frequency-related features and {len(other_features)} other features\"\n",
    "    )\n",
    "    return freq_features, other_features\n",
    "\n",
    "\n",
    "# Modified prepare_data_for_pytorch to handle scaling\n",
    "def prepare_data_for_pytorch_with_scaling(\n",
    "    X_train, Y_train, X_test, Y_test, components, batch_size=128, scale_y=True\n",
    "):\n",
    "    \"\"\"Prepare data for PyTorch models with optional Y-scaling.\"\"\"\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "    # Handle Y data scaling if requested\n",
    "    if scale_y:\n",
    "        # Create scaler for Y values\n",
    "        y_scaler = StandardScaler()\n",
    "        Y_train_values = Y_train[components].values\n",
    "        Y_test_values = Y_test[components].values\n",
    "\n",
    "        # Fit scaler and transform data\n",
    "        Y_train_scaled = y_scaler.fit_transform(Y_train_values)\n",
    "        Y_test_scaled = y_scaler.transform(Y_test_values)\n",
    "\n",
    "        # Convert to tensors\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train_scaled)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test_scaled)\n",
    "\n",
    "        # Save scaler for later use\n",
    "        component_str = \"_\".join(components)\n",
    "        joblib.dump(y_scaler, f\"freq_aware_results/{component_str}_scaler.pkl\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            y_scaler,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # No scaling\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train[components].values)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test[components].values)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    X_test_tensor,\n",
    "    Y_test_tensor,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=15,\n",
    "    verbose=True,\n",
    "    lr_scheduler_type=\"reduce_on_plateau\",\n",
    "    warmup_epochs=5,\n",
    "):\n",
    "    \"\"\"Train a PyTorch model with early stopping and learning rate scheduling.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up learning rate scheduler based on specified type\n",
    "    if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.85, patience=5, verbose=verbose, min_lr=5e-7\n",
    "        )\n",
    "    elif lr_scheduler_type == \"cosine_annealing\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif lr_scheduler_type == \"one_cycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=optimizer.param_groups[0][\"lr\"],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # For early stopping\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Track losses and learning rates for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Apply learning rate warmup if needed\n",
    "        if warmup_epochs > 0 and epoch < warmup_epochs and scheduler is None:\n",
    "            lr_multiplier = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = optimizer.param_groups[0][\"lr\"] * lr_multiplier\n",
    "\n",
    "        # Record current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step OneCycleLR scheduler here if being used\n",
    "            if lr_scheduler_type == \"one_cycle\":\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, Y_test_tensor.to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {current_lr:.8f}\"\n",
    "            )\n",
    "\n",
    "        # Learning rate scheduler step (except for OneCycleLR which is done per iteration)\n",
    "        if scheduler is not None:\n",
    "            if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            elif lr_scheduler_type == \"cosine_annealing\":\n",
    "                scheduler.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot learning rate schedule\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(\"freq_aware_results/learning_rate_schedule.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "# Modified evaluate_model function to handle scaling\n",
    "def evaluate_model_with_scaling(\n",
    "    model, X_test_tensor, Y_test_tensor, Y_test, components, device, y_scaler=None\n",
    "):\n",
    "    \"\"\"Evaluate a trained model and calculate performance metrics.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform if scaler was used\n",
    "    if y_scaler is not None:\n",
    "        predictions_original = y_scaler.inverse_transform(predictions)\n",
    "        y_test_original = Y_test[components].values\n",
    "    else:\n",
    "        predictions_original = predictions\n",
    "        y_test_original = Y_test[components].values\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        y_true = y_test_original[:, i]\n",
    "        y_pred = predictions_original[:, i]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Use SMAPE instead of MAPE for S12\n",
    "        if \"S12\" in component or \"S_deemb(1,2)\" in component:\n",
    "            smape_val = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"smape\": smape_val,\n",
    "            }\n",
    "        else:\n",
    "            # Regular MAPE for other S-parameters\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "            }\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        \"rmse\": np.mean([metrics[comp][\"rmse\"] for comp in components]),\n",
    "        \"r2\": np.mean([metrics[comp][\"r2\"] for comp in components]),\n",
    "        \"mae\": np.mean([metrics[comp][\"mae\"] for comp in components]),\n",
    "    }\n",
    "\n",
    "    # Add SMAPE or MAPE average depending on which components were evaluated\n",
    "    if any(\"S12\" in comp or \"S_deemb(1,2)\" in comp for comp in components):\n",
    "        avg_metrics[\"smape\"] = np.mean([metrics[comp][\"smape\"] for comp in components])\n",
    "    else:\n",
    "        avg_metrics[\"mape\"] = np.mean([metrics[comp][\"mape\"] for comp in components])\n",
    "\n",
    "    return metrics, avg_metrics, predictions_original\n",
    "\n",
    "    return metrics, avg_metrics, predictions\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses, model_name):\n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Learning Curves for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"freq_aware_results/learning_curves_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_predictions(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot predictions vs actual values.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        ax.scatter(y_true, y_pred, alpha=0.3)\n",
    "        ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"{component}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/predictions_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_distribution(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot error distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        errors = y_pred - y_true\n",
    "\n",
    "        sns.histplot(errors, kde=True, ax=ax)\n",
    "        ax.set_xlabel(\"Prediction Error\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{component} Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/error_dist_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Modified train_frequency_aware_models function\n",
    "def train_frequency_aware_models(\n",
    "    X_train, X_test, Y_train, Y_test, hyperparameters=None, selected_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train frequency-aware models for each S-parameter with conditional scaling.\n",
    "    \"\"\"\n",
    "    # S-parameter definitions\n",
    "    s_parameter_models = {\n",
    "        \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "    }\n",
    "\n",
    "    # 'S12': ['S_deemb(1,2)_real', 'S_deemb(1,2)_imag']\n",
    "\n",
    "    # Set default hyperparameters if not provided\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {\n",
    "            \"hidden_sizes\": [64, 128, 256],\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 150,\n",
    "            \"early_stopping_patience\": 15,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"lr_scheduler_type\": \"one_cycle\",\n",
    "        }\n",
    "\n",
    "    # Filter features if requested\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train.columns)\n",
    "\n",
    "    # Store results and models\n",
    "    models = {}\n",
    "    all_results = {}\n",
    "    all_predictions = {}\n",
    "    scalers = {}  # Store scalers for each model\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train a model for each S-parameter\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training frequency-aware model for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Decide whether to scale Y data (only for S12)\n",
    "        scale_y = model_name == \"S12\"\n",
    "\n",
    "        # Prepare data with conditional scaling\n",
    "        prep_results = prepare_data_for_pytorch_with_scaling(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            components,\n",
    "            hyperparameters[\"batch_size\"],\n",
    "            scale_y=scale_y,\n",
    "        )\n",
    "\n",
    "        if scale_y:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                y_scaler,\n",
    "            ) = prep_results\n",
    "            scalers[model_name] = y_scaler\n",
    "            print(\"Applied StandardScaler to Y values for S12\")\n",
    "        else:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                _,\n",
    "            ) = prep_results\n",
    "\n",
    "        # Initialize model\n",
    "        model = FrequencyAwareNetwork(\n",
    "            len(freq_indices),\n",
    "            len(other_indices),\n",
    "            hyperparameters[\"hidden_sizes\"],\n",
    "            hyperparameters[\"dropout_rate\"],\n",
    "            hyperparameters.get(\"activation\", \"gelu\"),\n",
    "        )\n",
    "        model.set_feature_indices(freq_indices, other_indices)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        # Train model (use your existing train_model function)\n",
    "        trained_model, train_losses, val_losses = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            hyperparameters[\"epochs\"],\n",
    "            hyperparameters[\"early_stopping_patience\"],\n",
    "            lr_scheduler_type=hyperparameters.get(\"lr_scheduler_type\", \"one_cycle\"),\n",
    "        )\n",
    "\n",
    "        # Plot learning curves\n",
    "        plot_learning_curves(train_losses, val_losses, model_name)\n",
    "\n",
    "        # Evaluate model with proper scaling handling\n",
    "        metrics, avg_metrics, predictions = evaluate_model_with_scaling(\n",
    "            trained_model,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            Y_test,\n",
    "            components,\n",
    "            device,\n",
    "            scalers.get(model_name),\n",
    "        )\n",
    "\n",
    "        # Plot predictions and error distributions\n",
    "        plot_predictions(Y_test, predictions, components, model_name)\n",
    "        plot_error_distribution(Y_test, predictions, components, model_name)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nPerformance metrics for {model_name}:\")\n",
    "        for component, metric in metrics.items():\n",
    "            print(f\"  {component}:\")\n",
    "            print(f\"    RMSE: {metric['rmse']:.6f}\")\n",
    "            print(f\"    R²: {metric['r2']:.6f}\")\n",
    "            print(f\"    MAE: {metric['mae']:.6f}\")\n",
    "            if \"smape\" in metric:\n",
    "                print(f\"    SMAPE: {metric['smape']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"    MAPE: {metric['mape']:.2f}%\")\n",
    "\n",
    "        print(f\"\\nAverage metrics for {model_name}:\")\n",
    "        print(f\"  R²: {avg_metrics['r2']:.6f}\")\n",
    "        print(f\"  RMSE: {avg_metrics['rmse']:.6f}\")\n",
    "        print(f\"  MAE: {avg_metrics['mae']:.6f}\")\n",
    "        if \"smape\" in avg_metrics:\n",
    "            print(f\"  SMAPE: {avg_metrics['smape']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  MAPE: {avg_metrics['mape']:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        models[model_name] = trained_model\n",
    "        all_results[model_name] = {\n",
    "            \"component_metrics\": metrics,\n",
    "            \"avg_metrics\": avg_metrics,\n",
    "        }\n",
    "        all_predictions[model_name] = predictions\n",
    "\n",
    "    # Record total training time\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Save models\n",
    "    for model_name, model in models.items():\n",
    "        torch.save(model.state_dict(), f\"freq_aware_results/{model_name}_model.pth\")\n",
    "\n",
    "    print(\"Models and results saved to freq_aware_results/\")\n",
    "\n",
    "    return models, all_results, all_predictions, scalers\n",
    "\n",
    "\n",
    "# Function to experiment with different hyperparameters\n",
    "def hyperparameter_tuning(X_train, X_test, Y_train, Y_test, param_grid):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning by training models with different configurations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values to try\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each configuration\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    def generate_combinations(index, current_params):\n",
    "        if index == len(param_keys):\n",
    "            # Train model with current parameter combination\n",
    "            config_name = \"_\".join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "            print(f\"\\n\\n{'#' * 70}\")\n",
    "            print(f\"# Testing configuration: {config_name}\")\n",
    "            print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "            # Train models\n",
    "            _, all_results, _ = train_frequency_aware_models(\n",
    "                X_train, X_test, Y_train, Y_test, hyperparameters=current_params\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            avg_r2 = np.mean(\n",
    "                [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "            )\n",
    "            results[config_name] = {\n",
    "                \"params\": current_params.copy(),\n",
    "                \"avg_r2\": avg_r2,\n",
    "                \"detailed_results\": all_results,\n",
    "            }\n",
    "            return\n",
    "\n",
    "        # Recursive exploration of parameter combinations\n",
    "        for value in param_values[index]:\n",
    "            current_params[param_keys[index]] = value\n",
    "            generate_combinations(index + 1, current_params)\n",
    "\n",
    "    # Start generating combinations\n",
    "    generate_combinations(0, {})\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (config_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Configuration: {config_name}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "        print(f\"   Parameters: {result['params']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to test different feature subsets\n",
    "def feature_selection_experiment(X_train, X_test, Y_train, Y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Test different feature subsets to find optimal combinations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Complete feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    feature_sets : dict\n",
    "        Dictionary mapping set names to lists of feature columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each feature set\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for set_name, features in feature_sets.items():\n",
    "        print(f\"\\n\\n{'#' * 70}\")\n",
    "        print(f\"# Testing feature set: {set_name} ({len(features)} features)\")\n",
    "        print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "        # Train models with this feature set\n",
    "        _, all_results, _ = train_frequency_aware_models(\n",
    "            X_train, X_test, Y_train, Y_test, selected_features=features\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        avg_r2 = np.mean(\n",
    "            [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "        )\n",
    "        results[set_name] = {\n",
    "            \"features\": features,\n",
    "            \"feature_count\": len(features),\n",
    "            \"avg_r2\": avg_r2,\n",
    "            \"detailed_results\": all_results,\n",
    "        }\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE SELECTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (set_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Feature Set: {set_name}\")\n",
    "        print(f\"   Features: {len(result['features'])}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Example of running with all features and default hyperparameters\n",
    "# models, results, predictions = train_frequency_aware_models(\n",
    "#     X_train, X_test, Y_raw_train, Y_raw_test,\n",
    "#     hyperparameters=default_hyperparameters\n",
    "# )\n",
    "\n",
    "# Example of hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'dropout_rate': [0.1, 0.2, 0.3],\n",
    "#     'batch_size': [128, 256, 512]\n",
    "# }\n",
    "# tuning_results = hyperparameter_tuning(X_train, X_test, Y_raw_train, Y_raw_test, param_grid)\n",
    "\n",
    "# Example of feature selection experiment\n",
    "# core_features = ['freq', 'vb', 'vc', 'gm_abs_log']\n",
    "# freq_features = [col for col in X_train.columns if 'freq' in col]\n",
    "# impedance_features = [col for col in X_train.columns if 'Zin' in col or 'Zout' in col]\n",
    "\n",
    "# feature_sets = {\n",
    "#     'all_features': X_train.columns.tolist(),\n",
    "#     'frequency_only': freq_features,\n",
    "#     'core_plus_frequency': core_features + freq_features,\n",
    "#     'core_plus_impedance': core_features + impedance_features,\n",
    "#     'optimized_set': ['freq', 'freq_log', 'freq_log_norm', 'vb', 'vc', 'gm_abs_log',\n",
    "#                       'Zin_real_log', 'Zin_imag_log', 'Zout_real_log']\n",
    "# }\n",
    "# feature_results = feature_selection_experiment(X_train, X_test, Y_raw_train, Y_raw_test, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [384, 768, 1536, 768, 384],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S12\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"hidden_sizes\": [256, 512, 1024, 512],\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# Train with scaling for S11\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, hyperparameters=best_hyperparameters\n",
    ")\n",
    "\n",
    "# You can also save the scalers for future use\n",
    "joblib.dump(scalers, \"freq_aware_results/all_scalers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def hyperparameter_tuning2(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    param_grid,\n",
    "    s_parameter=None,\n",
    "    selected_features=None,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning by training models with different configurations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values to try\n",
    "    s_parameter : dict or None\n",
    "        Dictionary mapping model names to component lists (e.g., {'S21': [...]})\n",
    "    selected_features : list or None\n",
    "        List of feature columns to use\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each configuration\n",
    "    best_config : dict\n",
    "        Best hyperparameter configuration\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    results = {}\n",
    "    best_score = -float(\"inf\")\n",
    "    best_config = None\n",
    "    best_model = None\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    # Create all combinations\n",
    "    combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# Starting hyperparameter tuning with {len(combinations)} configurations\")\n",
    "    print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "    for i, combination in enumerate(combinations):\n",
    "        # Create hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(param_names, combination))\n",
    "\n",
    "        # Create config name for identification\n",
    "        config_name = \"_\".join([f\"{k}={v}\" for k, v in hyperparameters.items()])\n",
    "        print(f\"\\n{'-' * 70}\")\n",
    "        print(f\"Configuration {i + 1}/{len(combinations)}: {config_name}\")\n",
    "        print(f\"{'-' * 70}\")\n",
    "\n",
    "        try:\n",
    "            # Train models with current configuration\n",
    "            # Remove s_parameters and seed parameters as train_frequency_aware_models doesn't accept them\n",
    "            models, all_results, all_predictions, scalers = (\n",
    "                train_frequency_aware_models(\n",
    "                    X_train,\n",
    "                    X_test,\n",
    "                    Y_train,\n",
    "                    Y_test,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    selected_features=selected_features,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Calculate overall score (using average R² as primary metric)\n",
    "            r2_scores = []\n",
    "            for model_name, result in all_results.items():\n",
    "                r2_scores.append(result[\"avg_metrics\"][\"r2\"])\n",
    "\n",
    "            avg_r2 = np.mean(r2_scores)\n",
    "\n",
    "            # Store results\n",
    "            results[config_name] = {\n",
    "                \"hyperparameters\": hyperparameters.copy(),\n",
    "                \"avg_r2\": avg_r2,\n",
    "                \"detailed_results\": deepcopy(all_results),\n",
    "                \"models\": models,\n",
    "                \"scalers\": scalers,\n",
    "            }\n",
    "\n",
    "            print(f\"\\nConfiguration {i + 1} results:\")\n",
    "            print(f\"  Average R²: {avg_r2:.6f}\")\n",
    "\n",
    "            # Update best configuration\n",
    "            if avg_r2 > best_score:\n",
    "                best_score = avg_r2\n",
    "                best_config = hyperparameters.copy()\n",
    "                best_model = models\n",
    "                print(\"  ** New best configuration! **\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in configuration {i + 1}: {str(e)}\")\n",
    "            results[config_name] = {\n",
    "                \"hyperparameters\": hyperparameters.copy(),\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Rank results - only process valid results\n",
    "    valid_results = {k: v for k, v in results.items() if \"error\" not in v}\n",
    "\n",
    "    if valid_results:  # Check if there are any valid results\n",
    "        ranked_results = sorted(\n",
    "            valid_results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True\n",
    "        )\n",
    "\n",
    "        for i, (config_name, result) in enumerate(ranked_results[:5]):  # Show top 5\n",
    "            print(f\"\\n{i + 1}. Configuration: {config_name}\")\n",
    "            print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "            print(\"   Hyperparameters:\")\n",
    "            for k, v in result[\"hyperparameters\"].items():\n",
    "                print(f\"     {k}: {v}\")\n",
    "\n",
    "        print(f\"\\nBest overall configuration achieves R² = {best_score:.6f}\")\n",
    "\n",
    "        # Save detailed results to CSV\n",
    "        summary_data = []\n",
    "        for config_name, result in valid_results.items():\n",
    "            row = {\"configuration\": config_name, \"avg_r2\": result[\"avg_r2\"]}\n",
    "            row.update(result[\"hyperparameters\"])\n",
    "            summary_data.append(row)\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df = summary_df.sort_values(\"avg_r2\", ascending=False)\n",
    "        summary_df.to_csv(\n",
    "            \"freq_aware_results/hyperparameter_tuning_results.csv\", index=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo valid configurations completed successfully!\")\n",
    "        print(\"All configurations resulted in errors.\")\n",
    "\n",
    "    return results, best_config\n",
    "\n",
    "\n",
    "def create_architecture_grid():\n",
    "    \"\"\"\n",
    "    Create a predefined grid of architecture patterns to test.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"symmetric_pyramid_small\": [128, 256, 512, 256, 128],\n",
    "        \"symmetric_pyramid_medium\": [256, 512, 1024, 512, 256],\n",
    "        \"symmetric_pyramid_large\": [384, 768, 1536, 768, 384],\n",
    "        \"progressive_narrow\": [64, 128, 256, 512, 1024],\n",
    "        \"progressive_wide\": [128, 256, 512, 1024, 2048],\n",
    "        \"deep_narrow\": [128, 256, 512, 512, 512, 256, 128],\n",
    "        \"deep_wide\": [256, 512, 1024, 1024, 1024, 512, 256],\n",
    "        \"wide_shallow\": [512, 1024, 512],\n",
    "        \"hourglass\": [512, 1024, 256, 1024, 512],\n",
    "        \"alternating\": [256, 512, 256, 512, 256],\n",
    "        \"compact\": [256, 256, 512, 512],\n",
    "        \"simple_deep\": [256, 512, 1024, 1024, 512, 256],\n",
    "    }\n",
    "\n",
    "\n",
    "def test_all_s_parameters_architectures(\n",
    "    X_train, X_test, Y_train, Y_test, s_parameters_to_test=None, architecture_grid=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Test different architectures for all specified S-parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    s_parameters_to_test : dict or None\n",
    "        Dictionary of S-parameters to test. If None, tests all.\n",
    "    architecture_grid : dict or None\n",
    "        Dictionary of architectures to test. If None, uses default grid.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    all_results : dict\n",
    "        Dictionary of results for each S-parameter\n",
    "    best_configs : dict\n",
    "        Dictionary of best configurations for each S-parameter\n",
    "    overall_best : dict\n",
    "        Overall best configuration across all S-parameters\n",
    "    \"\"\"\n",
    "    # Default S-parameter models\n",
    "    if s_parameters_to_test is None:\n",
    "        s_parameters_to_test = {\n",
    "            \"S11\": [\"S_deemb(1,1)_real\", \"S_deemb(1,1)_imag\"],\n",
    "            \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "            \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "            \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "        }\n",
    "\n",
    "    # Default architecture grid\n",
    "    if architecture_grid is None:\n",
    "        architecture_grid = create_architecture_grid()\n",
    "\n",
    "    # Create parameter grid with architectures\n",
    "    param_grid = {\n",
    "        \"hidden_sizes\": list(architecture_grid.values()),\n",
    "        \"dropout_rate\": [0.1],\n",
    "        \"learning_rate\": [0.002],\n",
    "        \"activation\": [\"gelu\"],\n",
    "        \"lr_scheduler_type\": [\"reduce_on_plateau\"],\n",
    "        \"batch_size\": [512],\n",
    "        \"epochs\": [300],\n",
    "        \"early_stopping_patience\": [40],\n",
    "    }\n",
    "\n",
    "    all_results = {}\n",
    "    best_configs = {}\n",
    "    best_r2_scores = {}\n",
    "\n",
    "    # Test each S-parameter individually\n",
    "    for s_param_name, s_param_components in s_parameters_to_test.items():\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TESTING ARCHITECTURES FOR {s_param_name}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        # Create single S-parameter configuration\n",
    "        s_param_single = {s_param_name: s_param_components}\n",
    "\n",
    "        # Run hyperparameter tuning\n",
    "        results, best_config = hyperparameter_tuning2(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            Y_train,\n",
    "            Y_test,\n",
    "            param_grid=param_grid,\n",
    "            s_parameter=s_param_single,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        all_results[s_param_name] = results\n",
    "        best_configs[s_param_name] = best_config\n",
    "\n",
    "        # Extract best R² score\n",
    "        valid_results = {k: v for k, v in results.items() if \"error\" not in v}\n",
    "        if valid_results:\n",
    "            best_r2 = max(r[\"avg_r2\"] for r in valid_results.values())\n",
    "            best_r2_scores[s_param_name] = best_r2\n",
    "\n",
    "        # Analyze architecture performance for this S-parameter\n",
    "        analysis = analyze_architecture_performance(results, s_param_name)\n",
    "\n",
    "    # Find overall best configuration\n",
    "    overall_best_r2 = -float(\"inf\")\n",
    "    overall_best_config = None\n",
    "    overall_best_s_param = None\n",
    "\n",
    "    for s_param, r2_score in best_r2_scores.items():\n",
    "        if r2_score > overall_best_r2:\n",
    "            overall_best_r2 = r2_score\n",
    "            overall_best_config = best_configs[s_param]\n",
    "            overall_best_s_param = s_param\n",
    "\n",
    "    overall_best = {\n",
    "        \"s_parameter\": overall_best_s_param,\n",
    "        \"r2_score\": overall_best_r2,\n",
    "        \"config\": overall_best_config,\n",
    "    }\n",
    "\n",
    "    # Create comprehensive summary\n",
    "    create_comprehensive_summary(\n",
    "        all_results, best_configs, best_r2_scores, architecture_grid\n",
    "    )\n",
    "\n",
    "    return all_results, best_configs, overall_best\n",
    "\n",
    "\n",
    "def analyze_architecture_performance(tuning_results, s_param_name):\n",
    "    \"\"\"\n",
    "    Analyze architecture performance for a specific S-parameter.\n",
    "    \"\"\"\n",
    "    architecture_performance = []\n",
    "\n",
    "    # Extract valid results\n",
    "    valid_results = {k: v for k, v in tuning_results.items() if \"error\" not in v}\n",
    "\n",
    "    # Check if there are any valid results\n",
    "    if not valid_results:\n",
    "        print(f\"\\nNo valid results to analyze for {s_param_name}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    for config_name, result in valid_results.items():\n",
    "        try:\n",
    "            arch_data = {\n",
    "                \"configuration\": config_name,\n",
    "                \"avg_r2\": result[\"avg_r2\"],\n",
    "                \"hidden_sizes\": result[\"hyperparameters\"][\"hidden_sizes\"],\n",
    "                \"dropout_rate\": result[\"hyperparameters\"][\"dropout_rate\"],\n",
    "                \"learning_rate\": result[\"hyperparameters\"][\"learning_rate\"],\n",
    "                \"activation\": result[\"hyperparameters\"][\"activation\"],\n",
    "                \"num_layers\": len(result[\"hyperparameters\"][\"hidden_sizes\"]),\n",
    "                \"total_params\": sum(result[\"hyperparameters\"][\"hidden_sizes\"]),\n",
    "                \"architecture_type\": categorize_architecture(\n",
    "                    result[\"hyperparameters\"][\"hidden_sizes\"]\n",
    "                ),\n",
    "            }\n",
    "            architecture_performance.append(arch_data)\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Missing key in configuration {config_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Check if we have any data to analyze\n",
    "    if not architecture_performance:\n",
    "        print(f\"\\nNo valid architecture performance data for {s_param_name}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    arch_df = pd.DataFrame(architecture_performance)\n",
    "\n",
    "    try:\n",
    "        # Group by architecture type\n",
    "        arch_summary = (\n",
    "            arch_df.groupby(\"architecture_type\")\n",
    "            .agg({\"avg_r2\": [\"mean\", \"max\", \"min\", \"count\"]})\n",
    "            .round(6)\n",
    "        )\n",
    "\n",
    "        print(f\"\\nArchitecture Type Performance Summary for {s_param_name}:\")\n",
    "        print(arch_summary)\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Unable to group data: {e}\")\n",
    "        print(arch_df.columns if not arch_df.empty else \"Empty DataFrame\")\n",
    "\n",
    "    # Save detailed analysis\n",
    "    arch_df.to_csv(\n",
    "        f\"freq_aware_results/{s_param_name}_architecture_analysis.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Visualize architecture performance if we have data\n",
    "    if not arch_df.empty:\n",
    "        plot_architecture_performance(arch_df, s_param_name)\n",
    "\n",
    "    return arch_df\n",
    "\n",
    "\n",
    "def categorize_architecture(hidden_sizes):\n",
    "    \"\"\"\n",
    "    Categorize architecture based on pattern.\n",
    "    \"\"\"\n",
    "    if len(hidden_sizes) == 3:\n",
    "        return \"simple\"\n",
    "    elif len(hidden_sizes) == 5:\n",
    "        if hidden_sizes[0] == hidden_sizes[-1]:\n",
    "            return \"symmetric_pyramid\"\n",
    "        else:\n",
    "            return \"asymmetric\"\n",
    "    elif len(hidden_sizes) == 6:\n",
    "        if all(i <= j for i, j in zip(hidden_sizes, hidden_sizes[1:])):\n",
    "            return \"progressive\"\n",
    "        else:\n",
    "            return \"deep_mixed\"\n",
    "    else:\n",
    "        return f\"custom_{len(hidden_sizes)}_layers\"\n",
    "\n",
    "\n",
    "def plot_architecture_performance(arch_df, s_param_name):\n",
    "    \"\"\"\n",
    "    Plot architecture performance visualization for a specific S-parameter.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Box plot by architecture type\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=arch_df, x=\"architecture_type\", y=\"avg_r2\")\n",
    "    plt.title(f\"R² Performance by Architecture Type ({s_param_name})\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Scatter plot: Total parameters vs R²\n",
    "    plt.subplot(1, 2, 2)\n",
    "    scatter = plt.scatter(\n",
    "        arch_df[\"total_params\"],\n",
    "        arch_df[\"avg_r2\"],\n",
    "        c=arch_df[\"num_layers\"],\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.colorbar(scatter, label=\"Number of Layers\")\n",
    "    plt.xlabel(\"Total Parameters\")\n",
    "    plt.ylabel(\"Average R²\")\n",
    "    plt.title(f\"Model Size vs Performance ({s_param_name})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"freq_aware_results/{s_param_name}_architecture_performance_analysis.png\",\n",
    "        dpi=300,\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Heatmap of hyperparameter combinations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pivot_data = arch_df.pivot_table(\n",
    "        values=\"avg_r2\", index=\"activation\", columns=\"dropout_rate\", aggfunc=\"mean\"\n",
    "    )\n",
    "    sns.heatmap(pivot_data, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "    plt.title(f\"Average R² by Activation and Dropout Rate ({s_param_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"freq_aware_results/{s_param_name}_hyperparameter_heatmap.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_comprehensive_summary(\n",
    "    all_results, best_configs, best_r2_scores, architecture_grid\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary of all S-parameter tuning results.\n",
    "    \"\"\"\n",
    "    # Check if we have any valid results\n",
    "    if not best_r2_scores:\n",
    "        print(\"\\nNo valid results to summarize!\")\n",
    "        return\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "\n",
    "    for s_param in all_results.keys():\n",
    "        row = {\n",
    "            \"S_Parameter\": s_param,\n",
    "            \"Best_R2\": best_r2_scores.get(s_param, np.nan),\n",
    "            \"Best_Architecture\": str(best_configs[s_param][\"hidden_sizes\"])\n",
    "            if s_param in best_configs\n",
    "            else None,\n",
    "            \"Best_Dropout\": best_configs[s_param][\"dropout_rate\"]\n",
    "            if s_param in best_configs\n",
    "            else None,\n",
    "            \"Best_LR\": best_configs[s_param][\"learning_rate\"]\n",
    "            if s_param in best_configs\n",
    "            else None,\n",
    "            \"Best_Activation\": best_configs[s_param][\"activation\"]\n",
    "            if s_param in best_configs\n",
    "            else None,\n",
    "            \"Best_Scheduler\": best_configs[s_param][\"lr_scheduler_type\"]\n",
    "            if s_param in best_configs\n",
    "            else None,\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "\n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df = summary_df.sort_values(\"Best_R2\", ascending=False)\n",
    "        summary_df.to_csv(\n",
    "            \"freq_aware_results/all_s_parameters_best_configs.csv\", index=False\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"BEST CONFIGURATIONS FOR ALL S-PARAMETERS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        # Create comparative visualization\n",
    "        plot_comparative_summary(summary_df, best_configs)\n",
    "    else:\n",
    "        print(\"\\nNo summary data to save!\")\n",
    "\n",
    "\n",
    "def plot_comparative_summary(summary_df, best_configs):\n",
    "    \"\"\"\n",
    "    Create comparative visualizations across all S-parameters.\n",
    "    \"\"\"\n",
    "    # Figure 1: Performance comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(\n",
    "        summary_df[\"S_Parameter\"], summary_df[\"Best_R2\"], alpha=0.8, color=\"skyblue\"\n",
    "    )\n",
    "    plt.xlabel(\"S-Parameter\")\n",
    "    plt.ylabel(\"Best R² Score\")\n",
    "    plt.title(\"Best Performance for Each S-Parameter\")\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{height:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"freq_aware_results/all_s_parameters_performance_comparison.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Figure 2: Architecture patterns used\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    arch_counts = {}\n",
    "    for s_param in best_configs:\n",
    "        arch_type = categorize_architecture(best_configs[s_param][\"hidden_sizes\"])\n",
    "        arch_counts[arch_type] = arch_counts.get(arch_type, 0) + 1\n",
    "\n",
    "    plt.pie(arch_counts.values(), labels=arch_counts.keys(), autopct=\"%1.1f%%\")\n",
    "    plt.title(\"Most Successful Architecture Types Across All S-Parameters\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"freq_aware_results/architecture_distribution.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Usage example with specific S-parameters\n",
    "def run_comprehensive_architecture_testing():\n",
    "    \"\"\"\n",
    "    Run comprehensive architecture testing for all or selected S-parameters.\n",
    "    \"\"\"\n",
    "    # Example 1: Test all S-parameters\n",
    "    all_results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "        X_train, X_test, Y_raw_train, Y_raw_test\n",
    "    )\n",
    "\n",
    "    # Example 2: Test specific S-parameters\n",
    "    selected_s_params = {\n",
    "        \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "        \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "    }\n",
    "\n",
    "    selected_results, selected_best_configs, selected_overall_best = (\n",
    "        test_all_s_parameters_architectures(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            Y_raw_train,\n",
    "            Y_raw_test,\n",
    "            s_parameters_to_test=selected_s_params,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return all_results, best_configs, overall_best\n",
    "\n",
    "\n",
    "# Custom architecture testing\n",
    "def run_custom_architectures():\n",
    "    \"\"\"\n",
    "    Test custom architecture patterns.\n",
    "    \"\"\"\n",
    "    # Define custom architectures\n",
    "    custom_architectures = {\n",
    "        \"ultra_deep\": [128] * 8,  # 8 layers of same size\n",
    "        \"funnel\": [1024, 512, 256, 128, 64, 32],  # Decreasing\n",
    "        \"reverse_funnel\": [32, 64, 128, 256, 512, 1024],  # Increasing\n",
    "        \"v_shape\": [512, 256, 128, 256, 512],  # V pattern\n",
    "        \"diamond\": [256, 512, 1024, 2048, 1024, 512, 256],  # Wide middle\n",
    "    }\n",
    "\n",
    "    all_results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "        X_train, X_test, Y_raw_train, Y_raw_test, architecture_grid=custom_architectures\n",
    "    )\n",
    "\n",
    "    return all_results, best_configs, overall_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only S12 and S21\n",
    "selected_s_params = {\"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"]}\n",
    "\n",
    "# Define custom architectures\n",
    "custom_architectures = {\n",
    "    # Basic/Simple\n",
    "    \"simple\": [512, 512, 512],\n",
    "    # Progressive (Increasing)\n",
    "    \"progressive\": [128, 256, 512, 1024],\n",
    "    # Reverse Progressive (Decreasing)\n",
    "    \"reverse_progressive\": [1024, 512, 256, 128],\n",
    "    # Symmetric Pyramid\n",
    "    \"sym_pyramid\": [256, 512, 1024, 512, 256],\n",
    "    # Asymmetric Pyramid\n",
    "    \"asym_pyramid\": [256, 512, 1024, 512, 256, 128],\n",
    "    # Deep Mixed\n",
    "    \"deep_mixed\": [256, 512, 256, 1024, 512, 256],\n",
    "    # V-Shape\n",
    "    \"v_shape\": [512, 256, 128, 256, 512],\n",
    "    # Inverted V-Shape\n",
    "    \"inv_v_shape\": [128, 256, 512, 256, 128],\n",
    "    # Hourglass\n",
    "    \"hourglass\": [512, 256, 128, 256, 512],\n",
    "    # Alternating\n",
    "    \"alternating\": [256, 512, 256, 512, 256],\n",
    "    # Compact\n",
    "    \"compact\": [512, 1024, 512],\n",
    "    # Ultra-Deep\n",
    "    \"ultra_deep\": [256, 256, 256, 256, 256, 256],\n",
    "    # Wide Shallow\n",
    "    \"wide_shallow\": [1024, 2048, 1024],\n",
    "    # Funnel\n",
    "    \"funnel\": [1024, 512, 256, 128],\n",
    "    # Reverse Funnel\n",
    "    \"reverse_funnel\": [128, 256, 512, 1024],\n",
    "    # Diamond\n",
    "    \"diamond\": [256, 512, 1024, 512, 256],\n",
    "    # Step Pattern\n",
    "    \"step\": [256, 256, 512, 512, 1024],\n",
    "    # Zigzag\n",
    "    \"zigzag\": [256, 512, 256, 512, 256],\n",
    "    # Exponential Growth\n",
    "    \"exponential\": [128, 256, 512, 1024, 2048],\n",
    "}\n",
    "\n",
    "results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    s_parameters_to_test=selected_s_params,\n",
    "    architecture_grid=custom_architectures,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only S12 and S21\n",
    "selected_s_params = {\"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"]}\n",
    "\n",
    "# Define custom architectures\n",
    "custom_architectures = {\n",
    "    # Best performer and variations\n",
    "    \"wide_best\": [1024, 2048, 1024],\n",
    "    \"ultra_wide\": [1024, 4096, 1024],\n",
    "    \"extra_wide\": [2048, 4096, 2048],\n",
    "    # Explore intermediate widths\n",
    "    \"wide_medium_1\": [768, 1536, 768],\n",
    "    \"wide_medium_2\": [1536, 3072, 1536],\n",
    "    # Slight asymmetry variations of best\n",
    "    \"asym_wide_1\": [512, 2048, 1024],\n",
    "    \"asym_wide_2\": [1024, 3072, 2048],\n",
    "    \"asym_wide_3\": [2048, 4096, 1024],\n",
    "    # Four-layer wide patterns\n",
    "    \"wide_4layer_1\": [512, 1024, 2048, 1024],\n",
    "    \"wide_4layer_2\": [1024, 2048, 2048, 1024],\n",
    "    \"wide_4layer_3\": [512, 2048, 2048, 512],\n",
    "    # Five-layer variations maintaining width\n",
    "    \"wide_5layer_1\": [512, 1024, 2048, 1024, 512],\n",
    "    \"wide_5layer_2\": [1024, 1024, 2048, 1024, 1024],\n",
    "    # Alternative wide patterns\n",
    "    \"wide_alt_1\": [1024, 1536, 1024],\n",
    "    \"wide_alt_2\": [2048, 3072, 2048],\n",
    "    \"wide_alt_3\": [1024, 2048, 1536, 1024],\n",
    "    # Exploration beyond widths - depth variations\n",
    "    \"deep_wide_1\": [1024, 1024, 1024, 1024],\n",
    "    \"deep_wide_2\": [1536, 1536, 1536],\n",
    "    \"deep_wide_3\": [2048, 2048, 2048],\n",
    "    # Compact wide variations\n",
    "    \"compact_wide_1\": [2048, 4096, 2048],\n",
    "    \"compact_wide_2\": [3072, 6144, 3072],\n",
    "}\n",
    "\n",
    "results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    s_parameters_to_test=selected_s_params,\n",
    "    architecture_grid=custom_architectures,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create directory for results\n",
    "os.makedirs(\"freq_aware_results\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Define SMAPE function for better handling of small values\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate SMAPE with protection against division by zero.\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + epsilon\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    smape = numerator / denominator\n",
    "    return np.mean(smape) * 100\n",
    "\n",
    "\n",
    "# Define mean absolute percentage error function\n",
    "def mean_absolute_percentage_error(y_true, y_pred, epsilon=1e-10):\n",
    "    \"\"\"Calculate MAPE with protection against division by zero.\"\"\"\n",
    "    non_zero = np.abs(y_true) > epsilon\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan\n",
    "    percentage_errors = (\n",
    "        np.abs(\n",
    "            (y_true[non_zero] - y_pred[non_zero]) / (np.abs(y_true[non_zero]) + epsilon)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "    return np.mean(percentage_errors)\n",
    "\n",
    "\n",
    "# Frequency-aware neural network\n",
    "class FrequencyAwareNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq_features,\n",
    "        other_features,\n",
    "        hidden_sizes=[64, 128, 256],\n",
    "        dropout_rate=0.2,\n",
    "        activation=\"silu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == \"silu\":\n",
    "            activation_fn = nn.SiLU()\n",
    "        elif activation == \"relu\":\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            activation_fn = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        # Frequency-specific processing branch\n",
    "        freq_layers = []\n",
    "        prev_size = freq_features\n",
    "        for h_size in hidden_sizes[:2]:  # First two hidden sizes for branches\n",
    "            freq_layers.append(nn.Linear(prev_size, h_size))\n",
    "            freq_layers.append(\n",
    "                activation_fn\n",
    "            )  # Using SiLU (Swish) activation for better performance\n",
    "            freq_layers.append(nn.BatchNorm1d(h_size))\n",
    "            freq_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.freq_branch = nn.Sequential(*freq_layers)\n",
    "\n",
    "        # Other parameters branch\n",
    "        other_layers = []\n",
    "        prev_size = other_features\n",
    "        for h_size in hidden_sizes[:2]:\n",
    "            other_layers.append(nn.Linear(prev_size, h_size))\n",
    "            other_layers.append(activation_fn)\n",
    "            other_layers.append(nn.BatchNorm1d(h_size))\n",
    "            other_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        self.other_branch = nn.Sequential(*other_layers)\n",
    "\n",
    "        # Combined processing with residual connections\n",
    "        combined_layers = []\n",
    "        prev_size = hidden_sizes[1] * 2  # Output size from both branches combined\n",
    "\n",
    "        for h_size in hidden_sizes[2:]:\n",
    "            combined_layers.append(nn.Linear(prev_size, h_size))\n",
    "            combined_layers.append(activation_fn)\n",
    "            combined_layers.append(nn.BatchNorm1d(h_size))\n",
    "            combined_layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = h_size\n",
    "\n",
    "        # Final output layer for real and imaginary components\n",
    "        combined_layers.append(nn.Linear(prev_size, 2))\n",
    "\n",
    "        self.combined = nn.Sequential(*combined_layers)\n",
    "\n",
    "        # Store feature indices for processing\n",
    "        self.freq_indices = None\n",
    "        self.other_indices = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into frequency and other features\n",
    "        if self.freq_indices is None or self.other_indices is None:\n",
    "            raise ValueError(\n",
    "                \"Feature indices not set. Call set_feature_indices() first.\"\n",
    "            )\n",
    "\n",
    "        freq_input = x[:, self.freq_indices]\n",
    "        other_input = x[:, self.other_indices]\n",
    "\n",
    "        # Process through branches\n",
    "        freq_features = self.freq_branch(freq_input)\n",
    "        other_features = self.other_branch(other_input)\n",
    "\n",
    "        # Combine and output\n",
    "        combined = torch.cat([freq_features, other_features], dim=1)\n",
    "        return self.combined(combined)\n",
    "\n",
    "    def set_feature_indices(self, freq_indices, other_indices):\n",
    "        \"\"\"Set indices for frequency and other features.\"\"\"\n",
    "        self.freq_indices = freq_indices\n",
    "        self.other_indices = other_indices\n",
    "\n",
    "\n",
    "# Helper function to identify frequency-related features\n",
    "def identify_frequency_features(X_columns):\n",
    "    \"\"\"Identify frequency-related features in the dataset.\"\"\"\n",
    "    freq_features = [\n",
    "        i\n",
    "        for i, col in enumerate(X_columns)\n",
    "        if \"freq\" in col.lower() or \"band\" in col.lower()\n",
    "    ]\n",
    "    other_features = [i for i in range(len(X_columns)) if i not in freq_features]\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(freq_features)} frequency-related features and {len(other_features)} other features\"\n",
    "    )\n",
    "    return freq_features, other_features\n",
    "\n",
    "\n",
    "# Modified prepare_data_for_pytorch to handle scaling\n",
    "def prepare_data_for_pytorch_with_scaling(\n",
    "    X_train, Y_train, X_test, Y_test, components, batch_size=128, scale_y=True\n",
    "):\n",
    "    \"\"\"Prepare data for PyTorch models with optional Y-scaling.\"\"\"\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "    # Handle Y data scaling if requested\n",
    "    if scale_y:\n",
    "        # Create scaler for Y values\n",
    "        y_scaler = StandardScaler()\n",
    "        Y_train_values = Y_train[components].values\n",
    "        Y_test_values = Y_test[components].values\n",
    "\n",
    "        # Fit scaler and transform data\n",
    "        Y_train_scaled = y_scaler.fit_transform(Y_train_values)\n",
    "        Y_test_scaled = y_scaler.transform(Y_test_values)\n",
    "\n",
    "        # Convert to tensors\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train_scaled)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test_scaled)\n",
    "\n",
    "        # Save scaler for later use\n",
    "        component_str = \"_\".join(components)\n",
    "        joblib.dump(y_scaler, f\"freq_aware_results/{component_str}_scaler.pkl\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            y_scaler,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # No scaling\n",
    "        Y_train_tensor = torch.FloatTensor(Y_train[components].values)\n",
    "        Y_test_tensor = torch.FloatTensor(Y_test[components].values)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return (\n",
    "            X_train_tensor,\n",
    "            Y_train_tensor,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            train_loader,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    X_test_tensor,\n",
    "    Y_test_tensor,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=15,\n",
    "    verbose=True,\n",
    "    lr_scheduler_type=\"reduce_on_plateau\",\n",
    "    warmup_epochs=5,\n",
    "):\n",
    "    \"\"\"Train a PyTorch model with early stopping and learning rate scheduling.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up learning rate scheduler based on specified type\n",
    "    if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.85, patience=5, verbose=verbose, min_lr=5e-7\n",
    "        )\n",
    "    elif lr_scheduler_type == \"cosine_annealing\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif lr_scheduler_type == \"one_cycle\":\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=optimizer.param_groups[0][\"lr\"],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # For early stopping\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Track losses and learning rates for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Apply learning rate warmup if needed\n",
    "        if warmup_epochs > 0 and epoch < warmup_epochs and scheduler is None:\n",
    "            lr_multiplier = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = optimizer.param_groups[0][\"lr\"] * lr_multiplier\n",
    "\n",
    "        # Record current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step OneCycleLR scheduler here if being used\n",
    "            if lr_scheduler_type == \"one_cycle\":\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_tensor.to(device))\n",
    "            val_loss = criterion(val_outputs, Y_test_tensor.to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {current_lr:.8f}\"\n",
    "            )\n",
    "\n",
    "        # Learning rate scheduler step (except for OneCycleLR which is done per iteration)\n",
    "        if scheduler is not None:\n",
    "            if lr_scheduler_type == \"reduce_on_plateau\":\n",
    "                scheduler.step(val_loss)\n",
    "            elif lr_scheduler_type == \"cosine_annealing\":\n",
    "                scheduler.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot learning rate schedule\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.savefig(\"freq_aware_results/learning_rate_schedule.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "# Modified evaluate_model function to handle scaling\n",
    "def evaluate_model_with_scaling(\n",
    "    model, X_test_tensor, Y_test_tensor, Y_test, components, device, y_scaler=None\n",
    "):\n",
    "    \"\"\"Evaluate a trained model and calculate performance metrics.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform if scaler was used\n",
    "    if y_scaler is not None:\n",
    "        predictions_original = y_scaler.inverse_transform(predictions)\n",
    "        y_test_original = Y_test[components].values\n",
    "    else:\n",
    "        predictions_original = predictions\n",
    "        y_test_original = Y_test[components].values\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        y_true = y_test_original[:, i]\n",
    "        y_pred = predictions_original[:, i]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Use SMAPE instead of MAPE for S12\n",
    "        if \"S12\" in component or \"S_deemb(1,2)\" in component:\n",
    "            smape_val = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"smape\": smape_val,\n",
    "            }\n",
    "        else:\n",
    "            # Regular MAPE for other S-parameters\n",
    "            metrics[component] = {\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"r2\": r2,\n",
    "                \"mae\": mae,\n",
    "                \"mape\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "            }\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        \"rmse\": np.mean([metrics[comp][\"rmse\"] for comp in components]),\n",
    "        \"r2\": np.mean([metrics[comp][\"r2\"] for comp in components]),\n",
    "        \"mae\": np.mean([metrics[comp][\"mae\"] for comp in components]),\n",
    "    }\n",
    "\n",
    "    # Add SMAPE or MAPE average depending on which components were evaluated\n",
    "    if any(\"S12\" in comp or \"S_deemb(1,2)\" in comp for comp in components):\n",
    "        avg_metrics[\"smape\"] = np.mean([metrics[comp][\"smape\"] for comp in components])\n",
    "    else:\n",
    "        avg_metrics[\"mape\"] = np.mean([metrics[comp][\"mape\"] for comp in components])\n",
    "\n",
    "    return metrics, avg_metrics, predictions_original\n",
    "\n",
    "    return metrics, avg_metrics, predictions\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses, model_name):\n",
    "    \"\"\"Plot the learning curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Learning Curves for {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"freq_aware_results/learning_curves_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_predictions(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot predictions vs actual values.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        ax.scatter(y_true, y_pred, alpha=0.3)\n",
    "        ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        ax.set_title(f\"{component}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/predictions_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_error_distribution(Y_test, predictions, components, model_name):\n",
    "    \"\"\"Plot error distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(components), figsize=(15, 5))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        ax = axes[i] if len(components) > 1 else axes\n",
    "        y_true = Y_test[component].values\n",
    "        y_pred = predictions[:, i]\n",
    "\n",
    "        errors = y_pred - y_true\n",
    "\n",
    "        sns.histplot(errors, kde=True, ax=ax)\n",
    "        ax.set_xlabel(\"Prediction Error\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{component} Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"freq_aware_results/error_dist_{model_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Modified train_frequency_aware_models function\n",
    "def train_frequency_aware_models(\n",
    "    X_train, X_test, Y_train, Y_test, hyperparameters=None, selected_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train frequency-aware models for each S-parameter with conditional scaling.\n",
    "    \"\"\"\n",
    "    # S-parameter definitions\n",
    "    s_parameter_models = {\n",
    "        \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "    }\n",
    "\n",
    "    # 'S12': ['S_deemb(1,2)_real', 'S_deemb(1,2)_imag']\n",
    "\n",
    "    # Set default hyperparameters if not provided\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {\n",
    "            \"hidden_sizes\": [64, 128, 256],\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 256,\n",
    "            \"epochs\": 150,\n",
    "            \"early_stopping_patience\": 15,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"lr_scheduler_type\": \"one_cycle\",\n",
    "        }\n",
    "\n",
    "    # Filter features if requested\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Identify frequency-related features\n",
    "    freq_indices, other_indices = identify_frequency_features(X_train.columns)\n",
    "\n",
    "    # Store results and models\n",
    "    models = {}\n",
    "    all_results = {}\n",
    "    all_predictions = {}\n",
    "    scalers = {}  # Store scalers for each model\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train a model for each S-parameter\n",
    "    for model_name, components in s_parameter_models.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training frequency-aware model for {model_name}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        # Decide whether to scale Y data (only for S12)\n",
    "        scale_y = model_name == \"S12\"\n",
    "\n",
    "        # Prepare data with conditional scaling\n",
    "        prep_results = prepare_data_for_pytorch_with_scaling(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            components,\n",
    "            hyperparameters[\"batch_size\"],\n",
    "            scale_y=scale_y,\n",
    "        )\n",
    "\n",
    "        if scale_y:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                y_scaler,\n",
    "            ) = prep_results\n",
    "            scalers[model_name] = y_scaler\n",
    "            print(\"Applied StandardScaler to Y values for S12\")\n",
    "        else:\n",
    "            (\n",
    "                X_train_tensor,\n",
    "                Y_train_tensor,\n",
    "                X_test_tensor,\n",
    "                Y_test_tensor,\n",
    "                train_loader,\n",
    "                _,\n",
    "            ) = prep_results\n",
    "\n",
    "        # Initialize model\n",
    "        model = FrequencyAwareNetwork(\n",
    "            len(freq_indices),\n",
    "            len(other_indices),\n",
    "            hyperparameters[\"hidden_sizes\"],\n",
    "            hyperparameters[\"dropout_rate\"],\n",
    "            hyperparameters.get(\"activation\", \"gelu\"),\n",
    "        )\n",
    "        model.set_feature_indices(freq_indices, other_indices)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        # Train model (use your existing train_model function)\n",
    "        trained_model, train_losses, val_losses = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            hyperparameters[\"epochs\"],\n",
    "            hyperparameters[\"early_stopping_patience\"],\n",
    "            lr_scheduler_type=hyperparameters.get(\"lr_scheduler_type\", \"one_cycle\"),\n",
    "        )\n",
    "\n",
    "        # Plot learning curves\n",
    "        plot_learning_curves(train_losses, val_losses, model_name)\n",
    "\n",
    "        # Evaluate model with proper scaling handling\n",
    "        metrics, avg_metrics, predictions = evaluate_model_with_scaling(\n",
    "            trained_model,\n",
    "            X_test_tensor,\n",
    "            Y_test_tensor,\n",
    "            Y_test,\n",
    "            components,\n",
    "            device,\n",
    "            scalers.get(model_name),\n",
    "        )\n",
    "\n",
    "        # Plot predictions and error distributions\n",
    "        plot_predictions(Y_test, predictions, components, model_name)\n",
    "        plot_error_distribution(Y_test, predictions, components, model_name)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nPerformance metrics for {model_name}:\")\n",
    "        for component, metric in metrics.items():\n",
    "            print(f\"  {component}:\")\n",
    "            print(f\"    RMSE: {metric['rmse']:.6f}\")\n",
    "            print(f\"    R²: {metric['r2']:.6f}\")\n",
    "            print(f\"    MAE: {metric['mae']:.6f}\")\n",
    "            if \"smape\" in metric:\n",
    "                print(f\"    SMAPE: {metric['smape']:.2f}%\")\n",
    "            else:\n",
    "                print(f\"    MAPE: {metric['mape']:.2f}%\")\n",
    "\n",
    "        print(f\"\\nAverage metrics for {model_name}:\")\n",
    "        print(f\"  R²: {avg_metrics['r2']:.6f}\")\n",
    "        print(f\"  RMSE: {avg_metrics['rmse']:.6f}\")\n",
    "        print(f\"  MAE: {avg_metrics['mae']:.6f}\")\n",
    "        if \"smape\" in avg_metrics:\n",
    "            print(f\"  SMAPE: {avg_metrics['smape']:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  MAPE: {avg_metrics['mape']:.2f}%\")\n",
    "\n",
    "        # Store results\n",
    "        models[model_name] = trained_model\n",
    "        all_results[model_name] = {\n",
    "            \"component_metrics\": metrics,\n",
    "            \"avg_metrics\": avg_metrics,\n",
    "        }\n",
    "        all_predictions[model_name] = predictions\n",
    "\n",
    "    # Record total training time\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # Save models\n",
    "    for model_name, model in models.items():\n",
    "        torch.save(model.state_dict(), f\"freq_aware_results/{model_name}_model.pth\")\n",
    "\n",
    "    print(\"Models and results saved to freq_aware_results/\")\n",
    "\n",
    "    return models, all_results, all_predictions, scalers\n",
    "\n",
    "\n",
    "# Function to experiment with different hyperparameters\n",
    "def hyperparameter_tuning(X_train, X_test, Y_train, Y_test, param_grid):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning by training models with different configurations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Preprocessed feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameter values to try\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each configuration\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Generate all hyperparameter combinations\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "\n",
    "    def generate_combinations(index, current_params):\n",
    "        if index == len(param_keys):\n",
    "            # Train model with current parameter combination\n",
    "            config_name = \"_\".join([f\"{k}={v}\" for k, v in current_params.items()])\n",
    "            print(f\"\\n\\n{'#' * 70}\")\n",
    "            print(f\"# Testing configuration: {config_name}\")\n",
    "            print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "            # Train models\n",
    "            _, all_results, _ = train_frequency_aware_models(\n",
    "                X_train, X_test, Y_train, Y_test, hyperparameters=current_params\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            avg_r2 = np.mean(\n",
    "                [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "            )\n",
    "            results[config_name] = {\n",
    "                \"params\": current_params.copy(),\n",
    "                \"avg_r2\": avg_r2,\n",
    "                \"detailed_results\": all_results,\n",
    "            }\n",
    "            return\n",
    "\n",
    "        # Recursive exploration of parameter combinations\n",
    "        for value in param_values[index]:\n",
    "            current_params[param_keys[index]] = value\n",
    "            generate_combinations(index + 1, current_params)\n",
    "\n",
    "    # Start generating combinations\n",
    "    generate_combinations(0, {})\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (config_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Configuration: {config_name}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "        print(f\"   Parameters: {result['params']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to test different feature subsets\n",
    "def feature_selection_experiment(X_train, X_test, Y_train, Y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Test different feature subsets to find optimal combinations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame\n",
    "        Complete feature datasets\n",
    "    Y_train, Y_test : pd.DataFrame\n",
    "        Target S-parameter datasets\n",
    "    feature_sets : dict\n",
    "        Dictionary mapping set names to lists of feature columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of results for each feature set\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for set_name, features in feature_sets.items():\n",
    "        print(f\"\\n\\n{'#' * 70}\")\n",
    "        print(f\"# Testing feature set: {set_name} ({len(features)} features)\")\n",
    "        print(f\"{'#' * 70}\\n\")\n",
    "\n",
    "        # Train models with this feature set\n",
    "        _, all_results, _ = train_frequency_aware_models(\n",
    "            X_train, X_test, Y_train, Y_test, selected_features=features\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        avg_r2 = np.mean(\n",
    "            [result[\"avg_metrics\"][\"r2\"] for result in all_results.values()]\n",
    "        )\n",
    "        results[set_name] = {\n",
    "            \"features\": features,\n",
    "            \"feature_count\": len(features),\n",
    "            \"avg_r2\": avg_r2,\n",
    "            \"detailed_results\": all_results,\n",
    "        }\n",
    "\n",
    "    # Rank results\n",
    "    ranked_results = sorted(results.items(), key=lambda x: x[1][\"avg_r2\"], reverse=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE SELECTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (set_name, result) in enumerate(ranked_results):\n",
    "        print(f\"\\n{i + 1}. Feature Set: {set_name}\")\n",
    "        print(f\"   Features: {len(result['features'])}\")\n",
    "        print(f\"   Average R²: {result['avg_r2']:.6f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Example of running with all features and default hyperparameters\n",
    "# models, results, predictions = train_frequency_aware_models(\n",
    "#     X_train, X_test, Y_raw_train, Y_raw_test,\n",
    "#     hyperparameters=default_hyperparameters\n",
    "# )\n",
    "\n",
    "# Example of hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
    "#     'dropout_rate': [0.1, 0.2, 0.3],\n",
    "#     'batch_size': [128, 256, 512]\n",
    "# }\n",
    "# tuning_results = hyperparameter_tuning(X_train, X_test, Y_raw_train, Y_raw_test, param_grid)\n",
    "\n",
    "# Example of feature selection experiment\n",
    "# core_features = ['freq', 'vb', 'vc', 'gm_abs_log']\n",
    "# freq_features = [col for col in X_train.columns if 'freq' in col]\n",
    "# impedance_features = [col for col in X_train.columns if 'Zin' in col or 'Zout' in col]\n",
    "\n",
    "# feature_sets = {\n",
    "#     'all_features': X_train.columns.tolist(),\n",
    "#     'frequency_only': freq_features,\n",
    "#     'core_plus_frequency': core_features + freq_features,\n",
    "#     'core_plus_impedance': core_features + impedance_features,\n",
    "#     'optimized_set': ['freq', 'freq_log', 'freq_log_norm', 'vb', 'vc', 'gm_abs_log',\n",
    "#                       'Zin_real_log', 'Zin_imag_log', 'Zout_real_log']\n",
    "# }\n",
    "# feature_results = feature_selection_experiment(X_train, X_test, Y_raw_train, Y_raw_test, feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only S12 and S21\n",
    "selected_s_params = {\"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"]}\n",
    "\n",
    "# Define custom architectures\n",
    "custom_architectures = {\n",
    "    # Basic/Simple\n",
    "    \"simple\": [512, 512, 512],\n",
    "    # Progressive (Increasing)\n",
    "    \"progressive\": [128, 256, 512, 1024],\n",
    "    # Reverse Progressive (Decreasing)\n",
    "    \"reverse_progressive\": [1024, 512, 256, 128],\n",
    "    # Symmetric Pyramid\n",
    "    \"sym_pyramid\": [256, 512, 1024, 512, 256],\n",
    "    # Asymmetric Pyramid\n",
    "    \"asym_pyramid\": [256, 512, 1024, 512, 256, 128],\n",
    "    # Deep Mixed\n",
    "    \"deep_mixed\": [256, 512, 256, 1024, 512, 256],\n",
    "    # V-Shape\n",
    "    \"v_shape\": [512, 256, 128, 256, 512],\n",
    "    # Inverted V-Shape\n",
    "    \"inv_v_shape\": [128, 256, 512, 256, 128],\n",
    "    # Hourglass\n",
    "    \"hourglass\": [512, 256, 128, 256, 512],\n",
    "    # Alternating\n",
    "    \"alternating\": [256, 512, 256, 512, 256],\n",
    "    # Compact\n",
    "    \"compact\": [512, 1024, 512],\n",
    "    # Ultra-Deep\n",
    "    \"ultra_deep\": [256, 256, 256, 256, 256, 256],\n",
    "    # Wide Shallow\n",
    "    \"wide_shallow\": [1024, 2048, 1024],\n",
    "    # Funnel\n",
    "    \"funnel\": [1024, 512, 256, 128],\n",
    "    # Reverse Funnel\n",
    "    \"reverse_funnel\": [128, 256, 512, 1024],\n",
    "    # Diamond\n",
    "    \"diamond\": [256, 512, 1024, 512, 256],\n",
    "    # Step Pattern\n",
    "    \"step\": [256, 256, 512, 512, 1024],\n",
    "    # Zigzag\n",
    "    \"zigzag\": [256, 512, 256, 512, 256],\n",
    "    # Exponential Growth\n",
    "    \"exponential\": [128, 256, 512, 1024, 2048],\n",
    "}\n",
    "\n",
    "results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    s_parameters_to_test=selected_s_params,\n",
    "    architecture_grid=custom_architectures,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only S12 and S21\n",
    "selected_s_params = {\"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"]}\n",
    "\n",
    "# Define custom architectures\n",
    "custom_architectures = {\n",
    "    # Based on the ultra-deep pattern (best MAE)\n",
    "    \"ultra_deep_256\": [256, 256, 256, 256, 256, 256],\n",
    "    \"ultra_deep_8\": [256, 256, 256, 256, 256, 256, 256, 256],\n",
    "    \"ultra_deep_128\": [128, 128, 128, 128, 128, 128, 128, 128],\n",
    "    \"ultra_deep_512\": [512, 512, 512, 512, 512, 512],\n",
    "    # Based on the wide-narrow-wide pattern (strong performers)\n",
    "    \"compact_512\": [512, 1024, 512],\n",
    "    \"compact_768\": [768, 1536, 768],\n",
    "    \"compact_1024\": [1024, 2048, 1024],\n",
    "    \"compact_256\": [256, 512, 256],\n",
    "    # Based on alternating pattern (good performance)\n",
    "    \"alternating_v1\": [256, 512, 256, 512, 256],\n",
    "    \"alternating_v2\": [128, 256, 128, 256, 128],\n",
    "    \"alternating_deep\": [256, 512, 256, 512, 256, 512, 256],\n",
    "    \"alternating_wide\": [512, 1024, 512, 1024, 512],\n",
    "    # Progressive increases (successful in top 5)\n",
    "    \"progressive_fine\": [128, 192, 256, 320, 384, 448, 512],\n",
    "    \"progressive_quad\": [256, 512, 1024, 2048],\n",
    "    \"progressive_exp\": [64, 128, 256, 512, 1024],\n",
    "    # Decreasing pattern variations (good performance)\n",
    "    \"decrease_fine\": [1024, 896, 768, 640, 512, 384, 256],\n",
    "    \"decrease_quad\": [2048, 1024, 512, 256],\n",
    "    \"decrease_exp\": [1024, 512, 256, 128, 64],\n",
    "    # Hybrid patterns (combining best elements)\n",
    "    \"hybrid_1\": [256, 512, 1024, 512, 256],  # Like Config 4\n",
    "    \"hybrid_2\": [512, 1024, 2048, 1024, 512],\n",
    "    \"hybrid_3\": [128, 256, 512, 1024, 512, 256, 128],\n",
    "    # Bottleneck variations\n",
    "    \"bottleneck_32\": [256, 128, 64, 32, 64, 128, 256],\n",
    "    \"bottleneck_16\": [512, 256, 128, 64, 32, 16, 32, 64, 128, 256, 512],\n",
    "    \"bottleneck_64\": [512, 256, 128, 64, 128, 256, 512],\n",
    "    # Plateau patterns (inspired by top performers)\n",
    "    \"plateau_256\": [128, 256, 256, 256, 256, 128],\n",
    "    \"plateau_512\": [256, 512, 512, 512, 512, 256],\n",
    "    \"plateau_1024\": [512, 1024, 1024, 1024, 1024, 512],\n",
    "    # Fine-grained ultra-deep\n",
    "    \"ultra_deep_fine_1\": [192, 192, 192, 192, 192, 192, 192, 192],\n",
    "    \"ultra_deep_fine_2\": [384, 384, 384, 384, 384, 384],\n",
    "    \"ultra_deep_fine_3\": [448, 448, 448, 448, 448, 448],\n",
    "    # Similar to best performers but with slight modifications\n",
    "    \"modified_config_12\": [256, 256, 256, 256, 256, 256, 256],\n",
    "    \"modified_config_13\": [1024, 1536, 2048, 1536, 1024],\n",
    "    \"modified_config_18\": [256, 512, 256, 512, 256, 512],\n",
    "}\n",
    "\n",
    "results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    s_parameters_to_test=selected_s_params,\n",
    "    architecture_grid=custom_architectures,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom architectures\n",
    "custom_architectures = {\n",
    "    \"custom_1\": [512, 1024, 2048, 1024, 512],\n",
    "    \"custom_2\": [128, 256, 512, 1024, 512, 256],\n",
    "    \"custom_3\": [384, 768, 384],\n",
    "}\n",
    "\n",
    "results, best_configs, overall_best = test_all_s_parameters_architectures(\n",
    "    X_train, X_test, Y_raw_train, Y_raw_test, architecture_grid=custom_architectures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [384, 768, 1536, 768, 384],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Configure which S-parameters to train\n",
    "s_parameters_to_train = {\n",
    "    \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "}\n",
    "\n",
    "# Train models with reproducible results\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    s_parameters=s_parameters_to_train,\n",
    "    seed=42,  # Ensures reproducible results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [384, 768, 1536, 768, 384],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Configure which S-parameters to train\n",
    "s_parameters_to_train = {\n",
    "    \"S12\": [\"S_deemb(1,2)_real\", \"S_deemb(1,2)_imag\"],\n",
    "}\n",
    "\n",
    "# Train models with reproducible results\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    s_parameters=s_parameters_to_train,\n",
    "    seed=42,  # Ensures reproducible results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [256, 512, 1024, 512, 256],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Configure which S-parameters to train\n",
    "s_parameters_to_train = {\n",
    "    \"S22\": [\"S_deemb(2,2)_real\", \"S_deemb(2,2)_imag\"],\n",
    "}\n",
    "\n",
    "# Train models with reproducible results\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    s_parameters=s_parameters_to_train,\n",
    "    seed=42,  # Ensures reproducible results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "best_hyperparameters = {\n",
    "    \"hidden_sizes\": [256, 512, 1024, 512, 256],\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"learning_rate\": 0.002,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 300,\n",
    "    \"early_stopping_patience\": 40,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"lr_scheduler_type\": \"reduce_on_plateau\",\n",
    "}\n",
    "\n",
    "# Configure which S-parameters to train\n",
    "s_parameters_to_train = {\n",
    "    \"S21\": [\"S_deemb(2,1)_real\", \"S_deemb(2,1)_imag\"],\n",
    "}\n",
    "\n",
    "# Train models with reproducible results\n",
    "models, results, predictions, scalers = train_frequency_aware_models(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_raw_train,\n",
    "    Y_raw_test,\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    s_parameters=s_parameters_to_train,\n",
    "    seed=42,  # Ensures reproducible results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
